def forwards ( self , orm ) : NEW_LINE # ▁ Adding ▁ model ▁ ' TestCenterUser ' ENDCOM INDENT db . create_table ( ' student _ testcenteruser ' , ( ( ' id ' , self . gf ( ' django . db . models . fields . AutoField ' ) ( primary_key = True ) ) , ( ' user ' , self . gf ( ' django . db . models . fields . related . ForeignKey ' ) ( default = None , to = orm [ ' auth . User ' ] , unique = True ) ) , ( ' created _ at ' , self . gf ( ' django . db . models . fields . DateTimeField ' ) ( auto_now_add = True , db_index = True , blank = True ) ) , ( ' updated _ at ' , self . gf ( ' django . db . models . fields . DateTimeField ' ) ( auto_now = True , db_index = True , blank = True ) ) , ( ' user _ updated _ at ' , self . gf ( ' django . db . models . fields . DateTimeField ' ) ( db_index = True ) ) , ( ' candidate _ id ' , self . gf ( ' django . db . models . fields . IntegerField ' ) ( null = True , db_index = True ) ) , ( ' client _ candidate _ id ' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 50 , db_index = True ) ) , ( ' first _ name ' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 30 , db_index = True ) ) , ( ' last _ name ' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 50 , db_index = True ) ) , ( ' middle _ name ' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 30 , blank = True ) ) , ( ' suffix ' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 255 , blank = True ) ) , ( ' salutation ' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 50 , blank = True ) ) , ( ' address _ 1' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 40 ) ) , ( ' address _ 2' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 40 , blank = True ) ) , ( ' address _ 3' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 40 , blank = True ) ) , ( ' city ' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 32 , db_index = True ) ) , ( ' state ' , self . gf ( ' django . db . models . fields . CharField ' ) ( db_index = True , max_length = 20 , blank = True ) ) , ( ' postal _ code ' , self . gf ( ' django . db . models . fields . CharField ' ) ( db_index = True , max_length = 16 , blank = True ) ) , ( ' country ' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 3 , db_index = True ) ) , ( ' phone ' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 35 ) ) , ( ' extension ' , self . gf ( ' django . db . models . fields . CharField ' ) ( db_index = True , max_length = 8 , blank = True ) ) , ( ' phone _ country _ code ' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 3 , db_index = True ) ) , ( ' fax ' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 35 , blank = True ) ) , ( ' fax _ country _ code ' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 3 , blank = True ) ) , ( ' company _ name ' , self . gf ( ' django . db . models . fields . CharField ' ) ( max_length = 50 , blank = True ) ) , ) ) NEW_LINE db . send_create_signal ( ' student ' , [ ' TestCenterUser ' ] ) NEW_LINE DEDENT
def backwards ( self , orm ) : NEW_LINE # ▁ Deleting ▁ model ▁ ' TestCenterUser ' ENDCOM INDENT db . delete_table ( ' student _ testcenteruser ' ) NEW_LINE DEDENT
def default_get ( self , cr , uid , fields , context = None ) : NEW_LINE INDENT if context is None : NEW_LINE INDENT context = { } NEW_LINE DEDENT res = super ( stock_move_consume , self ) . default_get ( cr , uid , fields , context = context ) NEW_LINE move = self . pool . get ( ' stock . move ' ) . browse ( cr , uid , context [ ' active _ id ' ] , context = context ) NEW_LINE if ' product _ id ' in fields : NEW_LINE INDENT res . update ( { ' product _ id ' : move . product_id . id } ) NEW_LINE DEDENT if ' product _ uom ' in fields : NEW_LINE INDENT res . update ( { ' product _ uom ' : move . product_uom . id } ) NEW_LINE DEDENT if ' product _ qty ' in fields : NEW_LINE INDENT res . update ( { ' product _ qty ' : move . product_uom_qty } ) NEW_LINE DEDENT if ' location _ id ' in fields : NEW_LINE INDENT res . update ( { ' location _ id ' : move . location_id . id } ) NEW_LINE DEDENT return res NEW_LINE DEDENT
def do_move_consume ( self , cr , uid , ids , context = None ) : NEW_LINE INDENT if context is None : NEW_LINE INDENT context = { } NEW_LINE DEDENT move_obj = self . pool . get ( ' stock . move ' ) NEW_LINE uom_obj = self . pool . get ( ' product . uom ' ) NEW_LINE production_obj = self . pool . get ( ' mrp . production ' ) NEW_LINE move_ids = context [ ' active _ ids ' ] NEW_LINE move = move_obj . browse ( cr , uid , move_ids [ 0 ] , context = context ) NEW_LINE production_id = move . raw_material_production_id . id NEW_LINE production = production_obj . browse ( cr , uid , production_id , context = context ) NEW_LINE precision = self . pool [ ' decimal . precision ' ] . precision_get ( cr , uid , ' Product ▁ Unit ▁ of ▁ Measure ' ) NEW_LINE for data in self . browse ( cr , uid , ids , context = context ) : NEW_LINE INDENT qty = uom_obj . _compute_qty ( cr , uid , data [ ' product _ uom ' ] . id , data . product_qty , data . product_id . uom_id . id ) NEW_LINE remaining_qty = move . product_qty - qty NEW_LINE # check ▁ for ▁ product ▁ quantity ▁ is ▁ less ▁ than ▁ previously ▁ planned ENDCOM if float_compare ( remaining_qty , 0 , precision_digits = precision ) >= 0 : NEW_LINE INDENT move_obj . action_consume ( cr , uid , move_ids , qty , data . location_id . id , restrict_lot_id = data . restrict_lot_id . id , context = context ) NEW_LINE DEDENT else : NEW_LINE INDENT consumed_qty = min ( move . product_qty , qty ) NEW_LINE new_moves = move_obj . action_consume ( cr , uid , move_ids , consumed_qty , data . location_id . id , restrict_lot_id = data . restrict_lot_id . id , context = context ) NEW_LINE # consumed ▁ more ▁ in ▁ wizard ▁ than ▁ previously ▁ planned ENDCOM extra_more_qty = qty - consumed_qty NEW_LINE # create ▁ new ▁ line ▁ for ▁ a ▁ remaining ▁ qty ▁ of ▁ the ▁ product ENDCOM extra_move_id = production_obj . _make_consume_line_from_data ( cr , uid , production , data . product_id , data . product_id . uom_id . id , extra_more_qty , False , 0 , context = context ) NEW_LINE move_obj . write ( cr , uid , [ extra_move_id ] , { ' restrict _ lot _ id ' : data . restrict_lot_id . id } , context = context ) NEW_LINE move_obj . action_done ( cr , uid , [ extra_move_id ] , context = context ) NEW_LINE DEDENT DEDENT return { ' type ' : ' ir . actions . act _ window _ close ' } NEW_LINE DEDENT
def obj_load_attr ( self , attrname ) : NEW_LINE INDENT setattr ( self , attrname , ' loaded ! ' ) NEW_LINE DEDENT
def marco ( self , context ) : NEW_LINE INDENT return ' polo ' NEW_LINE DEDENT
def update_test ( self , context ) : NEW_LINE INDENT if context . project_id == ' alternate ' : NEW_LINE INDENT self . bar = ' alternate - context ' NEW_LINE DEDENT else : NEW_LINE INDENT self . bar = ' updated ' NEW_LINE DEDENT DEDENT
def save ( self , context ) : NEW_LINE INDENT self . obj_reset_changes ( ) NEW_LINE DEDENT
def refresh ( self , context ) : NEW_LINE INDENT self . foo = 321 NEW_LINE self . bar = ' refreshed ' NEW_LINE self . obj_reset_changes ( ) NEW_LINE DEDENT
def modify_save_modify ( self , context ) : NEW_LINE INDENT self . bar = ' meow ' NEW_LINE self . save ( ) NEW_LINE self . foo = 42 NEW_LINE DEDENT
def test_datetime_or_none ( self ) : NEW_LINE INDENT naive_dt = datetime . datetime . now ( ) NEW_LINE dt = timeutils . parse_isotime ( timeutils . isotime ( naive_dt ) ) NEW_LINE self . assertEqual ( utils . datetime_or_none ( dt ) , dt ) NEW_LINE self . assertEqual ( utils . datetime_or_none ( dt ) , naive_dt . replace ( tzinfo = iso8601 . iso8601 . Utc ( ) , microsecond = 0 ) ) NEW_LINE self . assertIsNone ( utils . datetime_or_none ( None ) ) NEW_LINE self . assertRaises ( ValueError , utils . datetime_or_none , ' foo ' ) NEW_LINE DEDENT
def test_datetime_or_str_or_none ( self ) : NEW_LINE INDENT dts = timeutils . isotime ( ) NEW_LINE dt = timeutils . parse_isotime ( dts ) NEW_LINE self . assertEqual ( utils . datetime_or_str_or_none ( dt ) , dt ) NEW_LINE self . assertIsNone ( utils . datetime_or_str_or_none ( None ) ) NEW_LINE self . assertEqual ( utils . datetime_or_str_or_none ( dts ) , dt ) NEW_LINE self . assertRaises ( ValueError , utils . datetime_or_str_or_none , ' foo ' ) NEW_LINE DEDENT
def test_int_or_none ( self ) : NEW_LINE INDENT self . assertEqual ( utils . int_or_none ( 1 ) , 1 ) NEW_LINE self . assertEqual ( utils . int_or_none ( '1' ) , 1 ) NEW_LINE self . assertIsNone ( utils . int_or_none ( None ) ) NEW_LINE self . assertRaises ( ValueError , utils . int_or_none , ' foo ' ) NEW_LINE DEDENT
def test_str_or_none ( self ) : NEW_LINE INDENT class Obj ( object ) : NEW_LINE INDENT pass NEW_LINE DEDENT self . assertEqual ( utils . str_or_none ( ' foo ' ) , ' foo ' ) NEW_LINE self . assertEqual ( utils . str_or_none ( 1 ) , '1' ) NEW_LINE self . assertIsNone ( utils . str_or_none ( None ) ) NEW_LINE DEDENT
def test_ip_or_none ( self ) : NEW_LINE INDENT ip4 = netaddr . IPAddress ( '1.2.3.4' , 4 ) NEW_LINE ip6 = netaddr . IPAddress ( '1 : : 2' , 6 ) NEW_LINE self . assertEqual ( utils . ip_or_none ( 4 ) ( '1.2.3.4' ) , ip4 ) NEW_LINE self . assertEqual ( utils . ip_or_none ( 6 ) ( '1 : : 2' ) , ip6 ) NEW_LINE self . assertIsNone ( utils . ip_or_none ( 4 ) ( None ) ) NEW_LINE self . assertIsNone ( utils . ip_or_none ( 6 ) ( None ) ) NEW_LINE self . assertRaises ( netaddr . AddrFormatError , utils . ip_or_none ( 4 ) , ' foo ' ) NEW_LINE self . assertRaises ( netaddr . AddrFormatError , utils . ip_or_none ( 6 ) , ' foo ' ) NEW_LINE DEDENT
def test_dt_serializer ( self ) : NEW_LINE INDENT class Obj ( object ) : NEW_LINE INDENT foo = utils . dt_serializer ( ' bar ' ) NEW_LINE DEDENT obj = Obj ( ) NEW_LINE obj . bar = timeutils . parse_isotime ( '1955-11-05T00:00:00Z ' ) NEW_LINE self . assertEqual ( '1955-11-05T00:00:00Z ' , obj . foo ( ) ) NEW_LINE obj . bar = None NEW_LINE self . assertIsNone ( obj . foo ( ) ) NEW_LINE obj . bar = ' foo ' NEW_LINE self . assertRaises ( AttributeError , obj . foo ) NEW_LINE DEDENT
def test_dt_deserializer ( self ) : NEW_LINE INDENT dt = timeutils . parse_isotime ( '1955-11-05T00:00:00Z ' ) NEW_LINE self . assertEqual ( utils . dt_deserializer ( None , timeutils . isotime ( dt ) ) , dt ) NEW_LINE self . assertIsNone ( utils . dt_deserializer ( None , None ) ) NEW_LINE self . assertRaises ( ValueError , utils . dt_deserializer , None , ' foo ' ) NEW_LINE DEDENT
def test_hydration_type_error ( self ) : NEW_LINE INDENT primitive = { ' magnum _ object . name ' : ' MyObj ' , ' magnum _ object . namespace ' : ' magnum ' , ' magnum _ object . version ' : '1.5' , ' magnum _ object . data ' : { ' foo ' : ' a ' } } NEW_LINE self . assertRaises ( ValueError , MyObj . obj_from_primitive , primitive ) NEW_LINE DEDENT
def test_hydration ( self ) : NEW_LINE INDENT primitive = { ' magnum _ object . name ' : ' MyObj ' , ' magnum _ object . namespace ' : ' magnum ' , ' magnum _ object . version ' : '1.5' , ' magnum _ object . data ' : { ' foo ' : 1 } } NEW_LINE obj = MyObj . obj_from_primitive ( primitive ) NEW_LINE self . assertEqual ( 1 , obj . foo ) NEW_LINE DEDENT
def test_hydration_bad_ns ( self ) : NEW_LINE INDENT primitive = { ' magnum _ object . name ' : ' MyObj ' , ' magnum _ object . namespace ' : ' foo ' , ' magnum _ object . version ' : '1.5' , ' magnum _ object . data ' : { ' foo ' : 1 } } NEW_LINE self . assertRaises ( exception . UnsupportedObjectError , MyObj . obj_from_primitive , primitive ) NEW_LINE DEDENT
def test_dehydration ( self ) : NEW_LINE INDENT expected = { ' magnum _ object . name ' : ' MyObj ' , ' magnum _ object . namespace ' : ' magnum ' , ' magnum _ object . version ' : '1.5' , ' magnum _ object . data ' : { ' foo ' : 1 } } NEW_LINE obj = MyObj ( self . context ) NEW_LINE obj . foo = 1 NEW_LINE obj . obj_reset_changes ( ) NEW_LINE self . assertEqual ( expected , obj . obj_to_primitive ( ) ) NEW_LINE DEDENT
def test_get_updates ( self ) : NEW_LINE INDENT obj = MyObj ( self . context ) NEW_LINE self . assertEqual ( { } , obj . obj_get_changes ( ) ) NEW_LINE obj . foo = 123 NEW_LINE self . assertEqual ( { ' foo ' : 123 } , obj . obj_get_changes ( ) ) NEW_LINE obj . bar = ' test ' NEW_LINE self . assertEqual ( { ' foo ' : 123 , ' bar ' : ' test ' } , obj . obj_get_changes ( ) ) NEW_LINE obj . obj_reset_changes ( ) NEW_LINE self . assertEqual ( { } , obj . obj_get_changes ( ) ) NEW_LINE DEDENT
def test_object_property ( self ) : NEW_LINE INDENT obj = MyObj ( self . context , foo = 1 ) NEW_LINE self . assertEqual ( 1 , obj . foo ) NEW_LINE DEDENT
def test_object_property_type_error ( self ) : NEW_LINE INDENT obj = MyObj ( self . context ) NEW_LINE def fail ( ) : NEW_LINE INDENT obj . foo = ' a ' NEW_LINE DEDENT self . assertRaises ( ValueError , fail ) NEW_LINE DEDENT
def test_load ( self ) : NEW_LINE INDENT obj = MyObj ( self . context ) NEW_LINE self . assertEqual ( ' loaded ! ' , obj . bar ) NEW_LINE DEDENT
def test_load_in_base ( self ) : NEW_LINE INDENT class Foo ( base . MagnumObject ) : NEW_LINE INDENT fields = { ' foobar ' : fields . IntegerField ( ) } NEW_LINE DEDENT obj = Foo ( self . context ) NEW_LINE # ▁ NOTE ( danms ) : ▁ Can ' t ▁ use ▁ assertRaisesRegexp ( ) ▁ because ▁ of ▁ py26 ENDCOM raised = False NEW_LINE try : NEW_LINE INDENT obj . foobar NEW_LINE DEDENT except NotImplementedError as ex : NEW_LINE INDENT raised = True NEW_LINE DEDENT self . assertTrue ( raised ) NEW_LINE self . assertTrue ( ' foobar ' in str ( ex ) ) NEW_LINE DEDENT
def test_loaded_in_primitive ( self ) : NEW_LINE INDENT obj = MyObj ( self . context ) NEW_LINE obj . foo = 1 NEW_LINE obj . obj_reset_changes ( ) NEW_LINE self . assertEqual ( ' loaded ! ' , obj . bar ) NEW_LINE expected = { ' magnum _ object . name ' : ' MyObj ' , ' magnum _ object . namespace ' : ' magnum ' , ' magnum _ object . version ' : '1.0' , ' magnum _ object . changes ' : [ ' bar ' ] , ' magnum _ object . data ' : { ' foo ' : 1 , ' bar ' : ' loaded ! ' } } NEW_LINE self . assertEqual ( expected , obj . obj_to_primitive ( ) ) NEW_LINE DEDENT
def test_changes_in_primitive ( self ) : NEW_LINE INDENT obj = MyObj ( self . context ) NEW_LINE obj . foo = 123 NEW_LINE self . assertEqual ( set ( [ ' foo ' ] ) , obj . obj_what_changed ( ) ) NEW_LINE primitive = obj . obj_to_primitive ( ) NEW_LINE self . assertTrue ( ' magnum _ object . changes ' in primitive ) NEW_LINE obj2 = MyObj . obj_from_primitive ( primitive ) NEW_LINE self . assertEqual ( set ( [ ' foo ' ] ) , obj2 . obj_what_changed ( ) ) NEW_LINE obj2 . obj_reset_changes ( ) NEW_LINE self . assertEqual ( set ( ) , obj2 . obj_what_changed ( ) ) NEW_LINE DEDENT
def test_unknown_objtype ( self ) : NEW_LINE INDENT self . assertRaises ( exception . UnsupportedObjectError , base . MagnumObject . obj_class_from_name , ' foo ' , '1.0' ) NEW_LINE DEDENT
def test_with_alternate_context ( self ) : NEW_LINE INDENT context1 = magnum_context . RequestContext ( ' foo ' , ' foo ' ) NEW_LINE context2 = magnum_context . RequestContext ( ' bar ' , project_id = ' alternate ' ) NEW_LINE obj = MyObj . query ( context1 ) NEW_LINE obj . update_test ( context2 ) NEW_LINE self . assertEqual ( ' alternate - context ' , obj . bar ) NEW_LINE self . assertRemotes ( ) NEW_LINE DEDENT
def test_orphaned_object ( self ) : NEW_LINE INDENT obj = MyObj . query ( self . context ) NEW_LINE obj . _context = None NEW_LINE self . assertRaises ( exception . OrphanedObjectError , obj . update_test ) NEW_LINE self . assertRemotes ( ) NEW_LINE DEDENT
def test_changed_1 ( self ) : NEW_LINE INDENT obj = MyObj . query ( self . context ) NEW_LINE obj . foo = 123 NEW_LINE self . assertEqual ( set ( [ ' foo ' ] ) , obj . obj_what_changed ( ) ) NEW_LINE obj . update_test ( self . context ) NEW_LINE self . assertEqual ( set ( [ ' foo ' , ' bar ' ] ) , obj . obj_what_changed ( ) ) NEW_LINE self . assertEqual ( 123 , obj . foo ) NEW_LINE self . assertRemotes ( ) NEW_LINE DEDENT
def test_changed_2 ( self ) : NEW_LINE INDENT obj = MyObj . query ( self . context ) NEW_LINE obj . foo = 123 NEW_LINE self . assertEqual ( set ( [ ' foo ' ] ) , obj . obj_what_changed ( ) ) NEW_LINE obj . save ( ) NEW_LINE self . assertEqual ( set ( [ ] ) , obj . obj_what_changed ( ) ) NEW_LINE self . assertEqual ( 123 , obj . foo ) NEW_LINE self . assertRemotes ( ) NEW_LINE DEDENT
def test_changed_3 ( self ) : NEW_LINE INDENT obj = MyObj . query ( self . context ) NEW_LINE obj . foo = 123 NEW_LINE self . assertEqual ( set ( [ ' foo ' ] ) , obj . obj_what_changed ( ) ) NEW_LINE obj . refresh ( ) NEW_LINE self . assertEqual ( set ( [ ] ) , obj . obj_what_changed ( ) ) NEW_LINE self . assertEqual ( 321 , obj . foo ) NEW_LINE self . assertEqual ( ' refreshed ' , obj . bar ) NEW_LINE self . assertRemotes ( ) NEW_LINE DEDENT
def test_changed_4 ( self ) : NEW_LINE INDENT obj = MyObj . query ( self . context ) NEW_LINE obj . bar = ' something ' NEW_LINE self . assertEqual ( set ( [ ' bar ' ] ) , obj . obj_what_changed ( ) ) NEW_LINE obj . modify_save_modify ( self . context ) NEW_LINE self . assertEqual ( set ( [ ' foo ' ] ) , obj . obj_what_changed ( ) ) NEW_LINE self . assertEqual ( 42 , obj . foo ) NEW_LINE self . assertEqual ( ' meow ' , obj . bar ) NEW_LINE self . assertRemotes ( ) NEW_LINE DEDENT
def test_static_result ( self ) : NEW_LINE INDENT obj = MyObj . query ( self . context ) NEW_LINE self . assertEqual ( ' bar ' , obj . bar ) NEW_LINE result = obj . marco ( ) NEW_LINE self . assertEqual ( ' polo ' , result ) NEW_LINE self . assertRemotes ( ) NEW_LINE DEDENT
def test_updates ( self ) : NEW_LINE INDENT obj = MyObj . query ( self . context ) NEW_LINE self . assertEqual ( 1 , obj . foo ) NEW_LINE obj . update_test ( ) NEW_LINE self . assertEqual ( ' updated ' , obj . bar ) NEW_LINE self . assertRemotes ( ) NEW_LINE DEDENT
def test_base_attributes ( self ) : NEW_LINE INDENT dt = datetime . datetime ( 1955 , 11 , 5 ) NEW_LINE obj = MyObj ( self . context ) NEW_LINE obj . created_at = dt NEW_LINE obj . updated_at = dt NEW_LINE expected = { ' magnum _ object . name ' : ' MyObj ' , ' magnum _ object . namespace ' : ' magnum ' , ' magnum _ object . version ' : '1.0' , ' magnum _ object . changes ' : [ ' created _ at ' , ' updated _ at ' ] , ' magnum _ object . data ' : { ' created _ at ' : timeutils . isotime ( dt ) , ' updated _ at ' : timeutils . isotime ( dt ) } } NEW_LINE actual = obj . obj_to_primitive ( ) NEW_LINE # ▁ magnum _ object . changes ▁ is ▁ built ▁ from ▁ a ▁ set ▁ and ▁ order ▁ is ▁ undefined ENDCOM self . assertEqual ( sorted ( expected [ ' magnum _ object . changes ' ] ) , sorted ( actual [ ' magnum _ object . changes ' ] ) ) NEW_LINE del expected [ ' magnum _ object . changes ' ] , actual [ ' magnum _ object . changes ' ] NEW_LINE self . assertEqual ( expected , actual ) NEW_LINE DEDENT
def test_contains ( self ) : NEW_LINE INDENT obj = MyObj ( self . context ) NEW_LINE self . assertFalse ( ' foo ' in obj ) NEW_LINE obj . foo = 1 NEW_LINE self . assertTrue ( ' foo ' in obj ) NEW_LINE self . assertFalse ( ' does _ not _ exist ' in obj ) NEW_LINE DEDENT
def test_obj_attr_is_set ( self ) : NEW_LINE INDENT obj = MyObj ( self . context , foo = 1 ) NEW_LINE self . assertTrue ( obj . obj_attr_is_set ( ' foo ' ) ) NEW_LINE self . assertFalse ( obj . obj_attr_is_set ( ' bar ' ) ) NEW_LINE self . assertRaises ( AttributeError , obj . obj_attr_is_set , ' bang ' ) NEW_LINE DEDENT
def test_get ( self ) : NEW_LINE INDENT obj = MyObj ( self . context , foo = 1 ) NEW_LINE # ▁ Foo ▁ has ▁ value , ▁ should ▁ not ▁ get ▁ the ▁ default ENDCOM self . assertEqual ( obj . get ( ' foo ' , 2 ) , 1 ) NEW_LINE # ▁ Foo ▁ has ▁ value , ▁ should ▁ return ▁ the ▁ value ▁ without ▁ error ENDCOM self . assertEqual ( obj . get ( ' foo ' ) , 1 ) NEW_LINE # ▁ Bar ▁ is ▁ not ▁ loaded , ▁ so ▁ we ▁ should ▁ get ▁ the ▁ default ENDCOM self . assertEqual ( obj . get ( ' bar ' , ' not - loaded ' ) , ' not - loaded ' ) NEW_LINE # ▁ Bar ▁ without ▁ a ▁ default ▁ should ▁ lazy - load ENDCOM self . assertEqual ( obj . get ( ' bar ' ) , ' loaded ! ' ) NEW_LINE # ▁ Bar ▁ now ▁ has ▁ a ▁ default , ▁ but ▁ loaded ▁ value ▁ should ▁ be ▁ returned ENDCOM self . assertEqual ( obj . get ( ' bar ' , ' not - loaded ' ) , ' loaded ! ' ) NEW_LINE # ▁ Invalid ▁ attribute ▁ should ▁ raise ▁ AttributeError ENDCOM self . assertRaises ( AttributeError , obj . get , ' nothing ' ) NEW_LINE # ▁ . . . even ▁ with ▁ a ▁ default ENDCOM self . assertRaises ( AttributeError , obj . get , ' nothing ' , 3 ) NEW_LINE DEDENT
def test_object_inheritance ( self ) : NEW_LINE INDENT base_fields = list ( base . MagnumObject . fields . keys ( ) ) NEW_LINE myobj_fields = [ ' foo ' , ' bar ' , ' missing ' ] + base_fields NEW_LINE myobj3_fields = [ ' new _ field ' ] NEW_LINE self . assertTrue ( issubclass ( TestSubclassedObject , MyObj ) ) NEW_LINE self . assertEqual ( len ( myobj_fields ) , len ( MyObj . fields ) ) NEW_LINE self . assertEqual ( set ( myobj_fields ) , set ( MyObj . fields . keys ( ) ) ) NEW_LINE self . assertEqual ( len ( myobj_fields ) + len ( myobj3_fields ) , len ( TestSubclassedObject . fields ) ) NEW_LINE self . assertEqual ( set ( myobj_fields ) | set ( myobj3_fields ) , set ( TestSubclassedObject . fields . keys ( ) ) ) NEW_LINE DEDENT
def test_get_changes ( self ) : NEW_LINE INDENT obj = MyObj ( self . context ) NEW_LINE self . assertEqual ( { } , obj . obj_get_changes ( ) ) NEW_LINE obj . foo = 123 NEW_LINE self . assertEqual ( { ' foo ' : 123 } , obj . obj_get_changes ( ) ) NEW_LINE obj . bar = ' test ' NEW_LINE self . assertEqual ( { ' foo ' : 123 , ' bar ' : ' test ' } , obj . obj_get_changes ( ) ) NEW_LINE obj . obj_reset_changes ( ) NEW_LINE self . assertEqual ( { } , obj . obj_get_changes ( ) ) NEW_LINE DEDENT
def test_obj_fields ( self ) : NEW_LINE INDENT class TestObj ( base . MagnumObject ) : NEW_LINE INDENT fields = { ' foo ' : fields . IntegerField ( ) } NEW_LINE obj_extra_fields = [ ' bar ' ] NEW_LINE @ property NEW_LINE def bar ( self ) : NEW_LINE INDENT return ' this ▁ is ▁ bar ' NEW_LINE DEDENT DEDENT obj = TestObj ( self . context ) NEW_LINE self . assertEqual ( set ( [ ' created _ at ' , ' updated _ at ' , ' foo ' , ' bar ' ] ) , set ( obj . obj_fields ) ) NEW_LINE DEDENT
def test_obj_constructor ( self ) : NEW_LINE INDENT obj = MyObj ( self . context , foo = 123 , bar = ' abc ' ) NEW_LINE self . assertEqual ( 123 , obj . foo ) NEW_LINE self . assertEqual ( ' abc ' , obj . bar ) NEW_LINE self . assertEqual ( set ( [ ' foo ' , ' bar ' ] ) , obj . obj_what_changed ( ) ) NEW_LINE DEDENT
def test_object_serialization ( self ) : NEW_LINE INDENT ser = base . MagnumObjectSerializer ( ) NEW_LINE obj = MyObj ( self . context ) NEW_LINE primitive = ser . serialize_entity ( self . context , obj ) NEW_LINE self . assertTrue ( ' magnum _ object . name ' in primitive ) NEW_LINE obj2 = ser . deserialize_entity ( self . context , primitive ) NEW_LINE self . assertIsInstance ( obj2 , MyObj ) NEW_LINE self . assertEqual ( self . context , obj2 . _context ) NEW_LINE DEDENT
def test_object_serialization_iterables ( self ) : NEW_LINE INDENT ser = base . MagnumObjectSerializer ( ) NEW_LINE obj = MyObj ( self . context ) NEW_LINE for iterable in ( list , tuple , set ) : NEW_LINE INDENT thing = iterable ( [ obj ] ) NEW_LINE primitive = ser . serialize_entity ( self . context , thing ) NEW_LINE self . assertEqual ( 1 , len ( primitive ) ) NEW_LINE for item in primitive : NEW_LINE INDENT self . assertFalse ( isinstance ( item , base . MagnumObject ) ) NEW_LINE DEDENT thing2 = ser . deserialize_entity ( self . context , primitive ) NEW_LINE self . assertEqual ( 1 , len ( thing2 ) ) NEW_LINE for item in thing2 : NEW_LINE INDENT self . assertIsInstance ( item , MyObj ) NEW_LINE DEDENT DEDENT DEDENT
def test_order ( self ) : NEW_LINE INDENT """ STRNEWLINE ▁ Verify ▁ that ▁ drafts ▁ are ▁ imported ▁ in ▁ the ▁ correct ▁ order . STRNEWLINE ▁ """ NEW_LINE store = modulestore ( ) NEW_LINE course_items = import_course_from_xml ( store , self . user . id , TEST_DATA_DIR , [ ' import _ draft _ order ' ] , create_if_not_present = True ) NEW_LINE course_key = course_items [ 0 ] . id NEW_LINE sequential = store . get_item ( course_key . make_usage_key ( ' sequential ' , '0f4f7649b10141b0bdc9922dcf94515a ' ) ) NEW_LINE verticals = sequential . children NEW_LINE # ▁ The ▁ order ▁ that ▁ files ▁ are ▁ read ▁ in ▁ from ▁ the ▁ file ▁ system ▁ is ▁ not ▁ guaranteed ▁ ( cannot ▁ rely ▁ on ENDCOM # ▁ alphabetical ▁ ordering , ▁ for ▁ example ) . ▁ Therefore , ▁ I ▁ have ▁ added ▁ a ▁ lot ▁ of ▁ variation ▁ in ▁ filename ▁ and ▁ desired ENDCOM # ▁ ordering ▁ so ▁ that ▁ the ▁ test ▁ reliably ▁ failed ▁ with ▁ the ▁ bug , ▁ at ▁ least ▁ on ▁ Linux . ENDCOM # ▁ ' a ' , ▁ ' b ' , ▁ ' c ' , ▁ ' d ' , ▁ and ▁ ' z ' ▁ are ▁ all ▁ drafts , ▁ with ▁ ' index _ in _ children _ list ' ▁ of ENDCOM # ▁ 2 ▁ , ▁ 4 ▁ , ▁ 6 ▁ , ▁ 5 ▁ , ▁ and ▁ 0 ▁ respectively . ENDCOM # ▁ ' 5a05be9d59fc4bb79282c94c9e6b88c7 ' ▁ and ▁ ' second ' ▁ are ▁ public ▁ verticals . ENDCOM self . assertEqual ( 7 , len ( verticals ) ) NEW_LINE self . assertEqual ( course_key . make_usage_key ( ' vertical ' , ' z ' ) , verticals [ 0 ] ) NEW_LINE self . assertEqual ( course_key . make_usage_key ( ' vertical ' , '5a05be9d59fc4bb79282c94c9e6b88c7' ) , verticals [ 1 ] ) NEW_LINE self . assertEqual ( course_key . make_usage_key ( ' vertical ' , ' a ' ) , verticals [ 2 ] ) NEW_LINE self . assertEqual ( course_key . make_usage_key ( ' vertical ' , ' second ' ) , verticals [ 3 ] ) NEW_LINE self . assertEqual ( course_key . make_usage_key ( ' vertical ' , ' b ' ) , verticals [ 4 ] ) NEW_LINE self . assertEqual ( course_key . make_usage_key ( ' vertical ' , ' d ' ) , verticals [ 5 ] ) NEW_LINE self . assertEqual ( course_key . make_usage_key ( ' vertical ' , ' c ' ) , verticals [ 6 ] ) NEW_LINE # ▁ Now ▁ also ▁ test ▁ that ▁ the ▁ verticals ▁ in ▁ a ▁ second ▁ sequential ▁ are ▁ correct . ENDCOM sequential = store . get_item ( course_key . make_usage_key ( ' sequential ' , ' secondseq ' ) ) NEW_LINE verticals = sequential . children NEW_LINE # ▁ ' asecond ' ▁ and ▁ ' zsecond ' ▁ are ▁ drafts ▁ with ▁ ' index _ in _ children _ list ' ▁ 0 ▁ and ▁ 2 , ▁ respectively . ENDCOM # ▁ ' secondsubsection ' ▁ is ▁ a ▁ public ▁ vertical . ENDCOM self . assertEqual ( 3 , len ( verticals ) ) NEW_LINE self . assertEqual ( course_key . make_usage_key ( ' vertical ' , ' asecond ' ) , verticals [ 0 ] ) NEW_LINE self . assertEqual ( course_key . make_usage_key ( ' vertical ' , ' secondsubsection ' ) , verticals [ 1 ] ) NEW_LINE self . assertEqual ( course_key . make_usage_key ( ' vertical ' , ' zsecond ' ) , verticals [ 2 ] ) NEW_LINE DEDENT
def get_geometry_type ( self , table_name , geo_col ) : NEW_LINE INDENT cursor = self . connection . cursor ( ) NEW_LINE try : NEW_LINE # ▁ In ▁ order ▁ to ▁ get ▁ the ▁ specific ▁ geometry ▁ type ▁ of ▁ the ▁ field , ENDCOM # ▁ we ▁ introspect ▁ on ▁ the ▁ table ▁ definition ▁ using ▁ ` DESCRIBE ` . ENDCOM INDENT cursor . execute ( ' DESCRIBE ▁ % s ' % self . connection . ops . quote_name ( table_name ) ) NEW_LINE # ▁ Increment ▁ over ▁ description ▁ info ▁ until ▁ we ▁ get ▁ to ▁ the ▁ geometry ENDCOM # ▁ column . ENDCOM for column , typ , null , key , default , extra in cursor . fetchall ( ) : NEW_LINE INDENT if column == geo_col : NEW_LINE # ▁ Using ▁ OGRGeomType ▁ to ▁ convert ▁ from ▁ OGC ▁ name ▁ to ▁ Django ▁ field . ENDCOM # ▁ MySQL ▁ does ▁ not ▁ support ▁ 3D ▁ or ▁ SRIDs , ▁ so ▁ the ▁ field ▁ params ENDCOM # ▁ are ▁ empty . ENDCOM INDENT field_type = OGRGeomType ( typ ) . django NEW_LINE field_params = { } NEW_LINE break NEW_LINE DEDENT DEDENT DEDENT finally : NEW_LINE INDENT cursor . close ( ) NEW_LINE DEDENT return field_type , field_params NEW_LINE DEDENT
def supports_spatial_index ( self , cursor , table_name ) : NEW_LINE # ▁ Supported ▁ with ▁ MyISAM , ▁ or ▁ InnoDB ▁ on ▁ MySQL ▁ 5.7.5 + ENDCOM INDENT storage_engine = self . get_storage_engine ( cursor , table_name ) NEW_LINE return ( ( storage_engine == ' InnoDB ' and self . connection . mysql_version >= ( 5 , 7 , 5 ) ) or storage_engine == ' MyISAM ' ) NEW_LINE DEDENT
def __init__ ( self , endog , exog , bw , var_type , fform , estimator , nboot = 100 ) : NEW_LINE INDENT self . endog = endog NEW_LINE self . exog = exog NEW_LINE self . var_type = var_type NEW_LINE self . fform = fform NEW_LINE self . estimator = estimator NEW_LINE self . nboot = nboot NEW_LINE self . bw = KDEMultivariate ( exog , bw = bw , var_type = var_type ) . bw NEW_LINE self . sig = self . _compute_sig ( ) NEW_LINE DEDENT
def _compute_sig ( self ) : NEW_LINE INDENT Y = self . endog NEW_LINE X = self . exog NEW_LINE b = self . estimator ( Y , X ) NEW_LINE m = self . fform ( X , b ) NEW_LINE n = np . shape ( X ) [ 0 ] NEW_LINE resid = Y - m NEW_LINE resid = resid - np . mean ( resid ) # ▁ center ▁ residuals ENDCOM NEW_LINE self . test_stat = self . _compute_test_stat ( resid ) NEW_LINE sqrt5 = np . sqrt ( 5. ) NEW_LINE fct1 = ( 1 - sqrt5 ) / 2. NEW_LINE fct2 = ( 1 + sqrt5 ) / 2. NEW_LINE u1 = fct1 * resid NEW_LINE u2 = fct2 * resid NEW_LINE r = fct2 / sqrt5 NEW_LINE I_dist = np . empty ( ( self . nboot , 1 ) ) NEW_LINE for j in range ( self . nboot ) : NEW_LINE INDENT u_boot = u2 . copy ( ) NEW_LINE prob = np . random . uniform ( 0 , 1 , size = ( n , ) ) NEW_LINE ind = prob < r NEW_LINE u_boot [ ind ] = u1 [ ind ] NEW_LINE Y_boot = m + u_boot NEW_LINE b_hat = self . estimator ( Y_boot , X ) NEW_LINE m_hat = self . fform ( X , b_hat ) NEW_LINE u_boot_hat = Y_boot - m_hat NEW_LINE I_dist [ j ] = self . _compute_test_stat ( u_boot_hat ) NEW_LINE DEDENT self . boots_results = I_dist NEW_LINE sig = " Not ▁ Significant " NEW_LINE if self . test_stat > mquantiles ( I_dist , 0.9 ) : NEW_LINE INDENT sig = " * " NEW_LINE DEDENT if self . test_stat > mquantiles ( I_dist , 0.95 ) : NEW_LINE INDENT sig = " * * " NEW_LINE DEDENT if self . test_stat > mquantiles ( I_dist , 0.99 ) : NEW_LINE INDENT sig = " * * * " NEW_LINE DEDENT return sig NEW_LINE DEDENT
def _compute_test_stat ( self , u ) : NEW_LINE INDENT n = np . shape ( u ) [ 0 ] NEW_LINE XLOO = LeaveOneOut ( self . exog ) NEW_LINE uLOO = LeaveOneOut ( u [ : , None ] ) . __iter__ ( ) NEW_LINE I = 0 NEW_LINE S2 = 0 NEW_LINE for i , X_not_i in enumerate ( XLOO ) : NEW_LINE INDENT u_j = next ( uLOO ) NEW_LINE u_j = np . squeeze ( u_j ) NEW_LINE # ▁ See ▁ Bootstrapping ▁ procedure ▁ on ▁ p . ▁ 357 ▁ in ▁ [ 1 ] ENDCOM K = gpke ( self . bw , data = - X_not_i , data_predict = - self . exog [ i , : ] , var_type = self . var_type , tosum = False ) NEW_LINE f_i = ( u [ i ] * u_j * K ) NEW_LINE assert u_j . shape == K . shape NEW_LINE I += f_i . sum ( ) # ▁ See ▁ eq . ▁ 12.7 ▁ on ▁ p . ▁ 355 ▁ in ▁ [ 1 ] ENDCOM NEW_LINE S2 += ( f_i ** 2 ) . sum ( ) # ▁ See ▁ Theorem ▁ 12.1 ▁ on ▁ p . 356 ▁ in ▁ [ 1 ] ENDCOM NEW_LINE assert np . size ( I ) == 1 NEW_LINE assert np . size ( S2 ) == 1 NEW_LINE DEDENT I *= 1. / ( n * ( n - 1 ) ) NEW_LINE ix_cont = _get_type_pos ( self . var_type ) [ 0 ] NEW_LINE hp = self . bw [ ix_cont ] . prod ( ) NEW_LINE S2 *= 2 * hp / ( n * ( n - 1 ) ) NEW_LINE T = n * I * np . sqrt ( hp / S2 ) NEW_LINE return T NEW_LINE DEDENT
def __init__ ( self , endog , exog , var_type ) : NEW_LINE INDENT self . var_type = var_type NEW_LINE self . K = len ( var_type ) NEW_LINE self . var_type = self . var_type [ 0 ] NEW_LINE self . endog = _adjust_shape ( endog , 1 ) NEW_LINE self . exog = _adjust_shape ( exog , self . K ) NEW_LINE self . nobs = np . shape ( self . exog ) [ 0 ] NEW_LINE self . data_type = self . var_type NEW_LINE self . func = self . _est_loc_linear NEW_LINE self . b , self . bw = self . _est_b_bw ( ) NEW_LINE DEDENT
def _est_b_bw ( self ) : NEW_LINE INDENT params0 = np . random . uniform ( size = ( self . K + 1 , ) ) NEW_LINE b_bw = optimize . fmin ( self . cv_loo , params0 , disp = 0 ) NEW_LINE b = b_bw [ 0 : self . K ] NEW_LINE bw = b_bw [ self . K : ] NEW_LINE bw = self . _set_bw_bounds ( bw ) NEW_LINE return b , bw NEW_LINE DEDENT
def fit ( self , data_predict = None ) : NEW_LINE INDENT if data_predict is None : NEW_LINE INDENT data_predict = self . exog NEW_LINE DEDENT else : NEW_LINE INDENT data_predict = _adjust_shape ( data_predict , self . K ) NEW_LINE DEDENT N_data_predict = np . shape ( data_predict ) [ 0 ] NEW_LINE mean = np . empty ( ( N_data_predict , ) ) NEW_LINE mfx = np . empty ( ( N_data_predict , self . K ) ) NEW_LINE for i in range ( N_data_predict ) : NEW_LINE INDENT mean_mfx = self . func ( self . bw , self . endog , np . dot ( self . exog , self . b ) [ : , None ] , data_predict = np . dot ( data_predict [ i : i + 1 , : ] , self . b ) ) NEW_LINE mean [ i ] = mean_mfx [ 0 ] NEW_LINE mfx_c = np . squeeze ( mean_mfx [ 1 ] ) NEW_LINE mfx [ i , : ] = mfx_c NEW_LINE DEDENT return mean , mfx NEW_LINE DEDENT
def __init__ ( self , endog , exog , exog_nonparametric , var_type , k_linear ) : NEW_LINE INDENT self . endog = _adjust_shape ( endog , 1 ) NEW_LINE self . exog = _adjust_shape ( exog , k_linear ) NEW_LINE self . K = len ( var_type ) NEW_LINE self . exog_nonparametric = _adjust_shape ( exog_nonparametric , self . K ) NEW_LINE self . k_linear = k_linear NEW_LINE self . nobs = np . shape ( self . exog ) [ 0 ] NEW_LINE self . var_type = var_type NEW_LINE self . data_type = self . var_type NEW_LINE self . func = self . _est_loc_linear NEW_LINE self . b , self . bw = self . _est_b_bw ( ) NEW_LINE DEDENT
def _est_b_bw ( self ) : NEW_LINE INDENT """ STRNEWLINE ▁ Computes ▁ the ▁ ( beta ) ▁ coefficients ▁ and ▁ the ▁ bandwidths . STRNEWLINE STRNEWLINE ▁ Minimizes ▁ ` ` cv _ loo ` ` ▁ with ▁ respect ▁ to ▁ ` ` b ` ` ▁ and ▁ ` ` bw ` ` . STRNEWLINE ▁ """ NEW_LINE params0 = np . random . uniform ( size = ( self . k_linear + self . K , ) ) NEW_LINE b_bw = optimize . fmin ( self . cv_loo , params0 , disp = 0 ) NEW_LINE b = b_bw [ 0 : self . k_linear ] NEW_LINE bw = b_bw [ self . k_linear : ] NEW_LINE # bw ▁ = ▁ self . _ set _ bw _ bounds ( np . asarray ( bw ) ) ENDCOM return b , bw NEW_LINE DEDENT
def cv_loo ( self , params ) : NEW_LINE INDENT """ STRNEWLINE ▁ Similar ▁ to ▁ the ▁ cross ▁ validation ▁ leave - one - out ▁ estimator . STRNEWLINE STRNEWLINE ▁ Modified ▁ to ▁ reflect ▁ the ▁ linear ▁ components . STRNEWLINE STRNEWLINE ▁ Parameters STRNEWLINE ▁ - - - - - STRNEWLINE ▁ params : ▁ array _ like STRNEWLINE ▁ Vector ▁ consisting ▁ of ▁ the ▁ coefficients ▁ ( b ) ▁ and ▁ the ▁ bandwidths ▁ ( bw ) . STRNEWLINE ▁ The ▁ first ▁ ` ` k _ linear ` ` ▁ elements ▁ are ▁ the ▁ coefficients . STRNEWLINE STRNEWLINE ▁ Returns STRNEWLINE ▁ - - - - - STRNEWLINE ▁ L : ▁ float STRNEWLINE ▁ The ▁ value ▁ of ▁ the ▁ objective ▁ function STRNEWLINE STRNEWLINE ▁ References STRNEWLINE ▁ - - - - - STRNEWLINE ▁ See ▁ p . 254 ▁ in ▁ [ 1 ] STRNEWLINE ▁ """ NEW_LINE params = np . asarray ( params ) NEW_LINE b = params [ 0 : self . k_linear ] NEW_LINE bw = params [ self . k_linear : ] NEW_LINE LOO_X = LeaveOneOut ( self . exog ) NEW_LINE LOO_Y = LeaveOneOut ( self . endog ) . __iter__ ( ) NEW_LINE LOO_Z = LeaveOneOut ( self . exog_nonparametric ) . __iter__ ( ) NEW_LINE Xb = np . dot ( self . exog , b ) [ : , None ] NEW_LINE L = 0 NEW_LINE for ii , X_not_i in enumerate ( LOO_X ) : NEW_LINE INDENT Y = next ( LOO_Y ) NEW_LINE Z = next ( LOO_Z ) NEW_LINE Xb_j = np . dot ( X_not_i , b ) [ : , None ] NEW_LINE Yx = Y - Xb_j NEW_LINE G = self . func ( bw , endog = Yx , exog = - Z , data_predict = - self . exog_nonparametric [ ii , : ] ) [ 0 ] NEW_LINE lt = Xb [ ii , : ] # . sum ( ) ▁ # ▁ linear ▁ term ENDCOM NEW_LINE L += ( self . endog [ ii ] - lt - G ) ** 2 NEW_LINE DEDENT return L NEW_LINE DEDENT
def fit ( self , exog_predict = None , exog_nonparametric_predict = None ) : NEW_LINE INDENT """ Computes ▁ fitted ▁ values ▁ and ▁ marginal ▁ effects """ NEW_LINE if exog_predict is None : NEW_LINE INDENT exog_predict = self . exog NEW_LINE DEDENT else : NEW_LINE INDENT exog_predict = _adjust_shape ( exog_predict , self . k_linear ) NEW_LINE DEDENT if exog_nonparametric_predict is None : NEW_LINE INDENT exog_nonparametric_predict = self . exog_nonparametric NEW_LINE DEDENT else : NEW_LINE INDENT exog_nonparametric_predict = _adjust_shape ( exog_nonparametric_predict , self . K ) NEW_LINE DEDENT N_data_predict = np . shape ( exog_nonparametric_predict ) [ 0 ] NEW_LINE mean = np . empty ( ( N_data_predict , ) ) NEW_LINE mfx = np . empty ( ( N_data_predict , self . K ) ) NEW_LINE Y = self . endog - np . dot ( exog_predict , self . b ) [ : , None ] NEW_LINE for i in range ( N_data_predict ) : NEW_LINE INDENT mean_mfx = self . func ( self . bw , Y , self . exog_nonparametric , data_predict = exog_nonparametric_predict [ i , : ] ) NEW_LINE mean [ i ] = mean_mfx [ 0 ] NEW_LINE mfx_c = np . squeeze ( mean_mfx [ 1 ] ) NEW_LINE mfx [ i , : ] = mfx_c NEW_LINE DEDENT return mean , mfx NEW_LINE DEDENT
def _load_library ( self ) : NEW_LINE INDENT if self . library is not None : NEW_LINE INDENT if isinstance ( self . library , ( tuple , list ) ) : NEW_LINE INDENT name , mod_path = self . library NEW_LINE DEDENT else : NEW_LINE INDENT mod_path = self . library NEW_LINE DEDENT try : NEW_LINE INDENT module = importlib . import_module ( mod_path ) NEW_LINE DEDENT except ImportError as e : NEW_LINE INDENT raise ValueError ( " Couldn ' t ▁ load ▁ % r ▁ algorithm ▁ library : ▁ % s " % ( self . __class__ . __name__ , e ) ) NEW_LINE DEDENT return module NEW_LINE DEDENT raise ValueError ( " Hasher ▁ % r ▁ doesn ' t ▁ specify ▁ a ▁ library ▁ attribute " % self . __class__ . __name__ ) NEW_LINE DEDENT
def salt ( self ) : NEW_LINE INDENT """ Generate ▁ a ▁ cryptographically ▁ secure ▁ nonce ▁ salt ▁ in ▁ ASCII . """ NEW_LINE return get_random_string ( ) NEW_LINE DEDENT
def verify ( self , password , encoded ) : NEW_LINE INDENT """ Check ▁ if ▁ the ▁ given ▁ password ▁ is ▁ correct . """ NEW_LINE raise NotImplementedError ( ' subclasses ▁ of ▁ BasePasswordHasher ▁ must ▁ provide ▁ a ▁ verify ( ) ▁ method ' ) NEW_LINE DEDENT
def encode ( self , password , salt ) : NEW_LINE INDENT """ STRNEWLINE ▁ Create ▁ an ▁ encoded ▁ database ▁ value . STRNEWLINE STRNEWLINE ▁ The ▁ result ▁ is ▁ normally ▁ formatted ▁ as ▁ " algorithm $ salt $ hash " ▁ and STRNEWLINE ▁ must ▁ be ▁ fewer ▁ than ▁ 128 ▁ characters . STRNEWLINE ▁ """ NEW_LINE raise NotImplementedError ( ' subclasses ▁ of ▁ BasePasswordHasher ▁ must ▁ provide ▁ an ▁ encode ( ) ▁ method ' ) NEW_LINE DEDENT
def safe_summary ( self , encoded ) : NEW_LINE INDENT """ STRNEWLINE ▁ Return ▁ a ▁ summary ▁ of ▁ safe ▁ values . STRNEWLINE STRNEWLINE ▁ The ▁ result ▁ is ▁ a ▁ dictionary ▁ and ▁ will ▁ be ▁ used ▁ where ▁ the ▁ password ▁ field STRNEWLINE ▁ must ▁ be ▁ displayed ▁ to ▁ construct ▁ a ▁ safe ▁ representation ▁ of ▁ the ▁ password . STRNEWLINE ▁ """ NEW_LINE raise NotImplementedError ( ' subclasses ▁ of ▁ BasePasswordHasher ▁ must ▁ provide ▁ a ▁ safe _ summary ( ) ▁ method ' ) NEW_LINE DEDENT
def must_update ( self , encoded ) : NEW_LINE INDENT return False NEW_LINE DEDENT
def harden_runtime ( self , password , encoded ) : NEW_LINE INDENT """ STRNEWLINE ▁ Bridge ▁ the ▁ runtime ▁ gap ▁ between ▁ the ▁ work ▁ factor ▁ supplied ▁ in ▁ ` encoded ` STRNEWLINE ▁ and ▁ the ▁ work ▁ factor ▁ suggested ▁ by ▁ this ▁ hasher . STRNEWLINE STRNEWLINE ▁ Taking ▁ PBKDF2 ▁ as ▁ an ▁ example , ▁ if ▁ ` encoded ` ▁ contains ▁ 20000 ▁ iterations ▁ and STRNEWLINE ▁ ` self . iterations ` ▁ is ▁ 30000 , ▁ this ▁ method ▁ should ▁ run ▁ password ▁ through STRNEWLINE ▁ another ▁ 10000 ▁ iterations ▁ of ▁ PBKDF2 . ▁ Similar ▁ approaches ▁ should ▁ exist STRNEWLINE ▁ for ▁ any ▁ hasher ▁ that ▁ has ▁ a ▁ work ▁ factor . ▁ If ▁ not , ▁ this ▁ method ▁ should ▁ be STRNEWLINE ▁ defined ▁ as ▁ a ▁ no - op ▁ to ▁ silence ▁ the ▁ warning . STRNEWLINE ▁ """ NEW_LINE warnings . warn ( ' subclasses ▁ of ▁ BasePasswordHasher ▁ should ▁ provide ▁ a ▁ harden _ runtime ( ) ▁ method ' ) NEW_LINE DEDENT
def encode ( self , password , salt , iterations = None ) : NEW_LINE INDENT assert password is not None NEW_LINE assert salt and ' $ ' not in salt NEW_LINE if not iterations : NEW_LINE INDENT iterations = self . iterations NEW_LINE DEDENT hash = pbkdf2 ( password , salt , iterations , digest = self . digest ) NEW_LINE hash = base64 . b64encode ( hash ) . decode ( ' ascii ' ) . strip ( ) NEW_LINE return " % s $ % d $ % s $ % s " % ( self . algorithm , iterations , salt , hash ) NEW_LINE DEDENT
def verify ( self , password , encoded ) : NEW_LINE INDENT algorithm , iterations , salt , hash = encoded . split ( ' $ ' , 3 ) NEW_LINE assert algorithm == self . algorithm NEW_LINE encoded_2 = self . encode ( password , salt , int ( iterations ) ) NEW_LINE return constant_time_compare ( encoded , encoded_2 ) NEW_LINE DEDENT
def safe_summary ( self , encoded ) : NEW_LINE INDENT algorithm , iterations , salt , hash = encoded . split ( ' $ ' , 3 ) NEW_LINE assert algorithm == self . algorithm NEW_LINE return OrderedDict ( [ ( _ ( ' algorithm ' ) , algorithm ) , ( _ ( ' iterations ' ) , iterations ) , ( _ ( ' salt ' ) , mask_hash ( salt ) ) , ( _ ( ' hash ' ) , mask_hash ( hash ) ) , ] ) NEW_LINE DEDENT
def must_update ( self , encoded ) : NEW_LINE INDENT algorithm , iterations , salt , hash = encoded . split ( ' $ ' , 3 ) NEW_LINE return int ( iterations ) != self . iterations NEW_LINE DEDENT
def harden_runtime ( self , password , encoded ) : NEW_LINE INDENT algorithm , iterations , salt , hash = encoded . split ( ' $ ' , 3 ) NEW_LINE extra_iterations = self . iterations - int ( iterations ) NEW_LINE if extra_iterations > 0 : NEW_LINE INDENT self . encode ( password , salt , extra_iterations ) NEW_LINE DEDENT DEDENT
def encode ( self , password , salt ) : NEW_LINE INDENT argon2 = self . _load_library ( ) NEW_LINE data = argon2 . low_level . hash_secret ( force_bytes ( password ) , force_bytes ( salt ) , time_cost = self . time_cost , memory_cost = self . memory_cost , parallelism = self . parallelism , hash_len = argon2 . DEFAULT_HASH_LENGTH , type = argon2 . low_level . Type . I , ) NEW_LINE return self . algorithm + data . decode ( ' ascii ' ) NEW_LINE DEDENT
def verify ( self , password , encoded ) : NEW_LINE INDENT argon2 = self . _load_library ( ) NEW_LINE algorithm , rest = encoded . split ( ' $ ' , 1 ) NEW_LINE assert algorithm == self . algorithm NEW_LINE try : NEW_LINE INDENT return argon2 . low_level . verify_secret ( force_bytes ( ' $ ' + rest ) , force_bytes ( password ) , type = argon2 . low_level . Type . I , ) NEW_LINE DEDENT except argon2 . exceptions . VerificationError : NEW_LINE INDENT return False NEW_LINE DEDENT DEDENT
def safe_summary ( self , encoded ) : NEW_LINE INDENT ( algorithm , variety , version , time_cost , memory_cost , parallelism , salt , data ) = self . _decode ( encoded ) NEW_LINE assert algorithm == self . algorithm NEW_LINE return OrderedDict ( [ ( _ ( ' algorithm ' ) , algorithm ) , ( _ ( ' variety ' ) , variety ) , ( _ ( ' version ' ) , version ) , ( _ ( ' memory ▁ cost ' ) , memory_cost ) , ( _ ( ' time ▁ cost ' ) , time_cost ) , ( _ ( ' parallelism ' ) , parallelism ) , ( _ ( ' salt ' ) , mask_hash ( salt ) ) , ( _ ( ' hash ' ) , mask_hash ( data ) ) , ] ) NEW_LINE DEDENT
def must_update ( self , encoded ) : NEW_LINE INDENT ( algorithm , variety , version , time_cost , memory_cost , parallelism , salt , data ) = self . _decode ( encoded ) NEW_LINE assert algorithm == self . algorithm NEW_LINE argon2 = self . _load_library ( ) NEW_LINE return ( argon2 . low_level . ARGON2_VERSION != version or self . time_cost != time_cost or self . memory_cost != memory_cost or self . parallelism != parallelism ) NEW_LINE DEDENT
def harden_runtime ( self , password , encoded ) : NEW_LINE # ▁ The ▁ runtime ▁ for ▁ Argon2 ▁ is ▁ too ▁ complicated ▁ to ▁ implement ▁ a ▁ sensible ENDCOM # ▁ hardening ▁ algorithm . ENDCOM INDENT pass NEW_LINE DEDENT
def _decode ( self , encoded ) : NEW_LINE INDENT """ STRNEWLINE ▁ Split ▁ an ▁ encoded ▁ hash ▁ and ▁ return : ▁ ( STRNEWLINE ▁ algorithm , ▁ variety , ▁ version , ▁ time _ cost , ▁ memory _ cost , STRNEWLINE ▁ parallelism , ▁ salt , ▁ data , STRNEWLINE ▁ ) . STRNEWLINE ▁ """ NEW_LINE bits = encoded . split ( ' $ ' ) NEW_LINE if len ( bits ) == 5 : NEW_LINE # ▁ Argon2 ▁ < ▁ 1.3 ENDCOM INDENT algorithm , variety , raw_params , salt , data = bits NEW_LINE version = 0x10 NEW_LINE DEDENT else : NEW_LINE INDENT assert len ( bits ) == 6 NEW_LINE algorithm , variety , raw_version , raw_params , salt , data = bits NEW_LINE assert raw_version . startswith ( ' v = ' ) NEW_LINE version = int ( raw_version [ len ( ' v = ' ) : ] ) NEW_LINE DEDENT params = dict ( bit . split ( ' = ' , 1 ) for bit in raw_params . split ( ' , ' ) ) NEW_LINE assert len ( params ) == 3 and all ( x in params for x in ( ' t ' , ' m ' , ' p ' ) ) NEW_LINE time_cost = int ( params [ ' t ' ] ) NEW_LINE memory_cost = int ( params [ ' m ' ] ) NEW_LINE parallelism = int ( params [ ' p ' ] ) NEW_LINE return ( algorithm , variety , version , time_cost , memory_cost , parallelism , salt , data , ) NEW_LINE DEDENT
def salt ( self ) : NEW_LINE INDENT bcrypt = self . _load_library ( ) NEW_LINE return bcrypt . gensalt ( self . rounds ) NEW_LINE DEDENT
def encode ( self , password , salt ) : NEW_LINE INDENT bcrypt = self . _load_library ( ) NEW_LINE # ▁ Hash ▁ the ▁ password ▁ prior ▁ to ▁ using ▁ bcrypt ▁ to ▁ prevent ▁ password ENDCOM # ▁ truncation ▁ as ▁ described ▁ in ▁ # 20138 . ENDCOM if self . digest is not None : NEW_LINE # ▁ Use ▁ binascii . hexlify ( ) ▁ because ▁ a ▁ hex ▁ encoded ▁ bytestring ▁ is ▁ str . ENDCOM INDENT password = binascii . hexlify ( self . digest ( force_bytes ( password ) ) . digest ( ) ) NEW_LINE DEDENT else : NEW_LINE INDENT password = force_bytes ( password ) NEW_LINE DEDENT data = bcrypt . hashpw ( password , salt ) NEW_LINE return " % s $ % s " % ( self . algorithm , force_text ( data ) ) NEW_LINE DEDENT
def verify ( self , password , encoded ) : NEW_LINE INDENT algorithm , data = encoded . split ( ' $ ' , 1 ) NEW_LINE assert algorithm == self . algorithm NEW_LINE encoded_2 = self . encode ( password , force_bytes ( data ) ) NEW_LINE return constant_time_compare ( encoded , encoded_2 ) NEW_LINE DEDENT
def safe_summary ( self , encoded ) : NEW_LINE INDENT algorithm , empty , algostr , work_factor , data = encoded . split ( ' $ ' , 4 ) NEW_LINE assert algorithm == self . algorithm NEW_LINE salt , checksum = data [ : 22 ] , data [ 22 : ] NEW_LINE return OrderedDict ( [ ( _ ( ' algorithm ' ) , algorithm ) , ( _ ( ' work ▁ factor ' ) , work_factor ) , ( _ ( ' salt ' ) , mask_hash ( salt ) ) , ( _ ( ' checksum ' ) , mask_hash ( checksum ) ) , ] ) NEW_LINE DEDENT
def must_update ( self , encoded ) : NEW_LINE INDENT algorithm , empty , algostr , rounds , data = encoded . split ( ' $ ' , 4 ) NEW_LINE return int ( rounds ) != self . rounds NEW_LINE DEDENT
def harden_runtime ( self , password , encoded ) : NEW_LINE INDENT _ , data = encoded . split ( ' $ ' , 1 ) NEW_LINE salt = data [ : 29 ] # ▁ Length ▁ of ▁ the ▁ salt ▁ in ▁ bcrypt . ENDCOM NEW_LINE rounds = data . split ( ' $ ' ) [ 2 ] NEW_LINE # ▁ work ▁ factor ▁ is ▁ logarithmic , ▁ adding ▁ one ▁ doubles ▁ the ▁ load . ENDCOM diff = 2 ** ( self . rounds - int ( rounds ) ) - 1 NEW_LINE while diff > 0 : NEW_LINE INDENT self . encode ( password , force_bytes ( salt ) ) NEW_LINE diff -= 1 NEW_LINE DEDENT DEDENT
def encode ( self , password , salt ) : NEW_LINE INDENT assert password is not None NEW_LINE assert salt and ' $ ' not in salt NEW_LINE hash = hashlib . sha1 ( force_bytes ( salt + password ) ) . hexdigest ( ) NEW_LINE return " % s $ % s $ % s " % ( self . algorithm , salt , hash ) NEW_LINE DEDENT
def verify ( self , password , encoded ) : NEW_LINE INDENT algorithm , salt , hash = encoded . split ( ' $ ' , 2 ) NEW_LINE assert algorithm == self . algorithm NEW_LINE encoded_2 = self . encode ( password , salt ) NEW_LINE return constant_time_compare ( encoded , encoded_2 ) NEW_LINE DEDENT
def safe_summary ( self , encoded ) : NEW_LINE INDENT algorithm , salt , hash = encoded . split ( ' $ ' , 2 ) NEW_LINE assert algorithm == self . algorithm NEW_LINE return OrderedDict ( [ ( _ ( ' algorithm ' ) , algorithm ) , ( _ ( ' salt ' ) , mask_hash ( salt , show = 2 ) ) , ( _ ( ' hash ' ) , mask_hash ( hash ) ) , ] ) NEW_LINE DEDENT
def harden_runtime ( self , password , encoded ) : NEW_LINE INDENT pass NEW_LINE DEDENT
def encode ( self , password , salt ) : NEW_LINE INDENT assert password is not None NEW_LINE assert salt and ' $ ' not in salt NEW_LINE hash = hashlib . md5 ( force_bytes ( salt + password ) ) . hexdigest ( ) NEW_LINE return " % s $ % s $ % s " % ( self . algorithm , salt , hash ) NEW_LINE DEDENT
def verify ( self , password , encoded ) : NEW_LINE INDENT algorithm , salt , hash = encoded . split ( ' $ ' , 2 ) NEW_LINE assert algorithm == self . algorithm NEW_LINE encoded_2 = self . encode ( password , salt ) NEW_LINE return constant_time_compare ( encoded , encoded_2 ) NEW_LINE DEDENT
def safe_summary ( self , encoded ) : NEW_LINE INDENT algorithm , salt , hash = encoded . split ( ' $ ' , 2 ) NEW_LINE assert algorithm == self . algorithm NEW_LINE return OrderedDict ( [ ( _ ( ' algorithm ' ) , algorithm ) , ( _ ( ' salt ' ) , mask_hash ( salt , show = 2 ) ) , ( _ ( ' hash ' ) , mask_hash ( hash ) ) , ] ) NEW_LINE DEDENT
def harden_runtime ( self , password , encoded ) : NEW_LINE INDENT pass NEW_LINE DEDENT
def salt ( self ) : NEW_LINE INDENT return ' ' NEW_LINE DEDENT
def encode ( self , password , salt ) : NEW_LINE INDENT assert salt == ' ' NEW_LINE hash = hashlib . sha1 ( force_bytes ( password ) ) . hexdigest ( ) NEW_LINE return ' sha1 $ $ % s ' % hash NEW_LINE DEDENT
def verify ( self , password , encoded ) : NEW_LINE INDENT encoded_2 = self . encode ( password , ' ' ) NEW_LINE return constant_time_compare ( encoded , encoded_2 ) NEW_LINE DEDENT
def safe_summary ( self , encoded ) : NEW_LINE INDENT assert encoded . startswith ( ' sha1 $ $ ' ) NEW_LINE hash = encoded [ 6 : ] NEW_LINE return OrderedDict ( [ ( _ ( ' algorithm ' ) , self . algorithm ) , ( _ ( ' hash ' ) , mask_hash ( hash ) ) , ] ) NEW_LINE DEDENT
def harden_runtime ( self , password , encoded ) : NEW_LINE INDENT pass NEW_LINE DEDENT
def salt ( self ) : NEW_LINE INDENT return ' ' NEW_LINE DEDENT
def encode ( self , password , salt ) : NEW_LINE INDENT assert salt == ' ' NEW_LINE return hashlib . md5 ( force_bytes ( password ) ) . hexdigest ( ) NEW_LINE DEDENT
def verify ( self , password , encoded ) : NEW_LINE INDENT if len ( encoded ) == 37 and encoded . startswith ( ' md5 $ $ ' ) : NEW_LINE INDENT encoded = encoded [ 5 : ] NEW_LINE DEDENT encoded_2 = self . encode ( password , ' ' ) NEW_LINE return constant_time_compare ( encoded , encoded_2 ) NEW_LINE DEDENT
def safe_summary ( self , encoded ) : NEW_LINE INDENT return OrderedDict ( [ ( _ ( ' algorithm ' ) , self . algorithm ) , ( _ ( ' hash ' ) , mask_hash ( encoded , show = 3 ) ) , ] ) NEW_LINE DEDENT
def harden_runtime ( self , password , encoded ) : NEW_LINE INDENT pass NEW_LINE DEDENT
def salt ( self ) : NEW_LINE INDENT return get_random_string ( 2 ) NEW_LINE DEDENT
def encode ( self , password , salt ) : NEW_LINE INDENT crypt = self . _load_library ( ) NEW_LINE assert len ( salt ) == 2 NEW_LINE data = crypt . crypt ( password , salt ) NEW_LINE assert data is not None # ▁ A ▁ platform ▁ like ▁ OpenBSD ▁ with ▁ a ▁ dummy ▁ crypt ▁ module . ENDCOM NEW_LINE # ▁ we ▁ don ' t ▁ need ▁ to ▁ store ▁ the ▁ salt , ▁ but ▁ Django ▁ used ▁ to ▁ do ▁ this ENDCOM return " % s $ % s $ % s " % ( self . algorithm , ' ' , data ) NEW_LINE DEDENT
def verify ( self , password , encoded ) : NEW_LINE INDENT crypt = self . _load_library ( ) NEW_LINE algorithm , salt , data = encoded . split ( ' $ ' , 2 ) NEW_LINE assert algorithm == self . algorithm NEW_LINE return constant_time_compare ( data , crypt . crypt ( password , data ) ) NEW_LINE DEDENT
def safe_summary ( self , encoded ) : NEW_LINE INDENT algorithm , salt , data = encoded . split ( ' $ ' , 2 ) NEW_LINE assert algorithm == self . algorithm NEW_LINE return OrderedDict ( [ ( _ ( ' algorithm ' ) , algorithm ) , ( _ ( ' salt ' ) , salt ) , ( _ ( ' hash ' ) , mask_hash ( data , show = 3 ) ) , ] ) NEW_LINE DEDENT
def harden_runtime ( self , password , encoded ) : NEW_LINE INDENT pass NEW_LINE DEDENT
def __init__ ( self , name = ' results . json ' , data = ' ' ) : NEW_LINE INDENT self . master = ' MockMasterName ' NEW_LINE self . builder = ' MockBuilderName ' NEW_LINE self . test_type = ' MockTestType ' NEW_LINE self . name = name NEW_LINE self . data = data NEW_LINE DEDENT
def save ( self , data ) : NEW_LINE INDENT self . data = data NEW_LINE return True NEW_LINE DEDENT
def setUp ( self ) : NEW_LINE INDENT self . _builder = " Webkit " NEW_LINE self . old_log_level = logging . root . level NEW_LINE logging . root . setLevel ( logging . ERROR ) NEW_LINE DEDENT
def tearDown ( self ) : NEW_LINE INDENT logging . root . setLevel ( self . old_log_level ) NEW_LINE # ▁ Use ▁ this ▁ to ▁ get ▁ better ▁ error ▁ messages ▁ than ▁ just ▁ string ▁ compare ▁ gives . ENDCOM DEDENT
def assert_json_equal ( self , a , b ) : NEW_LINE INDENT self . maxDiff = None NEW_LINE a = json . loads ( a ) if isinstance ( a , str ) else a NEW_LINE b = json . loads ( b ) if isinstance ( b , str ) else b NEW_LINE self . assertEqual ( a , b ) NEW_LINE DEDENT
def test_strip_prefix_suffix ( self ) : NEW_LINE INDENT json = " [ ' contents ' ] " NEW_LINE self . assertEqual ( JsonResults . _strip_prefix_suffix ( " ADD _ RESULTS ( " + json + " ) ; " ) , json ) NEW_LINE self . assertEqual ( JsonResults . _strip_prefix_suffix ( json ) , json ) NEW_LINE DEDENT
def _make_test_json ( self , test_data , json_string = JSON_RESULTS_TEMPLATE , builder_name = " Webkit " ) : NEW_LINE INDENT if not test_data : NEW_LINE INDENT return " " NEW_LINE DEDENT builds = test_data [ " builds " ] NEW_LINE tests = test_data [ " tests " ] NEW_LINE if not builds or not tests : NEW_LINE INDENT return " " NEW_LINE DEDENT counts = [ ] NEW_LINE build_numbers = [ ] NEW_LINE webkit_revision = [ ] NEW_LINE chrome_revision = [ ] NEW_LINE times = [ ] NEW_LINE for build in builds : NEW_LINE INDENT counts . append ( JSON_RESULTS_COUNTS_TEMPLATE . replace ( " [ TESTDATA ] " , build ) ) NEW_LINE build_numbers . append ( "1000 % s " % build ) NEW_LINE webkit_revision . append ( "2000 % s " % build ) NEW_LINE chrome_revision . append ( "3000 % s " % build ) NEW_LINE times . append ( "100000 % s000" % build ) NEW_LINE DEDENT json_string = json_string . replace ( " [ BUILDER _ NAME ] " , builder_name ) NEW_LINE json_string = json_string . replace ( " [ TESTDATA _ COUNTS ] " , " , " . join ( counts ) ) NEW_LINE json_string = json_string . replace ( " [ TESTDATA _ COUNT ] " , " , " . join ( builds ) ) NEW_LINE json_string = json_string . replace ( " [ TESTDATA _ BUILDNUMBERS ] " , " , " . join ( build_numbers ) ) NEW_LINE json_string = json_string . replace ( " [ TESTDATA _ WEBKITREVISION ] " , " , " . join ( webkit_revision ) ) NEW_LINE json_string = json_string . replace ( " [ TESTDATA _ CHROMEREVISION ] " , " , " . join ( chrome_revision ) ) NEW_LINE json_string = json_string . replace ( " [ TESTDATA _ TIMES ] " , " , " . join ( times ) ) NEW_LINE version = str ( test_data [ " version " ] ) if " version " in test_data else "4" NEW_LINE json_string = json_string . replace ( " [ VERSION ] " , version ) NEW_LINE json_string = json_string . replace ( " { [ TESTDATA _ TESTS ] } " , json . dumps ( tests , separators = ( ' , ' , ' : ' ) , sort_keys = True ) ) NEW_LINE return json_string NEW_LINE DEDENT
def _test_merge ( self , aggregated_data , incremental_data , expected_data , max_builds = jsonresults . JSON_RESULTS_MAX_BUILDS ) : NEW_LINE INDENT aggregated_results = self . _make_test_json ( aggregated_data , builder_name = self . _builder ) NEW_LINE incremental_json , _ = JsonResults . _get_incremental_json ( self . _builder , self . _make_test_json ( incremental_data , builder_name = self . _builder ) , is_full_results_format = False ) NEW_LINE merged_results , status_code = JsonResults . merge ( self . _builder , aggregated_results , incremental_json , num_runs = max_builds , sort_keys = True ) NEW_LINE if expected_data : NEW_LINE INDENT expected_results = self . _make_test_json ( expected_data , builder_name = self . _builder ) NEW_LINE self . assert_json_equal ( merged_results , expected_results ) NEW_LINE self . assertEqual ( status_code , 200 ) NEW_LINE DEDENT else : NEW_LINE INDENT self . assertTrue ( status_code != 200 ) NEW_LINE DEDENT DEDENT
def _test_get_test_list ( self , input_data , expected_data ) : NEW_LINE INDENT input_results = self . _make_test_json ( input_data ) NEW_LINE expected_results = JSON_RESULTS_TEST_LIST_TEMPLATE . replace ( " { [ TESTDATA _ TESTS ] } " , json . dumps ( expected_data , separators = ( ' , ' , ' : ' ) ) ) NEW_LINE actual_results = JsonResults . get_test_list ( self . _builder , input_results ) NEW_LINE self . assert_json_equal ( actual_results , expected_results ) NEW_LINE DEDENT
def test_update_files_empty_aggregate_data ( self ) : NEW_LINE INDENT small_file = MockFile ( name = ' results - small . json ' ) NEW_LINE large_file = MockFile ( name = ' results . json ' ) NEW_LINE incremental_data = { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] , } } } NEW_LINE incremental_string = self . _make_test_json ( incremental_data , builder_name = small_file . builder ) NEW_LINE self . assertTrue ( JsonResults . update_files ( small_file . builder , incremental_string , small_file , large_file , is_full_results_format = False ) ) NEW_LINE self . assert_json_equal ( small_file . data , incremental_string ) NEW_LINE self . assert_json_equal ( large_file . data , incremental_string ) NEW_LINE DEDENT
def test_update_files_null_incremental_data ( self ) : NEW_LINE INDENT small_file = MockFile ( name = ' results - small . json ' ) NEW_LINE large_file = MockFile ( name = ' results . json ' ) NEW_LINE aggregated_data = { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] , } } } NEW_LINE aggregated_string = self . _make_test_json ( aggregated_data , builder_name = small_file . builder ) NEW_LINE small_file . data = large_file . data = aggregated_string NEW_LINE incremental_string = " " NEW_LINE self . assertEqual ( JsonResults . update_files ( small_file . builder , incremental_string , small_file , large_file , is_full_results_format = False ) , ( ' No ▁ incremental ▁ JSON ▁ data ▁ to ▁ merge . ' , 403 ) ) NEW_LINE self . assert_json_equal ( small_file . data , aggregated_string ) NEW_LINE self . assert_json_equal ( large_file . data , aggregated_string ) NEW_LINE DEDENT
def test_update_files_empty_incremental_data ( self ) : NEW_LINE INDENT small_file = MockFile ( name = ' results - small . json ' ) NEW_LINE large_file = MockFile ( name = ' results . json ' ) NEW_LINE aggregated_data = { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] , } } } NEW_LINE aggregated_string = self . _make_test_json ( aggregated_data , builder_name = small_file . builder ) NEW_LINE small_file . data = large_file . data = aggregated_string NEW_LINE incremental_data = { " builds " : [ ] , " tests " : { } } NEW_LINE incremental_string = self . _make_test_json ( incremental_data , builder_name = small_file . builder ) NEW_LINE self . assertEqual ( JsonResults . update_files ( small_file . builder , incremental_string , small_file , large_file , is_full_results_format = False ) , ( ' No ▁ incremental ▁ JSON ▁ data ▁ to ▁ merge . ' , 403 ) ) NEW_LINE self . assert_json_equal ( small_file . data , aggregated_string ) NEW_LINE self . assert_json_equal ( large_file . data , aggregated_string ) NEW_LINE DEDENT
def test_merge_with_empty_aggregated_results ( self ) : NEW_LINE INDENT incremental_data = { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] , } } } NEW_LINE incremental_results , _ = JsonResults . _get_incremental_json ( self . _builder , self . _make_test_json ( incremental_data ) , is_full_results_format = False ) NEW_LINE aggregated_results = " " NEW_LINE merged_results , _ = JsonResults . merge ( self . _builder , aggregated_results , incremental_results , num_runs = jsonresults . JSON_RESULTS_MAX_BUILDS , sort_keys = True ) NEW_LINE self . assert_json_equal ( merged_results , incremental_results ) NEW_LINE DEDENT
def test_failures_by_type_added ( self ) : NEW_LINE INDENT aggregated_results = self . _make_test_json ( { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 100 , TEXT ] , [ 100 , FAIL ] ] , " times " : [ [ 200 , 0 ] ] , } } } , json_string = JSON_RESULTS_OLD_TEMPLATE ) NEW_LINE incremental_results = self . _make_test_json ( { " builds " : [ "3" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , TEXT ] ] , " times " : [ [ 1 , 0 ] ] , } } } , json_string = JSON_RESULTS_OLD_TEMPLATE ) NEW_LINE incremental_json , _ = JsonResults . _get_incremental_json ( self . _builder , incremental_results , is_full_results_format = False ) NEW_LINE merged_results , _ = JsonResults . merge ( self . _builder , aggregated_results , incremental_json , num_runs = 201 , sort_keys = True ) NEW_LINE self . assert_json_equal ( merged_results , self . _make_test_json ( { " builds " : [ "3" , "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 101 , TEXT ] , [ 100 , FAIL ] ] , " times " : [ [ 201 , 0 ] ] , } } } ) ) NEW_LINE DEDENT
def test_merge_full_results_format ( self ) : NEW_LINE INDENT expected_incremental_results = { " Webkit " : { " blinkRevision " : [ "1234" ] , " buildNumbers " : [ "3" ] , " chromeRevision " : [ "5678" ] , " failure _ map " : CHAR_TO_FAILURE , " num _ failures _ by _ type " : { " AUDIO " : [ 0 ] , " CRASH " : [ 3 ] , " FAIL " : [ 2 ] , " IMAGE " : [ 1 ] , " IMAGE + TEXT " : [ 0 ] , " MISSING " : [ 0 ] , " PASS " : [ 10 ] , " SKIP " : [ 2 ] , " TEXT " : [ 3 ] , " TIMEOUT " : [ 16 ] } , " secondsSinceEpoch " : [ 1368146629 ] , " tests " : { " media " : { " W3C " : { " audio " : { " src " : { " src _ removal _ does _ not _ trigger _ loadstart . html " : { " results " : [ [ 1 , PASS ] ] , " times " : [ [ 1 , 4 ] ] , } } } } , " encrypted - media " : { " encrypted - media - v2 - events . html " : { " bugs " : [ " crbug . com / 1234" ] , " expected " : " TIMEOUT " , " results " : [ [ 1 , TIMEOUT ] ] , " times " : [ [ 1 , 6 ] ] , } , " encrypted - media - v2 - syntax . html " : { " expected " : " TIMEOUT " , " results " : [ [ 1 , TIMEOUT ] ] , " times " : [ [ 1 , 0 ] ] , } } , " media - document - audio - repaint . html " : { " expected " : " IMAGE " , " results " : [ [ 1 , IMAGE ] ] , " times " : [ [ 1 , 0 ] ] , } , " progress - events - generated - correctly . html " : { " expected " : " PASS ▁ FAIL ▁ IMAGE ▁ TIMEOUT ▁ CRASH ▁ MISSING " , " results " : [ [ 1 , TIMEOUT ] ] , " times " : [ [ 1 , 6 ] ] , } , " flaky - failed . html " : { " expected " : " PASS ▁ FAIL " , " results " : [ [ 1 , FAIL ] ] , " times " : [ [ 1 , 0 ] ] , } , " unexpected - fail . html " : { " results " : [ [ 1 , FAIL ] ] , " times " : [ [ 1 , 0 ] ] , } , } } } , " version " : 4 } NEW_LINE aggregated_results = " " NEW_LINE incremental_json , _ = JsonResults . _get_incremental_json ( self . _builder , FULL_RESULT_EXAMPLE , is_full_results_format = True ) NEW_LINE merged_results , _ = JsonResults . merge ( " Webkit " , aggregated_results , incremental_json , num_runs = jsonresults . JSON_RESULTS_MAX_BUILDS , sort_keys = True ) NEW_LINE self . assert_json_equal ( merged_results , expected_incremental_results ) NEW_LINE DEDENT
def test_merge_empty_aggregated_results ( self ) : NEW_LINE # ▁ No ▁ existing ▁ aggregated ▁ results . ENDCOM # ▁ Merged ▁ results ▁ = = ▁ new ▁ incremental ▁ results . ENDCOM INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM None , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] } } } , # ▁ Expected ▁ result ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] } } } ) NEW_LINE DEDENT
def test_merge_duplicate_build_number ( self ) : NEW_LINE INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 100 , TEXT ] ] , " times " : [ [ 100 , 0 ] ] } } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "2" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , TEXT ] ] , " times " : [ [ 1 , 0 ] ] } } } , # ▁ Expected ▁ results ENDCOM None ) NEW_LINE DEDENT
def test_merge_incremental_single_test_single_run_same_result ( self ) : NEW_LINE # ▁ Incremental ▁ results ▁ has ▁ the ▁ latest ▁ build ▁ and ▁ same ▁ test ▁ results ▁ for ENDCOM # ▁ that ▁ run . ENDCOM # ▁ Insert ▁ the ▁ incremental ▁ results ▁ at ▁ the ▁ first ▁ place ▁ and ▁ sum ▁ number ENDCOM # ▁ of ▁ runs ▁ for ▁ TEXT ▁ ( 200 ▁ + ▁ 1 ) ▁ to ▁ get ▁ merged ▁ results . ENDCOM INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] } } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , TEXT ] ] , " times " : [ [ 1 , 0 ] ] } } } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 201 , TEXT ] ] , " times " : [ [ 201 , 0 ] ] } } } ) NEW_LINE DEDENT
def test_merge_single_test_single_run_different_result ( self ) : NEW_LINE # ▁ Incremental ▁ results ▁ has ▁ the ▁ latest ▁ build ▁ but ▁ different ▁ test ▁ results ENDCOM # ▁ for ▁ that ▁ run . ENDCOM # ▁ Insert ▁ the ▁ incremental ▁ results ▁ at ▁ the ▁ first ▁ place . ENDCOM INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] } } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , IMAGE ] ] , " times " : [ [ 1 , 1 ] ] } } } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , IMAGE ] , [ 200 , TEXT ] ] , " times " : [ [ 1 , 1 ] , [ 200 , 0 ] ] } } } ) NEW_LINE DEDENT
def test_merge_single_test_single_run_result_changed ( self ) : NEW_LINE # ▁ Incremental ▁ results ▁ has ▁ the ▁ latest ▁ build ▁ but ▁ results ▁ which ▁ differ ▁ from ENDCOM # ▁ the ▁ latest ▁ result ▁ ( but ▁ are ▁ the ▁ same ▁ as ▁ an ▁ older ▁ result ) . ENDCOM INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , TEXT ] , [ 10 , IMAGE ] ] , " times " : [ [ 200 , 0 ] , [ 10 , 1 ] ] } } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , IMAGE ] ] , " times " : [ [ 1 , 1 ] ] } } } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , IMAGE ] , [ 200 , TEXT ] , [ 10 , IMAGE ] ] , " times " : [ [ 1 , 1 ] , [ 200 , 0 ] , [ 10 , 1 ] ] } } } ) NEW_LINE DEDENT
def test_merge_multiple_tests_single_run ( self ) : NEW_LINE # ▁ All ▁ tests ▁ have ▁ incremental ▁ updates . ENDCOM INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] } , "002 . html " : { " results " : [ [ 100 , IMAGE ] ] , " times " : [ [ 100 , 1 ] ] } } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , TEXT ] ] , " times " : [ [ 1 , 0 ] ] } , "002 . html " : { " results " : [ [ 1 , IMAGE ] ] , " times " : [ [ 1 , 1 ] ] } } } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 201 , TEXT ] ] , " times " : [ [ 201 , 0 ] ] } , "002 . html " : { " results " : [ [ 101 , IMAGE ] ] , " times " : [ [ 101 , 1 ] ] } } } ) NEW_LINE DEDENT
def test_merge_multiple_tests_single_run_one_no_result ( self ) : NEW_LINE INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] } , "002 . html " : { " results " : [ [ 100 , IMAGE ] ] , " times " : [ [ 100 , 1 ] ] } } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" ] , " tests " : { "002 . html " : { " results " : [ [ 1 , IMAGE ] ] , " times " : [ [ 1 , 1 ] ] } } } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , NO_DATA ] , [ 200 , TEXT ] ] , " times " : [ [ 201 , 0 ] ] } , "002 . html " : { " results " : [ [ 101 , IMAGE ] ] , " times " : [ [ 101 , 1 ] ] } } } ) NEW_LINE DEDENT
def test_merge_single_test_multiple_runs ( self ) : NEW_LINE INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] } } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "4" , "3" ] , " tests " : { "001 . html " : { " results " : [ [ 2 , IMAGE ] , [ 1 , FAIL ] ] , " times " : [ [ 3 , 2 ] ] } } } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "4" , "3" , "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , FAIL ] , [ 2 , IMAGE ] , [ 200 , TEXT ] ] , " times " : [ [ 3 , 2 ] , [ 200 , 0 ] ] } } } ) NEW_LINE DEDENT
def test_merge_multiple_tests_multiple_runs ( self ) : NEW_LINE INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] } , "002 . html " : { " results " : [ [ 10 , IMAGE_PLUS_TEXT ] ] , " times " : [ [ 10 , 0 ] ] } } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "4" , "3" ] , " tests " : { "001 . html " : { " results " : [ [ 2 , IMAGE ] ] , " times " : [ [ 2 , 2 ] ] } , "002 . html " : { " results " : [ [ 1 , CRASH ] ] , " times " : [ [ 1 , 1 ] ] } } } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "4" , "3" , "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 2 , IMAGE ] , [ 200 , TEXT ] ] , " times " : [ [ 2 , 2 ] , [ 200 , 0 ] ] } , "002 . html " : { " results " : [ [ 1 , CRASH ] , [ 10 , IMAGE_PLUS_TEXT ] ] , " times " : [ [ 1 , 1 ] , [ 10 , 0 ] ] } } } ) NEW_LINE DEDENT
def test_merge_incremental_result_older_build ( self ) : NEW_LINE # ▁ Test ▁ the ▁ build ▁ in ▁ incremental ▁ results ▁ is ▁ older ▁ than ▁ the ▁ most ▁ recent ENDCOM # ▁ build ▁ in ▁ aggregated ▁ results . ENDCOM INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "3" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 5 , TEXT ] ] , " times " : [ [ 5 , 0 ] ] } } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "2" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , TEXT ] ] , " times " : [ [ 1 , 0 ] ] } } } , # ▁ Expected ▁ no ▁ merge ▁ happens . ENDCOM { " builds " : [ "2" , "3" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 6 , TEXT ] ] , " times " : [ [ 6 , 0 ] ] } } } ) NEW_LINE DEDENT
def test_merge_incremental_result_same_build ( self ) : NEW_LINE # ▁ Test ▁ the ▁ build ▁ in ▁ incremental ▁ results ▁ is ▁ same ▁ as ▁ the ▁ build ▁ in ENDCOM # ▁ aggregated ▁ results . ENDCOM INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 5 , TEXT ] ] , " times " : [ [ 5 , 0 ] ] } } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" , "2" ] , " tests " : { "001 . html " : { " results " : [ [ 2 , TEXT ] ] , " times " : [ [ 2 , 0 ] ] } } } , # ▁ Expected ▁ no ▁ merge ▁ happens . ENDCOM { " builds " : [ "3" , "2" , "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 7 , TEXT ] ] , " times " : [ [ 7 , 0 ] ] } } } ) NEW_LINE DEDENT
def test_merge_remove_new_test ( self ) : NEW_LINE INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 199 , TEXT ] ] , " times " : [ [ 199 , 0 ] ] } , } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , TEXT ] ] , " times " : [ [ 1 , 0 ] ] } , "002 . html " : { " results " : [ [ 1 , PASS ] ] , " times " : [ [ 1 , 0 ] ] } , " notrun . html " : { " results " : [ [ 1 , NOTRUN ] ] , " times " : [ [ 1 , 0 ] ] } , "003 . html " : { " results " : [ [ 1 , NO_DATA ] ] , " times " : [ [ 1 , 0 ] ] } , } } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] } , } } , max_builds = 200 ) NEW_LINE DEDENT
def test_merge_remove_test ( self ) : NEW_LINE INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { " directory " : { " directory " : { "001 . html " : { " results " : [ [ 200 , PASS ] ] , " times " : [ [ 200 , 0 ] ] } } } , "002 . html " : { " results " : [ [ 10 , TEXT ] ] , " times " : [ [ 10 , 0 ] ] } , "003 . html " : { " results " : [ [ 190 , PASS ] , [ 9 , NO_DATA ] , [ 1 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] } , } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" ] , " tests " : { " directory " : { " directory " : { "001 . html " : { " results " : [ [ 1 , PASS ] ] , " times " : [ [ 1 , 0 ] ] } } } , "002 . html " : { " results " : [ [ 1 , PASS ] ] , " times " : [ [ 1 , 0 ] ] } , "003 . html " : { " results " : [ [ 1 , PASS ] ] , " times " : [ [ 1 , 0 ] ] } , } } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { "002 . html " : { " results " : [ [ 1 , PASS ] , [ 10 , TEXT ] ] , " times " : [ [ 11 , 0 ] ] } } } , max_builds = 200 ) NEW_LINE DEDENT
def test_merge_updates_expected ( self ) : NEW_LINE INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { " directory " : { " directory " : { "001 . html " : { " expected " : " FAIL " , " results " : [ [ 200 , PASS ] ] , " times " : [ [ 200 , 0 ] ] } } } , "002 . html " : { " bugs " : [ " crbug . com / 1234" ] , " expected " : " FAIL " , " results " : [ [ 10 , TEXT ] ] , " times " : [ [ 10 , 0 ] ] } , "003 . html " : { " expected " : " FAIL " , " results " : [ [ 190 , PASS ] , [ 9 , NO_DATA ] , [ 1 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] } , "004 . html " : { " results " : [ [ 199 , PASS ] , [ 1 , TEXT ] ] , " times " : [ [ 200 , 0 ] ] } , } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" ] , " tests " : { "002 . html " : { " expected " : " PASS " , " results " : [ [ 1 , PASS ] ] , " times " : [ [ 1 , 0 ] ] } , "003 . html " : { " expected " : " TIMEOUT " , " results " : [ [ 1 , PASS ] ] , " times " : [ [ 1 , 0 ] ] } , "004 . html " : { " bugs " : [ " crbug . com / 1234" ] , " results " : [ [ 1 , PASS ] ] , " times " : [ [ 1 , 0 ] ] } , } } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { "002 . html " : { " results " : [ [ 1 , PASS ] , [ 10 , TEXT ] ] , " times " : [ [ 11 , 0 ] ] } , "003 . html " : { " expected " : " TIMEOUT " , " results " : [ [ 191 , PASS ] , [ 9 , NO_DATA ] ] , " times " : [ [ 200 , 0 ] ] } , "004 . html " : { " bugs " : [ " crbug . com / 1234" ] , " results " : [ [ 200 , PASS ] ] , " times " : [ [ 200 , 0 ] ] } , } } , max_builds = 200 ) NEW_LINE DEDENT
def test_merge_keep_test_with_all_pass_but_slow_time ( self ) : NEW_LINE INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , PASS ] ] , " times " : [ [ 200 , jsonresults . JSON_RESULTS_MIN_TIME ] ] } , "002 . html " : { " results " : [ [ 10 , TEXT ] ] , " times " : [ [ 10 , 0 ] ] } } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , PASS ] ] , " times " : [ [ 1 , 1 ] ] } , "002 . html " : { " results " : [ [ 1 , PASS ] ] , " times " : [ [ 1 , 0 ] ] } } } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 201 , PASS ] ] , " times " : [ [ 1 , 1 ] , [ 200 , jsonresults . JSON_RESULTS_MIN_TIME ] ] } , "002 . html " : { " results " : [ [ 1 , PASS ] , [ 10 , TEXT ] ] , " times " : [ [ 11 , 0 ] ] } } } ) NEW_LINE DEDENT
def test_merge_pruning_slow_tests_for_debug_builders ( self ) : NEW_LINE INDENT self . _builder = " MockBuilder ( dbg ) " NEW_LINE self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 200 , PASS ] ] , " times " : [ [ 200 , 3 * jsonresults . JSON_RESULTS_MIN_TIME ] ] } , "002 . html " : { " results " : [ [ 10 , TEXT ] ] , " times " : [ [ 10 , 0 ] ] } } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , PASS ] ] , " times " : [ [ 1 , 1 ] ] } , "002 . html " : { " results " : [ [ 1 , PASS ] ] , " times " : [ [ 1 , 0 ] ] } , "003 . html " : { " results " : [ [ 1 , PASS ] ] , " times " : [ [ 1 , jsonresults . JSON_RESULTS_MIN_TIME ] ] } } } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 201 , PASS ] ] , " times " : [ [ 1 , 1 ] , [ 200 , 3 * jsonresults . JSON_RESULTS_MIN_TIME ] ] } , "002 . html " : { " results " : [ [ 1 , PASS ] , [ 10 , TEXT ] ] , " times " : [ [ 11 , 0 ] ] } } } ) NEW_LINE DEDENT
def test_merge_prune_extra_results ( self ) : NEW_LINE # ▁ Remove ▁ items ▁ from ▁ test ▁ results ▁ and ▁ times ▁ that ▁ exceed ▁ the ▁ max ▁ number ENDCOM # ▁ of ▁ builds ▁ to ▁ track . ENDCOM INDENT max_builds = jsonresults . JSON_RESULTS_MAX_BUILDS NEW_LINE self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ max_builds , TEXT ] , [ 1 , IMAGE ] ] , " times " : [ [ max_builds , 0 ] , [ 1 , 1 ] ] } } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , TIMEOUT ] ] , " times " : [ [ 1 , 1 ] ] } } } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , TIMEOUT ] , [ max_builds , TEXT ] ] , " times " : [ [ 1 , 1 ] , [ max_builds , 0 ] ] } } } ) NEW_LINE DEDENT
def test_merge_prune_extra_results_small ( self ) : NEW_LINE # ▁ Remove ▁ items ▁ from ▁ test ▁ results ▁ and ▁ times ▁ that ▁ exceed ▁ the ▁ max ▁ number ENDCOM # ▁ of ▁ builds ▁ to ▁ track , ▁ using ▁ smaller ▁ threshold . ENDCOM INDENT max_builds = jsonresults . JSON_RESULTS_MAX_BUILDS_SMALL NEW_LINE self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ max_builds , TEXT ] , [ 1 , IMAGE ] ] , " times " : [ [ max_builds , 0 ] , [ 1 , 1 ] ] } } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , TIMEOUT ] ] , " times " : [ [ 1 , 1 ] ] } } } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , TIMEOUT ] , [ max_builds , TEXT ] ] , " times " : [ [ 1 , 1 ] , [ max_builds , 0 ] ] } } } , int ( max_builds ) ) NEW_LINE DEDENT
def test_merge_prune_extra_results_with_new_result_of_same_type ( self ) : NEW_LINE # ▁ Test ▁ that ▁ merging ▁ in ▁ a ▁ new ▁ result ▁ of ▁ the ▁ same ▁ type ▁ as ▁ the ▁ last ▁ result ENDCOM # ▁ causes ▁ old ▁ results ▁ to ▁ fall ▁ off . ENDCOM INDENT max_builds = jsonresults . JSON_RESULTS_MAX_BUILDS_SMALL NEW_LINE self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ max_builds , TEXT ] , [ 1 , NO_DATA ] ] , " times " : [ [ max_builds , 0 ] , [ 1 , 1 ] ] } } } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" ] , " tests " : { "001 . html " : { " results " : [ [ 1 , TEXT ] ] , " times " : [ [ 1 , 0 ] ] } } } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { "001 . html " : { " results " : [ [ max_builds , TEXT ] ] , " times " : [ [ max_builds , 0 ] ] } } } , int ( max_builds ) ) NEW_LINE DEDENT
def test_merge_build_directory_hierarchy ( self ) : NEW_LINE INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { " bar " : { " baz " : { "003 . html " : { " results " : [ [ 25 , TEXT ] ] , " times " : [ [ 25 , 0 ] ] } } } , " foo " : { "001 . html " : { " results " : [ [ 50 , TEXT ] ] , " times " : [ [ 50 , 0 ] ] } , "002 . html " : { " results " : [ [ 100 , IMAGE ] ] , " times " : [ [ 100 , 0 ] ] } } } , " version " : 4 } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" ] , " tests " : { " baz " : { "004 . html " : { " results " : [ [ 1 , IMAGE ] ] , " times " : [ [ 1 , 0 ] ] } } , " foo " : { "001 . html " : { " results " : [ [ 1 , TEXT ] ] , " times " : [ [ 1 , 0 ] ] } , "002 . html " : { " results " : [ [ 1 , IMAGE ] ] , " times " : [ [ 1 , 0 ] ] } } } , " version " : 4 } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { " bar " : { " baz " : { "003 . html " : { " results " : [ [ 1 , NO_DATA ] , [ 25 , TEXT ] ] , " times " : [ [ 26 , 0 ] ] } } } , " baz " : { "004 . html " : { " results " : [ [ 1 , IMAGE ] ] , " times " : [ [ 1 , 0 ] ] } } , " foo " : { "001 . html " : { " results " : [ [ 51 , TEXT ] ] , " times " : [ [ 51 , 0 ] ] } , "002 . html " : { " results " : [ [ 101 , IMAGE ] ] , " times " : [ [ 101 , 0 ] ] } } } , " version " : 4 } ) NEW_LINE # ▁ FIXME ( aboxhall ) : ▁ Add ▁ some ▁ tests ▁ for ▁ xhtml / svg ▁ test ▁ results . ENDCOM DEDENT
def test_get_test_name_list ( self ) : NEW_LINE # ▁ Get ▁ test ▁ name ▁ list ▁ only . ▁ Don ' t ▁ include ▁ non - test - list ▁ data ▁ and ENDCOM # ▁ of ▁ test ▁ result ▁ details . ENDCOM # ▁ FIXME : ▁ This ▁ also ▁ tests ▁ a ▁ temporary ▁ bug ▁ in ▁ the ▁ data ▁ where ▁ directory - level ENDCOM # ▁ results ▁ have ▁ a ▁ results ▁ and ▁ times ▁ values . ▁ Once ▁ that ▁ bug ▁ is ▁ fixed , ENDCOM # ▁ remove ▁ this ▁ test - case ▁ and ▁ assert ▁ we ▁ don ' t ▁ ever ▁ hit ▁ it . ENDCOM INDENT self . _test_get_test_list ( # ▁ Input ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { " foo " : { "001 . html " : { " results " : [ [ 200 , PASS ] ] , " times " : [ [ 200 , 0 ] ] } , " results " : [ [ 1 , NO_DATA ] ] , " times " : [ [ 1 , 0 ] ] } , "002 . html " : { " results " : [ [ 10 , TEXT ] ] , " times " : [ [ 10 , 0 ] ] } } } , # ▁ Expected ▁ results ENDCOM { " foo " : { "001 . html " : { } } , "002 . html " : { } } ) NEW_LINE DEDENT
def test_gtest ( self ) : NEW_LINE INDENT self . _test_merge ( # ▁ Aggregated ▁ results ENDCOM { " builds " : [ "2" , "1" ] , " tests " : { " foo . bar " : { " results " : [ [ 50 , TEXT ] ] , " times " : [ [ 50 , 0 ] ] } , " foo . bar2" : { " results " : [ [ 100 , IMAGE ] ] , " times " : [ [ 100 , 0 ] ] } , " test . failed " : { " results " : [ [ 5 , FAIL ] ] , " times " : [ [ 5 , 0 ] ] } , } , " version " : 3 } , # ▁ Incremental ▁ results ENDCOM { " builds " : [ "3" ] , " tests " : { " foo . bar2" : { " results " : [ [ 1 , IMAGE ] ] , " times " : [ [ 1 , 0 ] ] } , " foo . bar3" : { " results " : [ [ 1 , TEXT ] ] , " times " : [ [ 1 , 0 ] ] } , " test . failed " : { " results " : [ [ 5 , FAIL ] ] , " times " : [ [ 5 , 0 ] ] } , } , " version " : 4 } , # ▁ Expected ▁ results ENDCOM { " builds " : [ "3" , "2" , "1" ] , " tests " : { " foo . bar " : { " results " : [ [ 1 , NO_DATA ] , [ 50 , TEXT ] ] , " times " : [ [ 51 , 0 ] ] } , " foo . bar2" : { " results " : [ [ 101 , IMAGE ] ] , " times " : [ [ 101 , 0 ] ] } , " foo . bar3" : { " results " : [ [ 1 , TEXT ] ] , " times " : [ [ 1 , 0 ] ] } , " test . failed " : { " results " : [ [ 10 , FAIL ] ] , " times " : [ [ 10 , 0 ] ] } , } , " version " : 4 } ) NEW_LINE DEDENT
