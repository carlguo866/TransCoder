def __init__ ( self , name , requires = None , suggestions = None ) : NEW_LINE INDENT msg = ' % s ▁ is ▁ not ▁ exported ▁ by ▁ the ▁ available ▁ OpenGL ▁ driver . ' % name NEW_LINE if requires : NEW_LINE INDENT msg += ' ▁ ▁ % s ▁ is ▁ required ▁ for ▁ this ▁ functionality . ' % requires NEW_LINE DEDENT if suggestions : NEW_LINE INDENT msg += ' ▁ ▁ Consider ▁ alternative ( s ) ▁ % s . ' % ' , ▁ ' . join ( suggestions ) NEW_LINE DEDENT Exception . __init__ ( self , msg ) NEW_LINE DEDENT
def shutdown ( self ) : NEW_LINE INDENT """ ▁ shutdown ▁ method : ▁ define ▁ when ▁ to ▁ shun ▁ down """ NEW_LINE while self . dataReady ( " control " ) : NEW_LINE INDENT data = self . recv ( " control " ) NEW_LINE if isinstance ( data , producerFinished ) or isinstance ( data , shutdownMicroprocess ) : NEW_LINE INDENT self . shutdown_mess = data NEW_LINE return True NEW_LINE DEDENT DEDENT return False NEW_LINE DEDENT
def main ( self ) : NEW_LINE INDENT """ ▁ main ▁ method : ▁ do ▁ stuff ▁ """ NEW_LINE previousNodes = [ ] NEW_LINE # ▁ Put ▁ all ▁ codes ▁ within ▁ the ▁ loop , ▁ so ▁ that ▁ others ▁ can ▁ be ▁ run ▁ even ▁ it ▁ doesn ' t ▁ shut ▁ down ENDCOM while not self . shutdown ( ) : NEW_LINE INDENT X = [ ] NEW_LINE links = [ ] NEW_LINE nodes = [ ] NEW_LINE updatedNodes = [ ] NEW_LINE while not self . anyReady ( ) : NEW_LINE INDENT self . pause ( ) NEW_LINE yield 1 NEW_LINE DEDENT while self . dataReady ( " inbox " ) : NEW_LINE INDENT L = self . recv ( " inbox " ) NEW_LINE if L . strip ( ) == " " : continue # ▁ empty ▁ line ENDCOM NEW_LINE if L . lstrip ( ) [ 0 ] == " # " : continue # ▁ comment ENDCOM NEW_LINE X . append ( L . strip ( ) ) NEW_LINE # yield ▁ 1 ENDCOM DEDENT for item in X : NEW_LINE INDENT if re . match ( ' ( . + ) \ ( ( . + ) , ( . + ) \ ) ' , item ) : # ▁ relation ENDCOM NEW_LINE INDENT command = parseRelation ( item ) NEW_LINE links . append ( command ) NEW_LINE DEDENT else : NEW_LINE INDENT isRepeated = False NEW_LINE for node in previousNodes : NEW_LINE INDENT if item . split ( ) [ 1 ] == node . split ( ) [ 2 ] : NEW_LINE INDENT isRepeated = True NEW_LINE DEDENT DEDENT if not isRepeated : # ▁ new ▁ entity ENDCOM NEW_LINE INDENT command = parseEntity ( item ) NEW_LINE nodes . append ( command ) NEW_LINE previousNodes . append ( command ) NEW_LINE DEDENT else : # ▁ old ▁ entity ENDCOM NEW_LINE INDENT command = parseUpdatedEntity ( item ) NEW_LINE updatedNodes . append ( command ) NEW_LINE # yield ▁ 1 ENDCOM DEDENT DEDENT DEDENT for node in nodes : NEW_LINE INDENT self . send ( node , " outbox " ) NEW_LINE DEDENT for updatedNode in updatedNodes : NEW_LINE INDENT self . send ( updatedNode , " outbox " ) NEW_LINE DEDENT for link in links : NEW_LINE INDENT self . send ( link , " outbox " ) NEW_LINE DEDENT yield 1 NEW_LINE DEDENT self . send ( self . shutdown_mess , " signal " ) NEW_LINE DEDENT
def __init__ ( self ) : NEW_LINE INDENT super ( Manager , self ) . __init__ ( CONF ) NEW_LINE self . _app = service . RedisApp ( ) NEW_LINE DEDENT
def update_status ( self , context ) : NEW_LINE INDENT """ STRNEWLINE ▁ Updates ▁ the ▁ redis ▁ trove ▁ instance . ▁ It ▁ is ▁ decorated ▁ with STRNEWLINE ▁ perodic ▁ task ▁ so ▁ it ▁ is ▁ automatically ▁ called ▁ every ▁ 3 ▁ ticks . STRNEWLINE ▁ """ NEW_LINE LOG . debug ( " Update ▁ status ▁ called . " ) NEW_LINE self . _app . status . update ( ) NEW_LINE DEDENT
def rpc_ping ( self , context ) : NEW_LINE INDENT LOG . debug ( " Responding ▁ to ▁ RPC ▁ ping . " ) NEW_LINE return True NEW_LINE DEDENT
def change_passwords ( self , context , users ) : NEW_LINE INDENT """ STRNEWLINE ▁ Changes ▁ the ▁ redis ▁ instance ▁ password , STRNEWLINE ▁ it ▁ is ▁ currently ▁ not ▁ not ▁ implemented . STRNEWLINE ▁ """ NEW_LINE LOG . debug ( " Change ▁ passwords ▁ called . " ) NEW_LINE raise exception . DatastoreOperationNotSupported ( operation = ' change _ passwords ' , datastore = MANAGER ) NEW_LINE DEDENT
def reset_configuration ( self , context , configuration ) : NEW_LINE INDENT """ STRNEWLINE ▁ Resets ▁ to ▁ the ▁ default ▁ configuration , STRNEWLINE ▁ currently ▁ this ▁ does ▁ nothing . STRNEWLINE ▁ """ NEW_LINE LOG . debug ( " Reset ▁ configuration ▁ called . " ) NEW_LINE self . _app . reset_configuration ( configuration ) NEW_LINE DEDENT
def _perform_restore ( self , backup_info , context , restore_location , app ) : NEW_LINE INDENT """ Perform ▁ a ▁ restore ▁ on ▁ this ▁ instance . """ NEW_LINE LOG . info ( _ ( " Restoring ▁ database ▁ from ▁ backup ▁ % s . " ) % backup_info [ ' id ' ] ) NEW_LINE try : NEW_LINE INDENT backup . restore ( context , backup_info , restore_location ) NEW_LINE DEDENT except Exception : NEW_LINE INDENT LOG . exception ( _ ( " Error ▁ performing ▁ restore ▁ from ▁ backup ▁ % s . " ) % backup_info [ ' id ' ] ) NEW_LINE app . status . set_status ( rd_instance . ServiceStatuses . FAILED ) NEW_LINE raise NEW_LINE DEDENT LOG . info ( _ ( " Restored ▁ database ▁ successfully . " ) ) NEW_LINE DEDENT
def prepare ( self , context , packages , databases , memory_mb , users , device_path = None , mount_point = None , backup_info = None , config_contents = None , root_password = None , overrides = None , cluster_config = None , snapshot = None ) : NEW_LINE INDENT """ STRNEWLINE ▁ This ▁ is ▁ called ▁ when ▁ the ▁ trove ▁ instance ▁ first ▁ comes ▁ online . STRNEWLINE ▁ It ▁ is ▁ the ▁ first ▁ rpc ▁ message ▁ passed ▁ from ▁ the ▁ task ▁ manager . STRNEWLINE ▁ prepare ▁ handles ▁ all ▁ the ▁ base ▁ configuration ▁ of ▁ the ▁ redis ▁ instance . STRNEWLINE ▁ """ NEW_LINE try : NEW_LINE INDENT self . _app . status . begin_install ( ) NEW_LINE if device_path : NEW_LINE INDENT device = volume . VolumeDevice ( device_path ) NEW_LINE # ▁ unmount ▁ if ▁ device ▁ is ▁ already ▁ mounted ENDCOM device . unmount_device ( device_path ) NEW_LINE device . format ( ) NEW_LINE device . mount ( mount_point ) NEW_LINE operating_system . chown ( mount_point , ' redis ' , ' redis ' , as_root = True ) NEW_LINE LOG . debug ( ' Mounted ▁ the ▁ volume . ' ) NEW_LINE DEDENT self . _app . install_if_needed ( packages ) NEW_LINE LOG . info ( _ ( ' Writing ▁ redis ▁ configuration . ' ) ) NEW_LINE if cluster_config : NEW_LINE INDENT config_contents = ( config_contents + " \n " + " cluster - enabled ▁ yes \n " + " cluster - config - file ▁ cluster . conf \n " ) NEW_LINE DEDENT self . _app . configuration_manager . save_configuration ( config_contents ) NEW_LINE self . _app . apply_initial_guestagent_configuration ( ) NEW_LINE if backup_info : NEW_LINE INDENT persistence_dir = self . _app . get_working_dir ( ) NEW_LINE self . _perform_restore ( backup_info , context , persistence_dir , self . _app ) NEW_LINE DEDENT if snapshot : NEW_LINE INDENT self . attach_replica ( context , snapshot , snapshot [ ' config ' ] ) NEW_LINE DEDENT self . _app . restart ( ) NEW_LINE if cluster_config : NEW_LINE INDENT self . _app . status . set_status ( rd_instance . ServiceStatuses . BUILD_PENDING ) NEW_LINE DEDENT else : NEW_LINE INDENT self . _app . complete_install_or_restart ( ) NEW_LINE DEDENT LOG . info ( _ ( ' Redis ▁ instance ▁ has ▁ been ▁ setup ▁ and ▁ configured . ' ) ) NEW_LINE DEDENT except Exception : NEW_LINE INDENT LOG . exception ( _ ( " Error ▁ setting ▁ up ▁ Redis ▁ instance . " ) ) NEW_LINE self . _app . status . set_status ( rd_instance . ServiceStatuses . FAILED ) NEW_LINE raise NEW_LINE DEDENT DEDENT
def restart ( self , context ) : NEW_LINE INDENT """ STRNEWLINE ▁ Restart ▁ this ▁ redis ▁ instance . STRNEWLINE ▁ This ▁ method ▁ is ▁ called ▁ when ▁ the ▁ guest ▁ agent STRNEWLINE ▁ gets ▁ a ▁ restart ▁ message ▁ from ▁ the ▁ taskmanager . STRNEWLINE ▁ """ NEW_LINE LOG . debug ( " Restart ▁ called . " ) NEW_LINE self . _app . restart ( ) NEW_LINE DEDENT
def start_db_with_conf_changes ( self , context , config_contents ) : NEW_LINE INDENT """ STRNEWLINE ▁ Start ▁ this ▁ redis ▁ instance ▁ with ▁ new ▁ conf ▁ changes . STRNEWLINE ▁ """ NEW_LINE LOG . debug ( " Start ▁ DB ▁ with ▁ conf ▁ changes ▁ called . " ) NEW_LINE self . _app . start_db_with_conf_changes ( config_contents ) NEW_LINE DEDENT
def stop_db ( self , context , do_not_start_on_reboot = False ) : NEW_LINE INDENT """ STRNEWLINE ▁ Stop ▁ this ▁ redis ▁ instance . STRNEWLINE ▁ This ▁ method ▁ is ▁ called ▁ when ▁ the ▁ guest ▁ agent STRNEWLINE ▁ gets ▁ a ▁ stop ▁ message ▁ from ▁ the ▁ taskmanager . STRNEWLINE ▁ """ NEW_LINE LOG . debug ( " Stop ▁ DB ▁ called . " ) NEW_LINE self . _app . stop_db ( do_not_start_on_reboot = do_not_start_on_reboot ) NEW_LINE DEDENT
def get_filesystem_stats ( self , context , fs_path ) : NEW_LINE INDENT """ Gets ▁ the ▁ filesystem ▁ stats ▁ for ▁ the ▁ path ▁ given . """ NEW_LINE LOG . debug ( " Get ▁ Filesystem ▁ Stats . " ) NEW_LINE mount_point = CONF . get ( ' mysql ' if not MANAGER else MANAGER ) . mount_point NEW_LINE return dbaas . get_filesystem_volume_stats ( mount_point ) NEW_LINE DEDENT
def create_backup ( self , context , backup_info ) : NEW_LINE INDENT """ Create ▁ a ▁ backup ▁ of ▁ the ▁ database . """ NEW_LINE LOG . debug ( " Creating ▁ backup . " ) NEW_LINE backup . backup ( context , backup_info ) NEW_LINE DEDENT
def mount_volume ( self , context , device_path = None , mount_point = None ) : NEW_LINE INDENT device = volume . VolumeDevice ( device_path ) NEW_LINE device . mount ( mount_point , write_to_fstab = False ) NEW_LINE LOG . debug ( " Mounted ▁ the ▁ device ▁ % s ▁ at ▁ the ▁ mount ▁ point ▁ % s . " % ( device_path , mount_point ) ) NEW_LINE DEDENT
def unmount_volume ( self , context , device_path = None , mount_point = None ) : NEW_LINE INDENT device = volume . VolumeDevice ( device_path ) NEW_LINE device . unmount ( mount_point ) NEW_LINE LOG . debug ( " Unmounted ▁ the ▁ device ▁ % s ▁ from ▁ the ▁ mount ▁ point ▁ % s . " % ( device_path , mount_point ) ) NEW_LINE DEDENT
def resize_fs ( self , context , device_path = None , mount_point = None ) : NEW_LINE INDENT device = volume . VolumeDevice ( device_path ) NEW_LINE device . resize_fs ( mount_point ) NEW_LINE LOG . debug ( " Resized ▁ the ▁ filesystem ▁ at ▁ % s . " % mount_point ) NEW_LINE DEDENT
def update_overrides ( self , context , overrides , remove = False ) : NEW_LINE INDENT LOG . debug ( " Updating ▁ overrides . " ) NEW_LINE if remove : NEW_LINE INDENT self . _app . remove_overrides ( ) NEW_LINE DEDENT else : NEW_LINE INDENT self . _app . update_overrides ( context , overrides , remove ) NEW_LINE DEDENT DEDENT
def apply_overrides ( self , context , overrides ) : NEW_LINE INDENT LOG . debug ( " Applying ▁ overrides . " ) NEW_LINE self . _app . apply_overrides ( self . _app . admin , overrides ) NEW_LINE DEDENT
def update_attributes ( self , context , username , hostname , user_attrs ) : NEW_LINE INDENT LOG . debug ( " Updating ▁ attributes . " ) NEW_LINE raise exception . DatastoreOperationNotSupported ( operation = ' update _ attributes ' , datastore = MANAGER ) NEW_LINE DEDENT
def create_database ( self , context , databases ) : NEW_LINE INDENT LOG . debug ( " Creating ▁ database . " ) NEW_LINE raise exception . DatastoreOperationNotSupported ( operation = ' create _ database ' , datastore = MANAGER ) NEW_LINE DEDENT
def create_user ( self , context , users ) : NEW_LINE INDENT LOG . debug ( " Creating ▁ user . " ) NEW_LINE raise exception . DatastoreOperationNotSupported ( operation = ' create _ user ' , datastore = MANAGER ) NEW_LINE DEDENT
def delete_database ( self , context , database ) : NEW_LINE INDENT LOG . debug ( " Deleting ▁ database . " ) NEW_LINE raise exception . DatastoreOperationNotSupported ( operation = ' delete _ database ' , datastore = MANAGER ) NEW_LINE DEDENT
def delete_user ( self , context , user ) : NEW_LINE INDENT LOG . debug ( " Deleting ▁ user . " ) NEW_LINE raise exception . DatastoreOperationNotSupported ( operation = ' delete _ user ' , datastore = MANAGER ) NEW_LINE DEDENT
def get_user ( self , context , username , hostname ) : NEW_LINE INDENT LOG . debug ( " Getting ▁ user . " ) NEW_LINE raise exception . DatastoreOperationNotSupported ( operation = ' get _ user ' , datastore = MANAGER ) NEW_LINE DEDENT
def grant_access ( self , context , username , hostname , databases ) : NEW_LINE INDENT LOG . debug ( " Granting ▁ access . " ) NEW_LINE raise exception . DatastoreOperationNotSupported ( operation = ' grant _ access ' , datastore = MANAGER ) NEW_LINE DEDENT
def revoke_access ( self , context , username , hostname , database ) : NEW_LINE INDENT LOG . debug ( " Revoking ▁ access . " ) NEW_LINE raise exception . DatastoreOperationNotSupported ( operation = ' revoke _ access ' , datastore = MANAGER ) NEW_LINE DEDENT
def list_access ( self , context , username , hostname ) : NEW_LINE INDENT LOG . debug ( " Listing ▁ access . " ) NEW_LINE raise exception . DatastoreOperationNotSupported ( operation = ' list _ access ' , datastore = MANAGER ) NEW_LINE DEDENT
def list_databases ( self , context , limit = None , marker = None , include_marker = False ) : NEW_LINE INDENT LOG . debug ( " Listing ▁ databases . " ) NEW_LINE raise exception . DatastoreOperationNotSupported ( operation = ' list _ databases ' , datastore = MANAGER ) NEW_LINE DEDENT
def list_users ( self , context , limit = None , marker = None , include_marker = False ) : NEW_LINE INDENT LOG . debug ( " Listing ▁ users . " ) NEW_LINE raise exception . DatastoreOperationNotSupported ( operation = ' list _ users ' , datastore = MANAGER ) NEW_LINE DEDENT
def enable_root ( self , context ) : NEW_LINE INDENT LOG . debug ( " Enabling ▁ root . " ) NEW_LINE raise exception . DatastoreOperationNotSupported ( operation = ' enable _ root ' , datastore = MANAGER ) NEW_LINE DEDENT
def enable_root_with_password ( self , context , root_password = None ) : NEW_LINE INDENT LOG . debug ( " Enabling ▁ root ▁ with ▁ password . " ) NEW_LINE raise exception . DatastoreOperationNotSupported ( operation = ' enable _ root _ with _ password ' , datastore = MANAGER ) NEW_LINE DEDENT
def is_root_enabled ( self , context ) : NEW_LINE INDENT LOG . debug ( " Checking ▁ if ▁ root ▁ is ▁ enabled . " ) NEW_LINE raise exception . DatastoreOperationNotSupported ( operation = ' is _ root _ enabled ' , datastore = MANAGER ) NEW_LINE DEDENT
def backup_required_for_replication ( self , context ) : NEW_LINE INDENT replication = REPLICATION_STRATEGY_CLASS ( context ) NEW_LINE return replication . backup_required_for_replication ( ) NEW_LINE DEDENT
def get_replication_snapshot ( self , context , snapshot_info , replica_source_config = None ) : NEW_LINE INDENT LOG . debug ( " Getting ▁ replication ▁ snapshot . " ) NEW_LINE replication = REPLICATION_STRATEGY_CLASS ( context ) NEW_LINE replication . enable_as_master ( self . _app , replica_source_config ) NEW_LINE snapshot_id , log_position = ( replication . snapshot_for_replication ( context , self . _app , None , snapshot_info ) ) NEW_LINE mount_point = CONF . get ( MANAGER ) . mount_point NEW_LINE volume_stats = dbaas . get_filesystem_volume_stats ( mount_point ) NEW_LINE replication_snapshot = { ' dataset ' : { ' datastore _ manager ' : MANAGER , ' dataset _ size ' : volume_stats . get ( ' used ' , 0.0 ) , ' volume _ size ' : volume_stats . get ( ' total ' , 0.0 ) , ' snapshot _ id ' : snapshot_id } , ' replication _ strategy ' : REPLICATION_STRATEGY , ' master ' : replication . get_master_ref ( self . _app , snapshot_info ) , ' log _ position ' : log_position } NEW_LINE return replication_snapshot NEW_LINE DEDENT
def enable_as_master ( self , context , replica_source_config ) : NEW_LINE INDENT LOG . debug ( " Calling ▁ enable _ as _ master . " ) NEW_LINE replication = REPLICATION_STRATEGY_CLASS ( context ) NEW_LINE replication . enable_as_master ( self . _app , replica_source_config ) NEW_LINE DEDENT
def detach_replica ( self , context , for_failover = False ) : NEW_LINE INDENT LOG . debug ( " Detaching ▁ replica . " ) NEW_LINE replication = REPLICATION_STRATEGY_CLASS ( context ) NEW_LINE replica_info = replication . detach_slave ( self . _app , for_failover ) NEW_LINE return replica_info NEW_LINE DEDENT
def get_replica_context ( self , context ) : NEW_LINE INDENT LOG . debug ( " Getting ▁ replica ▁ context . " ) NEW_LINE replication = REPLICATION_STRATEGY_CLASS ( context ) NEW_LINE replica_info = replication . get_replica_context ( self . _app ) NEW_LINE return replica_info NEW_LINE DEDENT
def _validate_slave_for_replication ( self , context , replica_info ) : NEW_LINE INDENT if ( replica_info [ ' replication _ strategy ' ] != REPLICATION_STRATEGY ) : NEW_LINE INDENT raise exception . IncompatibleReplicationStrategy ( replica_info . update ( { ' guest _ strategy ' : REPLICATION_STRATEGY } ) ) NEW_LINE DEDENT DEDENT
def attach_replica ( self , context , replica_info , slave_config ) : NEW_LINE INDENT LOG . debug ( " Attaching ▁ replica . " ) NEW_LINE try : NEW_LINE INDENT if ' replication _ strategy ' in replica_info : NEW_LINE INDENT self . _validate_slave_for_replication ( context , replica_info ) NEW_LINE DEDENT replication = REPLICATION_STRATEGY_CLASS ( context ) NEW_LINE replication . enable_as_slave ( self . _app , replica_info , slave_config ) NEW_LINE DEDENT except Exception : NEW_LINE INDENT LOG . exception ( " Error ▁ enabling ▁ replication . " ) NEW_LINE self . _app . status . set_status ( rd_instance . ServiceStatuses . FAILED ) NEW_LINE raise NEW_LINE DEDENT DEDENT
def make_read_only ( self , context , read_only ) : NEW_LINE INDENT LOG . debug ( " Executing ▁ make _ read _ only ( % s ) " % read_only ) NEW_LINE self . _app . make_read_only ( read_only ) NEW_LINE DEDENT
def _get_repl_info ( self ) : NEW_LINE INDENT return self . _app . admin . get_info ( ' replication ' ) NEW_LINE DEDENT
def _get_master_host ( self ) : NEW_LINE INDENT slave_info = self . _get_repl_info ( ) NEW_LINE return slave_info and slave_info [ ' master _ host ' ] or None NEW_LINE DEDENT
def _get_repl_offset ( self ) : NEW_LINE INDENT repl_info = self . _get_repl_info ( ) NEW_LINE LOG . debug ( " Got ▁ repl ▁ info : ▁ % s " % repl_info ) NEW_LINE offset_key = ' % s _ repl _ offset ' % repl_info [ ' role ' ] NEW_LINE offset = repl_info [ offset_key ] NEW_LINE LOG . debug ( " Found ▁ offset ▁ % s ▁ for ▁ key ▁ % s . " % ( offset , offset_key ) ) NEW_LINE return int ( offset ) NEW_LINE DEDENT
def get_last_txn ( self , context ) : NEW_LINE INDENT master_host = self . _get_master_host ( ) NEW_LINE repl_offset = self . _get_repl_offset ( ) NEW_LINE return master_host , repl_offset NEW_LINE DEDENT
def get_latest_txn_id ( self , context ) : NEW_LINE INDENT LOG . info ( _ ( " Retrieving ▁ latest ▁ repl ▁ offset . " ) ) NEW_LINE return self . _get_repl_offset ( ) NEW_LINE DEDENT
def wait_for_txn ( self , context , txn ) : NEW_LINE INDENT LOG . info ( _ ( " Waiting ▁ on ▁ repl ▁ offset ▁ ' % s ' . " ) % txn ) NEW_LINE def _wait_for_txn ( ) : NEW_LINE INDENT current_offset = self . _get_repl_offset ( ) NEW_LINE LOG . debug ( " Current ▁ offset : ▁ % s . " % current_offset ) NEW_LINE return current_offset >= txn NEW_LINE DEDENT try : NEW_LINE INDENT utils . poll_until ( _wait_for_txn , time_out = 120 ) NEW_LINE DEDENT except exception . PollTimeOut : NEW_LINE INDENT raise RuntimeError ( _ ( " Timeout ▁ occurred ▁ waiting ▁ for ▁ Redis ▁ repl ▁ " " offset ▁ to ▁ change ▁ to ▁ ' % s ' . " ) % txn ) NEW_LINE DEDENT DEDENT
def cleanup_source_on_replica_detach ( self , context , replica_info ) : NEW_LINE INDENT LOG . debug ( " Cleaning ▁ up ▁ the ▁ source ▁ on ▁ the ▁ detach ▁ of ▁ a ▁ replica . " ) NEW_LINE replication = REPLICATION_STRATEGY_CLASS ( context ) NEW_LINE replication . cleanup_source_on_replica_detach ( self . _app , replica_info ) NEW_LINE DEDENT
def demote_replication_master ( self , context ) : NEW_LINE INDENT LOG . debug ( " Demoting ▁ replica ▁ source . " ) NEW_LINE replication = REPLICATION_STRATEGY_CLASS ( context ) NEW_LINE replication . demote_master ( self . _app ) NEW_LINE DEDENT
def cluster_meet ( self , context , ip , port ) : NEW_LINE INDENT LOG . debug ( " Executing ▁ cluster _ meet ▁ to ▁ join ▁ node ▁ to ▁ cluster . " ) NEW_LINE self . _app . cluster_meet ( ip , port ) NEW_LINE DEDENT
def get_node_ip ( self , context ) : NEW_LINE INDENT LOG . debug ( " Retrieving ▁ cluster ▁ node ▁ ip ▁ address . " ) NEW_LINE return self . _app . get_node_ip ( ) NEW_LINE DEDENT
def get_node_id_for_removal ( self , context ) : NEW_LINE INDENT LOG . debug ( " Validating ▁ removal ▁ of ▁ node ▁ from ▁ cluster . " ) NEW_LINE return self . _app . get_node_id_for_removal ( ) NEW_LINE DEDENT
def remove_nodes ( self , context , node_ids ) : NEW_LINE INDENT LOG . debug ( " Removing ▁ nodes ▁ from ▁ cluster . " ) NEW_LINE self . _app . remove_nodes ( node_ids ) NEW_LINE DEDENT
def cluster_addslots ( self , context , first_slot , last_slot ) : NEW_LINE INDENT LOG . debug ( " Executing ▁ cluster _ addslots ▁ to ▁ assign ▁ hash ▁ slots ▁ % s - % s . " , first_slot , last_slot ) NEW_LINE self . _app . cluster_addslots ( first_slot , last_slot ) NEW_LINE DEDENT
def cluster_complete ( self , context ) : NEW_LINE INDENT LOG . debug ( " Cluster ▁ creation ▁ complete , ▁ starting ▁ status ▁ checks . " ) NEW_LINE self . _app . complete_install_or_restart ( ) NEW_LINE DEDENT
def __init__ ( self , module , name , engine , cache_engine_version , node_type , num_nodes , cache_port , cache_parameter_group , cache_subnet_group , cache_security_groups , security_group_ids , zone , wait , hard_modify , region , ** aws_connect_kwargs ) : NEW_LINE INDENT self . module = module NEW_LINE self . name = name NEW_LINE self . engine = engine . lower ( ) NEW_LINE self . cache_engine_version = cache_engine_version NEW_LINE self . node_type = node_type NEW_LINE self . num_nodes = num_nodes NEW_LINE self . cache_port = cache_port NEW_LINE self . cache_parameter_group = cache_parameter_group NEW_LINE self . cache_subnet_group = cache_subnet_group NEW_LINE self . cache_security_groups = cache_security_groups NEW_LINE self . security_group_ids = security_group_ids NEW_LINE self . zone = zone NEW_LINE self . wait = wait NEW_LINE self . hard_modify = hard_modify NEW_LINE self . region = region NEW_LINE self . aws_connect_kwargs = aws_connect_kwargs NEW_LINE self . changed = False NEW_LINE self . data = None NEW_LINE self . status = ' gone ' NEW_LINE self . conn = self . _get_elasticache_connection ( ) NEW_LINE self . _refresh_data ( ) NEW_LINE DEDENT
def ensure_present ( self ) : NEW_LINE INDENT """ Ensure ▁ cache ▁ cluster ▁ exists ▁ or ▁ create ▁ it ▁ if ▁ not """ NEW_LINE if self . exists ( ) : NEW_LINE INDENT self . sync ( ) NEW_LINE DEDENT else : NEW_LINE INDENT self . create ( ) NEW_LINE DEDENT DEDENT
def ensure_absent ( self ) : NEW_LINE INDENT """ Ensure ▁ cache ▁ cluster ▁ is ▁ gone ▁ or ▁ delete ▁ it ▁ if ▁ not """ NEW_LINE self . delete ( ) NEW_LINE DEDENT
def ensure_rebooted ( self ) : NEW_LINE INDENT """ Ensure ▁ cache ▁ cluster ▁ is ▁ gone ▁ or ▁ delete ▁ it ▁ if ▁ not """ NEW_LINE self . reboot ( ) NEW_LINE DEDENT
def exists ( self ) : NEW_LINE INDENT """ Check ▁ if ▁ cache ▁ cluster ▁ exists """ NEW_LINE return self . status in self . EXIST_STATUSES NEW_LINE DEDENT
def create ( self ) : NEW_LINE INDENT """ Create ▁ an ▁ ElastiCache ▁ cluster """ NEW_LINE if self . status == ' available ' : NEW_LINE INDENT return NEW_LINE DEDENT if self . status in [ ' creating ' , ' rebooting ' , ' modifying ' ] : NEW_LINE INDENT if self . wait : NEW_LINE INDENT self . _wait_for_status ( ' available ' ) NEW_LINE DEDENT return NEW_LINE DEDENT if self . status == ' deleting ' : NEW_LINE INDENT if self . wait : NEW_LINE INDENT self . _wait_for_status ( ' gone ' ) NEW_LINE DEDENT else : NEW_LINE INDENT msg = " ' % s ' ▁ is ▁ currently ▁ deleting . ▁ Cannot ▁ create . " NEW_LINE self . module . fail_json ( msg = msg % self . name ) NEW_LINE DEDENT DEDENT kwargs = dict ( CacheClusterId = self . name , NumCacheNodes = self . num_nodes , CacheNodeType = self . node_type , Engine = self . engine , EngineVersion = self . cache_engine_version , CacheSecurityGroupNames = self . cache_security_groups , SecurityGroupIds = self . security_group_ids , CacheParameterGroupName = self . cache_parameter_group , CacheSubnetGroupName = self . cache_subnet_group ) NEW_LINE if self . cache_port is not None : NEW_LINE INDENT kwargs [ ' Port ' ] = self . cache_port NEW_LINE DEDENT if self . zone is not None : NEW_LINE INDENT kwargs [ ' PreferredAvailabilityZone ' ] = self . zone NEW_LINE DEDENT try : NEW_LINE INDENT self . conn . create_cache_cluster ( ** kwargs ) NEW_LINE DEDENT except botocore . exceptions . ClientError as e : NEW_LINE INDENT self . module . fail_json ( msg = e . message , exception = format_exc ( ) , ** camel_dict_to_snake_dict ( e . response ) ) NEW_LINE DEDENT self . _refresh_data ( ) NEW_LINE self . changed = True NEW_LINE if self . wait : NEW_LINE INDENT self . _wait_for_status ( ' available ' ) NEW_LINE DEDENT return True NEW_LINE DEDENT
def delete ( self ) : NEW_LINE INDENT """ Destroy ▁ an ▁ ElastiCache ▁ cluster """ NEW_LINE if self . status == ' gone ' : NEW_LINE INDENT return NEW_LINE DEDENT if self . status == ' deleting ' : NEW_LINE INDENT if self . wait : NEW_LINE INDENT self . _wait_for_status ( ' gone ' ) NEW_LINE DEDENT return NEW_LINE DEDENT if self . status in [ ' creating ' , ' rebooting ' , ' modifying ' ] : NEW_LINE INDENT if self . wait : NEW_LINE INDENT self . _wait_for_status ( ' available ' ) NEW_LINE DEDENT else : NEW_LINE INDENT msg = " ' % s ' ▁ is ▁ currently ▁ % s . ▁ Cannot ▁ delete . " NEW_LINE self . module . fail_json ( msg = msg % ( self . name , self . status ) ) NEW_LINE DEDENT DEDENT try : NEW_LINE INDENT response = self . conn . delete_cache_cluster ( CacheClusterId = self . name ) NEW_LINE DEDENT except botocore . exceptions . ClientError as e : NEW_LINE INDENT self . module . fail_json ( msg = e . message , exception = format_exc ( ) , ** camel_dict_to_snake_dict ( e . response ) ) NEW_LINE DEDENT cache_cluster_data = response [ ' CacheCluster ' ] NEW_LINE self . _refresh_data ( cache_cluster_data ) NEW_LINE self . changed = True NEW_LINE if self . wait : NEW_LINE INDENT self . _wait_for_status ( ' gone ' ) NEW_LINE DEDENT DEDENT
def sync ( self ) : NEW_LINE INDENT """ Sync ▁ settings ▁ to ▁ cluster ▁ if ▁ required """ NEW_LINE if not self . exists ( ) : NEW_LINE INDENT msg = " ' % s ' ▁ is ▁ % s . ▁ Cannot ▁ sync . " NEW_LINE self . module . fail_json ( msg = msg % ( self . name , self . status ) ) NEW_LINE DEDENT if self . status in [ ' creating ' , ' rebooting ' , ' modifying ' ] : NEW_LINE INDENT if self . wait : NEW_LINE INDENT self . _wait_for_status ( ' available ' ) NEW_LINE DEDENT else : NEW_LINE # ▁ Cluster ▁ can ▁ only ▁ be ▁ synced ▁ if ▁ available . ▁ If ▁ we ▁ can ' t ▁ wait ENDCOM # ▁ for ▁ this , ▁ then ▁ just ▁ be ▁ done . ENDCOM INDENT return NEW_LINE DEDENT DEDENT if self . _requires_destroy_and_create ( ) : NEW_LINE INDENT if not self . hard_modify : NEW_LINE INDENT msg = " ' % s ' ▁ requires ▁ destructive ▁ modification . ▁ ' hard _ modify ' ▁ must ▁ be ▁ set ▁ to ▁ true ▁ to ▁ proceed . " NEW_LINE self . module . fail_json ( msg = msg % self . name ) NEW_LINE DEDENT if not self . wait : NEW_LINE INDENT msg = " ' % s ' ▁ requires ▁ destructive ▁ modification . ▁ ' wait ' ▁ must ▁ be ▁ set ▁ to ▁ true . " NEW_LINE self . module . fail_json ( msg = msg % self . name ) NEW_LINE DEDENT self . delete ( ) NEW_LINE self . create ( ) NEW_LINE return NEW_LINE DEDENT if self . _requires_modification ( ) : NEW_LINE INDENT self . modify ( ) NEW_LINE DEDENT DEDENT
def modify ( self ) : NEW_LINE INDENT """ Modify ▁ the ▁ cache ▁ cluster . ▁ Note ▁ it ' s ▁ only ▁ possible ▁ to ▁ modify ▁ a ▁ few ▁ select ▁ options . """ NEW_LINE nodes_to_remove = self . _get_nodes_to_remove ( ) NEW_LINE try : NEW_LINE INDENT self . conn . modify_cache_cluster ( CacheClusterId = self . name , NumCacheNodes = self . num_nodes , CacheNodeIdsToRemove = nodes_to_remove , CacheSecurityGroupNames = self . cache_security_groups , CacheParameterGroupName = self . cache_parameter_group , SecurityGroupIds = self . security_group_ids , ApplyImmediately = True , EngineVersion = self . cache_engine_version ) NEW_LINE DEDENT except botocore . exceptions . ClientError as e : NEW_LINE INDENT self . module . fail_json ( msg = e . message , exception = format_exc ( ) , ** camel_dict_to_snake_dict ( e . response ) ) NEW_LINE DEDENT self . _refresh_data ( ) NEW_LINE self . changed = True NEW_LINE if self . wait : NEW_LINE INDENT self . _wait_for_status ( ' available ' ) NEW_LINE DEDENT DEDENT
def reboot ( self ) : NEW_LINE INDENT """ Reboot ▁ the ▁ cache ▁ cluster """ NEW_LINE if not self . exists ( ) : NEW_LINE INDENT msg = " ' % s ' ▁ is ▁ % s . ▁ Cannot ▁ reboot . " NEW_LINE self . module . fail_json ( msg = msg % ( self . name , self . status ) ) NEW_LINE DEDENT if self . status == ' rebooting ' : NEW_LINE INDENT return NEW_LINE DEDENT if self . status in [ ' creating ' , ' modifying ' ] : NEW_LINE INDENT if self . wait : NEW_LINE INDENT self . _wait_for_status ( ' available ' ) NEW_LINE DEDENT else : NEW_LINE INDENT msg = " ' % s ' ▁ is ▁ currently ▁ % s . ▁ Cannot ▁ reboot . " NEW_LINE self . module . fail_json ( msg = msg % ( self . name , self . status ) ) NEW_LINE # ▁ Collect ▁ ALL ▁ nodes ▁ for ▁ reboot ENDCOM DEDENT DEDENT cache_node_ids = [ cn [ ' CacheNodeId ' ] for cn in self . data [ ' CacheNodes ' ] ] NEW_LINE try : NEW_LINE INDENT self . conn . reboot_cache_cluster ( CacheClusterId = self . name , CacheNodeIdsToReboot = cache_node_ids ) NEW_LINE DEDENT except botocore . exceptions . ClientError as e : NEW_LINE INDENT self . module . fail_json ( msg = e . message , exception = format_exc ( ) , ** camel_dict_to_snake_dict ( e . response ) ) NEW_LINE DEDENT self . _refresh_data ( ) NEW_LINE self . changed = True NEW_LINE if self . wait : NEW_LINE INDENT self . _wait_for_status ( ' available ' ) NEW_LINE DEDENT DEDENT
def get_info ( self ) : NEW_LINE INDENT """ Return ▁ basic ▁ info ▁ about ▁ the ▁ cache ▁ cluster """ NEW_LINE info = { ' name ' : self . name , ' status ' : self . status } NEW_LINE if self . data : NEW_LINE INDENT info [ ' data ' ] = self . data NEW_LINE DEDENT return info NEW_LINE DEDENT
def _wait_for_status ( self , awaited_status ) : NEW_LINE INDENT """ Wait ▁ for ▁ status ▁ to ▁ change ▁ from ▁ present ▁ status ▁ to ▁ awaited _ status """ NEW_LINE status_map = { ' creating ' : ' available ' , ' rebooting ' : ' available ' , ' modifying ' : ' available ' , ' deleting ' : ' gone ' } NEW_LINE if self . status == awaited_status : NEW_LINE # ▁ No ▁ need ▁ to ▁ wait , ▁ we ' re ▁ already ▁ done ENDCOM INDENT return NEW_LINE DEDENT if status_map [ self . status ] != awaited_status : NEW_LINE INDENT msg = " Invalid ▁ awaited ▁ status . ▁ ' % s ' ▁ cannot ▁ transition ▁ to ▁ ' % s ' " NEW_LINE self . module . fail_json ( msg = msg % ( self . status , awaited_status ) ) NEW_LINE DEDENT if awaited_status not in set ( status_map . values ( ) ) : NEW_LINE INDENT msg = " ' % s ' ▁ is ▁ not ▁ a ▁ valid ▁ awaited ▁ status . " NEW_LINE self . module . fail_json ( msg = msg % awaited_status ) NEW_LINE DEDENT while True : NEW_LINE INDENT sleep ( 1 ) NEW_LINE self . _refresh_data ( ) NEW_LINE if self . status == awaited_status : NEW_LINE INDENT break NEW_LINE DEDENT DEDENT DEDENT
def _requires_modification ( self ) : NEW_LINE INDENT """ Check ▁ if ▁ cluster ▁ requires ▁ ( nondestructive ) ▁ modification """ NEW_LINE # ▁ Check ▁ modifiable ▁ data ▁ attributes ENDCOM modifiable_data = { ' NumCacheNodes ' : self . num_nodes , ' EngineVersion ' : self . cache_engine_version } NEW_LINE for key , value in modifiable_data . items ( ) : NEW_LINE INDENT if value is not None and value and self . data [ key ] != value : NEW_LINE INDENT return True NEW_LINE # ▁ Check ▁ cache ▁ security ▁ groups ENDCOM DEDENT DEDENT cache_security_groups = [ ] NEW_LINE for sg in self . data [ ' CacheSecurityGroups ' ] : NEW_LINE INDENT cache_security_groups . append ( sg [ ' CacheSecurityGroupName ' ] ) NEW_LINE DEDENT if set ( cache_security_groups ) != set ( self . cache_security_groups ) : NEW_LINE INDENT return True NEW_LINE # ▁ check ▁ vpc ▁ security ▁ groups ENDCOM DEDENT if self . security_group_ids : NEW_LINE INDENT vpc_security_groups = [ ] NEW_LINE security_groups = self . data [ ' SecurityGroups ' ] or [ ] NEW_LINE for sg in security_groups : NEW_LINE INDENT vpc_security_groups . append ( sg [ ' SecurityGroupId ' ] ) NEW_LINE DEDENT if set ( vpc_security_groups ) != set ( self . security_group_ids ) : NEW_LINE INDENT return True NEW_LINE DEDENT DEDENT return False NEW_LINE DEDENT
def _requires_destroy_and_create ( self ) : NEW_LINE INDENT """ STRNEWLINE ▁ Check ▁ whether ▁ a ▁ destroy ▁ and ▁ create ▁ is ▁ required ▁ to ▁ synchronize ▁ cluster . STRNEWLINE ▁ """ NEW_LINE unmodifiable_data = { ' node _ type ' : self . data [ ' CacheNodeType ' ] , ' engine ' : self . data [ ' Engine ' ] , ' cache _ port ' : self . _get_port ( ) } NEW_LINE # ▁ Only ▁ check ▁ for ▁ modifications ▁ if ▁ zone ▁ is ▁ specified ENDCOM if self . zone is not None : NEW_LINE INDENT unmodifiable_data [ ' zone ' ] = self . data [ ' PreferredAvailabilityZone ' ] NEW_LINE DEDENT for key , value in unmodifiable_data . items ( ) : NEW_LINE INDENT if getattr ( self , key ) is not None and getattr ( self , key ) != value : NEW_LINE INDENT return True NEW_LINE DEDENT DEDENT return False NEW_LINE DEDENT
def _get_elasticache_connection ( self ) : NEW_LINE INDENT """ Get ▁ an ▁ elasticache ▁ connection """ NEW_LINE region , ec2_url , aws_connect_params = get_aws_connection_info ( self . module , boto3 = True ) NEW_LINE if region : NEW_LINE INDENT return boto3_conn ( self . module , conn_type = ' client ' , resource = ' elasticache ' , region = region , endpoint = ec2_url , ** aws_connect_params ) NEW_LINE DEDENT else : NEW_LINE INDENT self . module . fail_json ( msg = " region ▁ must ▁ be ▁ specified " ) NEW_LINE DEDENT DEDENT
def _get_port ( self ) : NEW_LINE INDENT """ Get ▁ the ▁ port . ▁ Where ▁ this ▁ information ▁ is ▁ retrieved ▁ from ▁ is ▁ engine ▁ dependent . """ NEW_LINE if self . data [ ' Engine ' ] == ' memcached ' : NEW_LINE INDENT return self . data [ ' ConfigurationEndpoint ' ] [ ' Port ' ] NEW_LINE DEDENT elif self . data [ ' Engine ' ] == ' redis ' : NEW_LINE # ▁ Redis ▁ only ▁ supports ▁ a ▁ single ▁ node ▁ ( presently ) ▁ so ▁ just ▁ use ENDCOM # ▁ the ▁ first ▁ and ▁ only ENDCOM INDENT return self . data [ ' CacheNodes ' ] [ 0 ] [ ' Endpoint ' ] [ ' Port ' ] NEW_LINE DEDENT DEDENT
def _refresh_data ( self , cache_cluster_data = None ) : NEW_LINE INDENT """ Refresh ▁ data ▁ about ▁ this ▁ cache ▁ cluster """ NEW_LINE if cache_cluster_data is None : NEW_LINE INDENT try : NEW_LINE INDENT response = self . conn . describe_cache_clusters ( CacheClusterId = self . name , ShowCacheNodeInfo = True ) NEW_LINE DEDENT except botocore . exceptions . ClientError as e : NEW_LINE INDENT if e . response [ ' Error ' ] [ ' Code ' ] == ' CacheClusterNotFound ' : NEW_LINE INDENT self . data = None NEW_LINE self . status = ' gone ' NEW_LINE return NEW_LINE DEDENT else : NEW_LINE INDENT self . module . fail_json ( msg = e . message , exception = format_exc ( ) , ** camel_dict_to_snake_dict ( e . response ) ) NEW_LINE DEDENT DEDENT cache_cluster_data = response [ ' CacheClusters ' ] [ 0 ] NEW_LINE DEDENT self . data = cache_cluster_data NEW_LINE self . status = self . data [ ' CacheClusterStatus ' ] NEW_LINE # ▁ The ▁ documentation ▁ for ▁ elasticache ▁ lies ▁ - - ▁ status ▁ on ▁ rebooting ▁ is ▁ set ENDCOM # ▁ to ▁ ' rebooting ▁ cache ▁ cluster ▁ nodes ' ▁ instead ▁ of ▁ ' rebooting ' . ▁ Fix ▁ it ENDCOM # ▁ here ▁ to ▁ make ▁ status ▁ checks ▁ etc . ▁ more ▁ sane . ENDCOM if self . status == ' rebooting ▁ cache ▁ cluster ▁ nodes ' : NEW_LINE INDENT self . status = ' rebooting ' NEW_LINE DEDENT DEDENT
def _get_nodes_to_remove ( self ) : NEW_LINE INDENT """ If ▁ there ▁ are ▁ nodes ▁ to ▁ remove , ▁ it ▁ figures ▁ out ▁ which ▁ need ▁ to ▁ be ▁ removed """ NEW_LINE num_nodes_to_remove = self . data [ ' NumCacheNodes ' ] - self . num_nodes NEW_LINE if num_nodes_to_remove <= 0 : NEW_LINE INDENT return [ ] NEW_LINE DEDENT if not self . hard_modify : NEW_LINE INDENT msg = " ' % s ' ▁ requires ▁ removal ▁ of ▁ cache ▁ nodes . ▁ ' hard _ modify ' ▁ must ▁ be ▁ set ▁ to ▁ true ▁ to ▁ proceed . " NEW_LINE self . module . fail_json ( msg = msg % self . name ) NEW_LINE DEDENT cache_node_ids = [ cn [ ' CacheNodeId ' ] for cn in self . data [ ' CacheNodes ' ] ] NEW_LINE return cache_node_ids [ - num_nodes_to_remove : ] NEW_LINE DEDENT
def null_srid ( self ) : NEW_LINE INDENT """ STRNEWLINE ▁ Returns ▁ the ▁ proper ▁ null ▁ SRID ▁ depending ▁ on ▁ the ▁ GEOS ▁ version . STRNEWLINE ▁ See ▁ the ▁ comments ▁ in ▁ ` test15 _ srid ` ▁ for ▁ more ▁ details . STRNEWLINE ▁ """ NEW_LINE info = geos_version_info ( ) NEW_LINE if info [ ' version ' ] == '3.0.0' and info [ ' release _ candidate ' ] : NEW_LINE INDENT return - 1 NEW_LINE DEDENT else : NEW_LINE INDENT return None NEW_LINE DEDENT DEDENT
def test00_base ( self ) : NEW_LINE INDENT " Tests ▁ out ▁ the ▁ GEOSBase ▁ class . " NEW_LINE # ▁ Testing ▁ out ▁ GEOSBase ▁ class , ▁ which ▁ provides ▁ a ▁ ` ptr ` ▁ property ENDCOM # ▁ that ▁ abstracts ▁ out ▁ access ▁ to ▁ underlying ▁ C ▁ pointers . ENDCOM class FakeGeom1 ( GEOSBase ) : NEW_LINE INDENT pass NEW_LINE # ▁ This ▁ one ▁ only ▁ accepts ▁ pointers ▁ to ▁ floats ENDCOM DEDENT c_float_p = ctypes . POINTER ( ctypes . c_float ) NEW_LINE class FakeGeom2 ( GEOSBase ) : NEW_LINE INDENT ptr_type = c_float_p NEW_LINE # ▁ Default ▁ ptr _ type ▁ is ▁ ` c _ void _ p ` . ENDCOM DEDENT fg1 = FakeGeom1 ( ) NEW_LINE # ▁ Default ▁ ptr _ type ▁ is ▁ C ▁ float ▁ pointer ENDCOM fg2 = FakeGeom2 ( ) NEW_LINE # ▁ These ▁ assignments ▁ are ▁ OK ▁ - - ▁ None ▁ is ▁ allowed ▁ because ENDCOM # ▁ it ' s ▁ equivalent ▁ to ▁ the ▁ NULL ▁ pointer . ENDCOM fg1 . ptr = ctypes . c_void_p ( ) NEW_LINE fg1 . ptr = None NEW_LINE fg2 . ptr = c_float_p ( ctypes . c_float ( 5.23 ) ) NEW_LINE fg2 . ptr = None NEW_LINE # ▁ Because ▁ pointers ▁ have ▁ been ▁ set ▁ to ▁ NULL , ▁ an ▁ exception ▁ should ▁ be ENDCOM # ▁ raised ▁ when ▁ we ▁ try ▁ to ▁ access ▁ it . ▁ Raising ▁ an ▁ exception ▁ is ENDCOM # ▁ preferrable ▁ to ▁ a ▁ segmentation ▁ fault ▁ that ▁ commonly ▁ occurs ▁ when ENDCOM # ▁ a ▁ C ▁ method ▁ is ▁ given ▁ a ▁ NULL ▁ memory ▁ reference . ENDCOM for fg in ( fg1 , fg2 ) : NEW_LINE # ▁ Equivalent ▁ to ▁ ` fg . ptr ` ENDCOM INDENT self . assertRaises ( GEOSException , fg . _get_ptr ) NEW_LINE # ▁ Anything ▁ that ▁ is ▁ either ▁ not ▁ None ▁ or ▁ the ▁ acceptable ▁ pointer ▁ type ▁ will ENDCOM # ▁ result ▁ in ▁ a ▁ TypeError ▁ when ▁ trying ▁ to ▁ assign ▁ it ▁ to ▁ the ▁ ` ptr ` ▁ property . ENDCOM # ▁ Thus , ▁ memmory ▁ addresses ▁ ( integers ) ▁ and ▁ pointers ▁ of ▁ the ▁ incorrect ▁ type ENDCOM # ▁ ( in ▁ ` bad _ ptrs ` ) ▁ will ▁ not ▁ be ▁ allowed . ENDCOM DEDENT bad_ptrs = ( 5 , ctypes . c_char_p ( ' foobar ' ) ) NEW_LINE for bad_ptr in bad_ptrs : NEW_LINE # ▁ Equivalent ▁ to ▁ ` fg . ptr ▁ = ▁ bad _ ptr ` ENDCOM INDENT self . assertRaises ( TypeError , fg1 . _set_ptr , bad_ptr ) NEW_LINE self . assertRaises ( TypeError , fg2 . _set_ptr , bad_ptr ) NEW_LINE DEDENT DEDENT
def test01a_wkt ( self ) : NEW_LINE INDENT " Testing ▁ WKT ▁ output . " NEW_LINE for g in self . geometries . wkt_out : NEW_LINE INDENT geom = fromstr ( g . wkt ) NEW_LINE self . assertEqual ( g . ewkt , geom . wkt ) NEW_LINE DEDENT DEDENT
def test01b_hex ( self ) : NEW_LINE INDENT " Testing ▁ HEX ▁ output . " NEW_LINE for g in self . geometries . hex_wkt : NEW_LINE INDENT geom = fromstr ( g . wkt ) NEW_LINE self . assertEqual ( g . hex , geom . hex ) NEW_LINE DEDENT DEDENT
def test01b_hexewkb ( self ) : NEW_LINE INDENT " Testing ▁ ( HEX ) EWKB ▁ output . " NEW_LINE from binascii import a2b_hex NEW_LINE # ▁ For ▁ testing ▁ HEX ( EWKB ) . ENDCOM ogc_hex = '01010000000000000000000000000000000000F03F ' NEW_LINE # ▁ ` SELECT ▁ ST _ AsHEXEWKB ( ST _ GeomFromText ( ' POINT ( 0 ▁ 1 ) ' , ▁ 4326 ) ) ; ` ENDCOM hexewkb_2d = '0101000020E61000000000000000000000000000000000F03F ' NEW_LINE # ▁ ` SELECT ▁ ST _ AsHEXEWKB ( ST _ GeomFromEWKT ( ' SRID = 4326 ; POINT ( 0 ▁ 1 ▁ 2 ) ' ) ) ; ` ENDCOM hexewkb_3d = '01010000A0E61000000000000000000000000000000000F03F0000000000000040' NEW_LINE pnt_2d = Point ( 0 , 1 , srid = 4326 ) NEW_LINE pnt_3d = Point ( 0 , 1 , 2 , srid = 4326 ) NEW_LINE # ▁ OGC - compliant ▁ HEX ▁ will ▁ not ▁ have ▁ SRID ▁ nor ▁ Z ▁ value . ENDCOM self . assertEqual ( ogc_hex , pnt_2d . hex ) NEW_LINE self . assertEqual ( ogc_hex , pnt_3d . hex ) NEW_LINE # ▁ HEXEWKB ▁ should ▁ be ▁ appropriate ▁ for ▁ its ▁ dimension ▁ - - ▁ have ▁ to ▁ use ▁ an ENDCOM # ▁ a ▁ WKBWriter ▁ w / dimension ▁ set ▁ accordingly , ▁ else ▁ GEOS ▁ will ▁ insert ENDCOM # ▁ garbage ▁ into ▁ 3D ▁ coordinate ▁ if ▁ there ▁ is ▁ none . ▁ Also , ▁ GEOS ▁ has ▁ a ENDCOM # ▁ a ▁ bug ▁ in ▁ versions ▁ prior ▁ to ▁ 3.1 ▁ that ▁ puts ▁ the ▁ X ▁ coordinate ▁ in ENDCOM # ▁ place ▁ of ▁ Z ; ▁ an ▁ exception ▁ should ▁ be ▁ raised ▁ on ▁ those ▁ versions . ENDCOM self . assertEqual ( hexewkb_2d , pnt_2d . hexewkb ) NEW_LINE if GEOS_PREPARE : NEW_LINE INDENT self . assertEqual ( hexewkb_3d , pnt_3d . hexewkb ) NEW_LINE self . assertEqual ( True , GEOSGeometry ( hexewkb_3d ) . hasz ) NEW_LINE DEDENT else : NEW_LINE INDENT try : NEW_LINE INDENT hexewkb = pnt_3d . hexewkb NEW_LINE DEDENT except GEOSException : NEW_LINE INDENT pass NEW_LINE DEDENT else : NEW_LINE INDENT self . fail ( ' Should ▁ have ▁ raised ▁ GEOSException . ' ) NEW_LINE # ▁ Same ▁ for ▁ EWKB . ENDCOM DEDENT DEDENT self . assertEqual ( buffer ( a2b_hex ( hexewkb_2d ) ) , pnt_2d . ewkb ) NEW_LINE if GEOS_PREPARE : NEW_LINE INDENT self . assertEqual ( buffer ( a2b_hex ( hexewkb_3d ) ) , pnt_3d . ewkb ) NEW_LINE DEDENT else : NEW_LINE INDENT try : NEW_LINE INDENT ewkb = pnt_3d . ewkb NEW_LINE DEDENT except GEOSException : NEW_LINE INDENT pass NEW_LINE DEDENT else : NEW_LINE INDENT self . fail ( ' Should ▁ have ▁ raised ▁ GEOSException ' ) NEW_LINE # ▁ Redundant ▁ sanity ▁ check . ENDCOM DEDENT DEDENT self . assertEqual ( 4326 , GEOSGeometry ( hexewkb_2d ) . srid ) NEW_LINE DEDENT
def test01c_kml ( self ) : NEW_LINE INDENT " Testing ▁ KML ▁ output . " NEW_LINE for tg in self . geometries . wkt_out : NEW_LINE INDENT geom = fromstr ( tg . wkt ) NEW_LINE kml = getattr ( tg , ' kml ' , False ) NEW_LINE if kml : self . assertEqual ( kml , geom . kml ) NEW_LINE DEDENT DEDENT
def test01e_wkb ( self ) : NEW_LINE INDENT " Testing ▁ WKB ▁ output . " NEW_LINE from binascii import b2a_hex NEW_LINE for g in self . geometries . hex_wkt : NEW_LINE INDENT geom = fromstr ( g . wkt ) NEW_LINE wkb = geom . wkb NEW_LINE self . assertEqual ( b2a_hex ( wkb ) . upper ( ) , g . hex ) NEW_LINE DEDENT DEDENT
def test01f_create_hex ( self ) : NEW_LINE INDENT " Testing ▁ creation ▁ from ▁ HEX . " NEW_LINE for g in self . geometries . hex_wkt : NEW_LINE INDENT geom_h = GEOSGeometry ( g . hex ) NEW_LINE # ▁ we ▁ need ▁ to ▁ do ▁ this ▁ so ▁ decimal ▁ places ▁ get ▁ normalised ENDCOM geom_t = fromstr ( g . wkt ) NEW_LINE self . assertEqual ( geom_t . wkt , geom_h . wkt ) NEW_LINE DEDENT DEDENT
def test01g_create_wkb ( self ) : NEW_LINE INDENT " Testing ▁ creation ▁ from ▁ WKB . " NEW_LINE from binascii import a2b_hex NEW_LINE for g in self . geometries . hex_wkt : NEW_LINE INDENT wkb = buffer ( a2b_hex ( g . hex ) ) NEW_LINE geom_h = GEOSGeometry ( wkb ) NEW_LINE # ▁ we ▁ need ▁ to ▁ do ▁ this ▁ so ▁ decimal ▁ places ▁ get ▁ normalised ENDCOM geom_t = fromstr ( g . wkt ) NEW_LINE self . assertEqual ( geom_t . wkt , geom_h . wkt ) NEW_LINE DEDENT DEDENT
def test01h_ewkt ( self ) : NEW_LINE INDENT " Testing ▁ EWKT . " NEW_LINE srid = 32140 NEW_LINE for p in self . geometries . polygons : NEW_LINE INDENT ewkt = ' SRID = % d ; % s ' % ( srid , p . wkt ) NEW_LINE poly = fromstr ( ewkt ) NEW_LINE self . assertEqual ( srid , poly . srid ) NEW_LINE self . assertEqual ( srid , poly . shell . srid ) NEW_LINE self . assertEqual ( srid , fromstr ( poly . ewkt ) . srid ) # ▁ Checking ▁ export ENDCOM NEW_LINE DEDENT DEDENT
def test01i_json ( self ) : NEW_LINE INDENT " Testing ▁ GeoJSON ▁ input / output ▁ ( via ▁ GDAL ) . " NEW_LINE if not gdal or not gdal . GEOJSON : return NEW_LINE for g in self . geometries . json_geoms : NEW_LINE INDENT geom = GEOSGeometry ( g . wkt ) NEW_LINE if not hasattr ( g , ' not _ equal ' ) : NEW_LINE INDENT self . assertEqual ( g . json , geom . json ) NEW_LINE self . assertEqual ( g . json , geom . geojson ) NEW_LINE DEDENT self . assertEqual ( GEOSGeometry ( g . wkt ) , GEOSGeometry ( geom . json ) ) NEW_LINE DEDENT DEDENT
def test01k_fromfile ( self ) : NEW_LINE INDENT " Testing ▁ the ▁ fromfile ( ) ▁ factory . " NEW_LINE from StringIO import StringIO NEW_LINE ref_pnt = GEOSGeometry ( ' POINT ( 5 ▁ 23 ) ' ) NEW_LINE wkt_f = StringIO ( ) NEW_LINE wkt_f . write ( ref_pnt . wkt ) NEW_LINE wkb_f = StringIO ( ) NEW_LINE wkb_f . write ( str ( ref_pnt . wkb ) ) NEW_LINE # ▁ Other ▁ tests ▁ use ▁ ` fromfile ( ) ` ▁ on ▁ string ▁ filenames ▁ so ▁ those ENDCOM # ▁ aren ' t ▁ tested ▁ here . ENDCOM for fh in ( wkt_f , wkb_f ) : NEW_LINE INDENT fh . seek ( 0 ) NEW_LINE pnt = fromfile ( fh ) NEW_LINE self . assertEqual ( ref_pnt , pnt ) NEW_LINE DEDENT DEDENT
def test01k_eq ( self ) : NEW_LINE INDENT " Testing ▁ equivalence . " NEW_LINE p = fromstr ( ' POINT ( 5 ▁ 23 ) ' ) NEW_LINE self . assertEqual ( p , p . wkt ) NEW_LINE self . assertNotEqual ( p , ' foo ' ) NEW_LINE ls = fromstr ( ' LINESTRING ( 0 ▁ 0 , ▁ 1 ▁ 1 , ▁ 5 ▁ 5 ) ' ) NEW_LINE self . assertEqual ( ls , ls . wkt ) NEW_LINE self . assertNotEqual ( p , ' bar ' ) NEW_LINE # ▁ Error ▁ shouldn ' t ▁ be ▁ raise ▁ on ▁ equivalence ▁ testing ▁ with ENDCOM # ▁ an ▁ invalid ▁ type . ENDCOM for g in ( p , ls ) : NEW_LINE INDENT self . assertNotEqual ( g , None ) NEW_LINE self . assertNotEqual ( g , { ' foo ' : ' bar ' } ) NEW_LINE self . assertNotEqual ( g , False ) NEW_LINE DEDENT DEDENT
def test02a_points ( self ) : NEW_LINE INDENT " Testing ▁ Point ▁ objects . " NEW_LINE prev = fromstr ( ' POINT ( 0 ▁ 0 ) ' ) NEW_LINE for p in self . geometries . points : NEW_LINE # ▁ Creating ▁ the ▁ point ▁ from ▁ the ▁ WKT ENDCOM INDENT pnt = fromstr ( p . wkt ) NEW_LINE self . assertEqual ( pnt . geom_type , ' Point ' ) NEW_LINE self . assertEqual ( pnt . geom_typeid , 0 ) NEW_LINE self . assertEqual ( p . x , pnt . x ) NEW_LINE self . assertEqual ( p . y , pnt . y ) NEW_LINE self . assertEqual ( True , pnt == fromstr ( p . wkt ) ) NEW_LINE self . assertEqual ( False , pnt == prev ) NEW_LINE # ▁ Making ▁ sure ▁ that ▁ the ▁ point ' s ▁ X , ▁ Y ▁ components ▁ are ▁ what ▁ we ▁ expect ENDCOM self . assertAlmostEqual ( p . x , pnt . tuple [ 0 ] , 9 ) NEW_LINE self . assertAlmostEqual ( p . y , pnt . tuple [ 1 ] , 9 ) NEW_LINE # ▁ Testing ▁ the ▁ third ▁ dimension , ▁ and ▁ getting ▁ the ▁ tuple ▁ arguments ENDCOM if hasattr ( p , ' z ' ) : NEW_LINE INDENT self . assertEqual ( True , pnt . hasz ) NEW_LINE self . assertEqual ( p . z , pnt . z ) NEW_LINE self . assertEqual ( p . z , pnt . tuple [ 2 ] , 9 ) NEW_LINE tup_args = ( p . x , p . y , p . z ) NEW_LINE set_tup1 = ( 2.71 , 3.14 , 5.23 ) NEW_LINE set_tup2 = ( 5.23 , 2.71 , 3.14 ) NEW_LINE DEDENT else : NEW_LINE INDENT self . assertEqual ( False , pnt . hasz ) NEW_LINE self . assertEqual ( None , pnt . z ) NEW_LINE tup_args = ( p . x , p . y ) NEW_LINE set_tup1 = ( 2.71 , 3.14 ) NEW_LINE set_tup2 = ( 3.14 , 2.71 ) NEW_LINE # ▁ Centroid ▁ operation ▁ on ▁ point ▁ should ▁ be ▁ point ▁ itself ENDCOM DEDENT self . assertEqual ( p . centroid , pnt . centroid . tuple ) NEW_LINE # ▁ Now ▁ testing ▁ the ▁ different ▁ constructors ENDCOM pnt2 = Point ( tup_args ) # ▁ e . g . , ▁ Point ( (1 , ▁ 2 ) ) ENDCOM NEW_LINE pnt3 = Point ( * tup_args ) # ▁ e . g . , ▁ Point ( 1 , ▁ 2 ) ENDCOM NEW_LINE self . assertEqual ( True , pnt == pnt2 ) NEW_LINE self . assertEqual ( True , pnt == pnt3 ) NEW_LINE # ▁ Now ▁ testing ▁ setting ▁ the ▁ x ▁ and ▁ y ENDCOM pnt . y = 3.14 NEW_LINE pnt . x = 2.71 NEW_LINE self . assertEqual ( 3.14 , pnt . y ) NEW_LINE self . assertEqual ( 2.71 , pnt . x ) NEW_LINE # ▁ Setting ▁ via ▁ the ▁ tuple / coords ▁ property ENDCOM pnt . tuple = set_tup1 NEW_LINE self . assertEqual ( set_tup1 , pnt . tuple ) NEW_LINE pnt . coords = set_tup2 NEW_LINE self . assertEqual ( set_tup2 , pnt . coords ) NEW_LINE prev = pnt # ▁ setting ▁ the ▁ previous ▁ geometry ENDCOM NEW_LINE DEDENT DEDENT
def test02b_multipoints ( self ) : NEW_LINE INDENT " Testing ▁ MultiPoint ▁ objects . " NEW_LINE for mp in self . geometries . multipoints : NEW_LINE INDENT mpnt = fromstr ( mp . wkt ) NEW_LINE self . assertEqual ( mpnt . geom_type , ' MultiPoint ' ) NEW_LINE self . assertEqual ( mpnt . geom_typeid , 4 ) NEW_LINE self . assertAlmostEqual ( mp . centroid [ 0 ] , mpnt . centroid . tuple [ 0 ] , 9 ) NEW_LINE self . assertAlmostEqual ( mp . centroid [ 1 ] , mpnt . centroid . tuple [ 1 ] , 9 ) NEW_LINE self . assertRaises ( GEOSIndexError , mpnt . __getitem__ , len ( mpnt ) ) NEW_LINE self . assertEqual ( mp . centroid , mpnt . centroid . tuple ) NEW_LINE self . assertEqual ( mp . coords , tuple ( m . tuple for m in mpnt ) ) NEW_LINE for p in mpnt : NEW_LINE INDENT self . assertEqual ( p . geom_type , ' Point ' ) NEW_LINE self . assertEqual ( p . geom_typeid , 0 ) NEW_LINE self . assertEqual ( p . empty , False ) NEW_LINE self . assertEqual ( p . valid , True ) NEW_LINE DEDENT DEDENT DEDENT
def test03a_linestring ( self ) : NEW_LINE INDENT " Testing ▁ LineString ▁ objects . " NEW_LINE prev = fromstr ( ' POINT ( 0 ▁ 0 ) ' ) NEW_LINE for l in self . geometries . linestrings : NEW_LINE INDENT ls = fromstr ( l . wkt ) NEW_LINE self . assertEqual ( ls . geom_type , ' LineString ' ) NEW_LINE self . assertEqual ( ls . geom_typeid , 1 ) NEW_LINE self . assertEqual ( ls . empty , False ) NEW_LINE self . assertEqual ( ls . ring , False ) NEW_LINE if hasattr ( l , ' centroid ' ) : NEW_LINE INDENT self . assertEqual ( l . centroid , ls . centroid . tuple ) NEW_LINE DEDENT if hasattr ( l , ' tup ' ) : NEW_LINE INDENT self . assertEqual ( l . tup , ls . tuple ) NEW_LINE DEDENT self . assertEqual ( True , ls == fromstr ( l . wkt ) ) NEW_LINE self . assertEqual ( False , ls == prev ) NEW_LINE self . assertRaises ( GEOSIndexError , ls . __getitem__ , len ( ls ) ) NEW_LINE prev = ls NEW_LINE # ▁ Creating ▁ a ▁ LineString ▁ from ▁ a ▁ tuple , ▁ list , ▁ and ▁ numpy ▁ array ENDCOM self . assertEqual ( ls , LineString ( ls . tuple ) ) # ▁ tuple ENDCOM NEW_LINE self . assertEqual ( ls , LineString ( * ls . tuple ) ) # ▁ as ▁ individual ▁ arguments ENDCOM NEW_LINE self . assertEqual ( ls , LineString ( [ list ( tup ) for tup in ls . tuple ] ) ) # ▁ as ▁ list ENDCOM NEW_LINE self . assertEqual ( ls . wkt , LineString ( * tuple ( Point ( tup ) for tup in ls . tuple ) ) . wkt ) # ▁ Point ▁ individual ▁ arguments ENDCOM NEW_LINE if numpy : self . assertEqual ( ls , LineString ( numpy . array ( ls . tuple ) ) ) # ▁ as ▁ numpy ▁ array ENDCOM NEW_LINE DEDENT DEDENT
def test03b_multilinestring ( self ) : NEW_LINE INDENT " Testing ▁ MultiLineString ▁ objects . " NEW_LINE prev = fromstr ( ' POINT ( 0 ▁ 0 ) ' ) NEW_LINE for l in self . geometries . multilinestrings : NEW_LINE INDENT ml = fromstr ( l . wkt ) NEW_LINE self . assertEqual ( ml . geom_type , ' MultiLineString ' ) NEW_LINE self . assertEqual ( ml . geom_typeid , 5 ) NEW_LINE self . assertAlmostEqual ( l . centroid [ 0 ] , ml . centroid . x , 9 ) NEW_LINE self . assertAlmostEqual ( l . centroid [ 1 ] , ml . centroid . y , 9 ) NEW_LINE self . assertEqual ( True , ml == fromstr ( l . wkt ) ) NEW_LINE self . assertEqual ( False , ml == prev ) NEW_LINE prev = ml NEW_LINE for ls in ml : NEW_LINE INDENT self . assertEqual ( ls . geom_type , ' LineString ' ) NEW_LINE self . assertEqual ( ls . geom_typeid , 1 ) NEW_LINE self . assertEqual ( ls . empty , False ) NEW_LINE DEDENT self . assertRaises ( GEOSIndexError , ml . __getitem__ , len ( ml ) ) NEW_LINE self . assertEqual ( ml . wkt , MultiLineString ( * tuple ( s . clone ( ) for s in ml ) ) . wkt ) NEW_LINE self . assertEqual ( ml , MultiLineString ( * tuple ( LineString ( s . tuple ) for s in ml ) ) ) NEW_LINE DEDENT DEDENT
def test04_linearring ( self ) : NEW_LINE INDENT " Testing ▁ LinearRing ▁ objects . " NEW_LINE for rr in self . geometries . linearrings : NEW_LINE INDENT lr = fromstr ( rr . wkt ) NEW_LINE self . assertEqual ( lr . geom_type , ' LinearRing ' ) NEW_LINE self . assertEqual ( lr . geom_typeid , 2 ) NEW_LINE self . assertEqual ( rr . n_p , len ( lr ) ) NEW_LINE self . assertEqual ( True , lr . valid ) NEW_LINE self . assertEqual ( False , lr . empty ) NEW_LINE # ▁ Creating ▁ a ▁ LinearRing ▁ from ▁ a ▁ tuple , ▁ list , ▁ and ▁ numpy ▁ array ENDCOM self . assertEqual ( lr , LinearRing ( lr . tuple ) ) NEW_LINE self . assertEqual ( lr , LinearRing ( * lr . tuple ) ) NEW_LINE self . assertEqual ( lr , LinearRing ( [ list ( tup ) for tup in lr . tuple ] ) ) NEW_LINE if numpy : self . assertEqual ( lr , LinearRing ( numpy . array ( lr . tuple ) ) ) NEW_LINE DEDENT DEDENT
def test05a_polygons ( self ) : NEW_LINE INDENT " Testing ▁ Polygon ▁ objects . " NEW_LINE # ▁ Testing ▁ ` from _ bbox ` ▁ class ▁ method ENDCOM bbox = ( - 180 , - 90 , 180 , 90 ) NEW_LINE p = Polygon . from_bbox ( bbox ) NEW_LINE self . assertEqual ( bbox , p . extent ) NEW_LINE prev = fromstr ( ' POINT ( 0 ▁ 0 ) ' ) NEW_LINE for p in self . geometries . polygons : NEW_LINE # ▁ Creating ▁ the ▁ Polygon , ▁ testing ▁ its ▁ properties . ENDCOM INDENT poly = fromstr ( p . wkt ) NEW_LINE self . assertEqual ( poly . geom_type , ' Polygon ' ) NEW_LINE self . assertEqual ( poly . geom_typeid , 3 ) NEW_LINE self . assertEqual ( poly . empty , False ) NEW_LINE self . assertEqual ( poly . ring , False ) NEW_LINE self . assertEqual ( p . n_i , poly . num_interior_rings ) NEW_LINE self . assertEqual ( p . n_i + 1 , len ( poly ) ) # ▁ Testing ▁ _ _ len _ _ ENDCOM NEW_LINE self . assertEqual ( p . n_p , poly . num_points ) NEW_LINE # ▁ Area ▁ & ▁ Centroid ENDCOM self . assertAlmostEqual ( p . area , poly . area , 9 ) NEW_LINE self . assertAlmostEqual ( p . centroid [ 0 ] , poly . centroid . tuple [ 0 ] , 9 ) NEW_LINE self . assertAlmostEqual ( p . centroid [ 1 ] , poly . centroid . tuple [ 1 ] , 9 ) NEW_LINE # ▁ Testing ▁ the ▁ geometry ▁ equivalence ENDCOM self . assertEqual ( True , poly == fromstr ( p . wkt ) ) NEW_LINE self . assertEqual ( False , poly == prev ) # ▁ Should ▁ not ▁ be ▁ equal ▁ to ▁ previous ▁ geometry ENDCOM NEW_LINE self . assertEqual ( True , poly != prev ) NEW_LINE # ▁ Testing ▁ the ▁ exterior ▁ ring ENDCOM ring = poly . exterior_ring NEW_LINE self . assertEqual ( ring . geom_type , ' LinearRing ' ) NEW_LINE self . assertEqual ( ring . geom_typeid , 2 ) NEW_LINE if p . ext_ring_cs : NEW_LINE INDENT self . assertEqual ( p . ext_ring_cs , ring . tuple ) NEW_LINE self . assertEqual ( p . ext_ring_cs , poly [ 0 ] . tuple ) # ▁ Testing ▁ _ _ getitem _ _ ENDCOM NEW_LINE # ▁ Testing ▁ _ _ getitem _ _ ▁ and ▁ _ _ setitem _ _ ▁ on ▁ invalid ▁ indices ENDCOM DEDENT self . assertRaises ( GEOSIndexError , poly . __getitem__ , len ( poly ) ) NEW_LINE self . assertRaises ( GEOSIndexError , poly . __setitem__ , len ( poly ) , False ) NEW_LINE self . assertRaises ( GEOSIndexError , poly . __getitem__ , - 1 * len ( poly ) - 1 ) NEW_LINE # ▁ Testing ▁ _ _ iter _ _ ENDCOM for r in poly : NEW_LINE INDENT self . assertEqual ( r . geom_type , ' LinearRing ' ) NEW_LINE self . assertEqual ( r . geom_typeid , 2 ) NEW_LINE # ▁ Testing ▁ polygon ▁ construction . ENDCOM DEDENT self . assertRaises ( TypeError , Polygon . __init__ , 0 , [ 1 , 2 , 3 ] ) NEW_LINE self . assertRaises ( TypeError , Polygon . __init__ , ' foo ' ) NEW_LINE # ▁ Polygon ( shell , ▁ ( hole1 , ▁ . . . ▁ holeN ) ) ENDCOM rings = tuple ( r for r in poly ) NEW_LINE self . assertEqual ( poly , Polygon ( rings [ 0 ] , rings [ 1 : ] ) ) NEW_LINE # ▁ Polygon ( shell _ tuple , ▁ hole _ tuple1 , ▁ . . . ▁ , ▁ hole _ tupleN ) ENDCOM ring_tuples = tuple ( r . tuple for r in poly ) NEW_LINE self . assertEqual ( poly , Polygon ( * ring_tuples ) ) NEW_LINE # ▁ Constructing ▁ with ▁ tuples ▁ of ▁ LinearRings . ENDCOM self . assertEqual ( poly . wkt , Polygon ( * tuple ( r for r in poly ) ) . wkt ) NEW_LINE self . assertEqual ( poly . wkt , Polygon ( * tuple ( LinearRing ( r . tuple ) for r in poly ) ) . wkt ) NEW_LINE DEDENT DEDENT
def test06a_memory_hijinks ( self ) : NEW_LINE INDENT " Testing ▁ Geometry ▁ _ _ del _ _ ( ) ▁ on ▁ rings ▁ and ▁ polygons . " NEW_LINE # # # # ▁ Memory ▁ issues ▁ with ▁ rings ▁ and ▁ polygons ENDCOM # ▁ These ▁ tests ▁ are ▁ needed ▁ to ▁ ensure ▁ sanity ▁ with ▁ writable ▁ geometries . ENDCOM # ▁ Getting ▁ a ▁ polygon ▁ with ▁ interior ▁ rings , ▁ and ▁ pulling ▁ out ▁ the ▁ interior ▁ rings ENDCOM poly = fromstr ( self . geometries . polygons [ 1 ] . wkt ) NEW_LINE ring1 = poly [ 0 ] NEW_LINE ring2 = poly [ 1 ] NEW_LINE # ▁ These ▁ deletes ▁ should ▁ be ▁ ' harmless ' ▁ since ▁ they ▁ are ▁ done ▁ on ▁ child ▁ geometries ENDCOM del ring1 NEW_LINE del ring2 NEW_LINE ring1 = poly [ 0 ] NEW_LINE ring2 = poly [ 1 ] NEW_LINE # ▁ Deleting ▁ the ▁ polygon ENDCOM del poly NEW_LINE # ▁ Access ▁ to ▁ these ▁ rings ▁ is ▁ OK ▁ since ▁ they ▁ are ▁ clones . ENDCOM s1 , s2 = str ( ring1 ) , str ( ring2 ) NEW_LINE DEDENT
def test09_relate_pattern ( self ) : NEW_LINE INDENT " Testing ▁ relate ( ) ▁ and ▁ relate _ pattern ( ) . " NEW_LINE g = fromstr ( ' POINT ▁ ( 0 ▁ 0 ) ' ) NEW_LINE self . assertRaises ( GEOSException , g . relate_pattern , 0 , ' invalid ▁ pattern , ▁ yo ' ) NEW_LINE for rg in self . geometries . relate_geoms : NEW_LINE INDENT a = fromstr ( rg . wkt_a ) NEW_LINE b = fromstr ( rg . wkt_b ) NEW_LINE self . assertEqual ( rg . result , a . relate_pattern ( b , rg . pattern ) ) NEW_LINE self . assertEqual ( rg . pattern , a . relate ( b ) ) NEW_LINE DEDENT DEDENT
def test15_srid ( self ) : NEW_LINE INDENT " Testing ▁ the ▁ SRID ▁ property ▁ and ▁ keyword . " NEW_LINE # ▁ Testing ▁ SRID ▁ keyword ▁ on ▁ Point ENDCOM pnt = Point ( 5 , 23 , srid = 4326 ) NEW_LINE self . assertEqual ( 4326 , pnt . srid ) NEW_LINE pnt . srid = 3084 NEW_LINE self . assertEqual ( 3084 , pnt . srid ) NEW_LINE self . assertRaises ( ctypes . ArgumentError , pnt . set_srid , '4326' ) NEW_LINE # ▁ Testing ▁ SRID ▁ keyword ▁ on ▁ fromstr ( ) , ▁ and ▁ on ▁ Polygon ▁ rings . ENDCOM poly = fromstr ( self . geometries . polygons [ 1 ] . wkt , srid = 4269 ) NEW_LINE self . assertEqual ( 4269 , poly . srid ) NEW_LINE for ring in poly : self . assertEqual ( 4269 , ring . srid ) NEW_LINE poly . srid = 4326 NEW_LINE self . assertEqual ( 4326 , poly . shell . srid ) NEW_LINE # ▁ Testing ▁ SRID ▁ keyword ▁ on ▁ GeometryCollection ENDCOM gc = GeometryCollection ( Point ( 5 , 23 ) , LineString ( ( 0 , 0 ) , ( 1.5 , 1.5 ) , ( 3 , 3 ) ) , srid = 32021 ) NEW_LINE self . assertEqual ( 32021 , gc . srid ) NEW_LINE for i in range ( len ( gc ) ) : self . assertEqual ( 32021 , gc [ i ] . srid ) NEW_LINE # ▁ GEOS ▁ may ▁ get ▁ the ▁ SRID ▁ from ▁ HEXEWKB ENDCOM # ▁ ' POINT ( 5 ▁ 23 ) ' ▁ at ▁ SRID = 4326 ▁ in ▁ hex ▁ form ▁ - - ▁ obtained ▁ from ▁ PostGIS ENDCOM # ▁ using ▁ ` SELECT ▁ GeomFromText ( ' POINT ▁ ( 5 ▁ 23 ) ' , ▁ 4326 ) ; ` . ENDCOM hex = '0101000020E610000000000000000014400000000000003740' NEW_LINE p1 = fromstr ( hex ) NEW_LINE self . assertEqual ( 4326 , p1 . srid ) NEW_LINE # ▁ In ▁ GEOS ▁ 3.0.0rc1-4 ▁ when ▁ the ▁ EWKB ▁ and / or ▁ HEXEWKB ▁ is ▁ exported , ENDCOM # ▁ the ▁ SRID ▁ information ▁ is ▁ lost ▁ and ▁ set ▁ to ▁ - 1 ▁ - - ▁ this ▁ is ▁ not ▁ a ENDCOM # ▁ problem ▁ on ▁ the ▁ 3.0.0 ▁ version ▁ ( another ▁ reason ▁ to ▁ upgrade ) . ENDCOM exp_srid = self . null_srid NEW_LINE p2 = fromstr ( p1 . hex ) NEW_LINE self . assertEqual ( exp_srid , p2 . srid ) NEW_LINE p3 = fromstr ( p1 . hex , srid = - 1 ) # ▁ - 1 ▁ is ▁ intended . ENDCOM NEW_LINE self . assertEqual ( - 1 , p3 . srid ) NEW_LINE DEDENT
def test17_threed ( self ) : NEW_LINE INDENT " Testing ▁ three - dimensional ▁ geometries . " NEW_LINE # ▁ Testing ▁ a ▁ 3D ▁ Point ENDCOM pnt = Point ( 2 , 3 , 8 ) NEW_LINE self . assertEqual ( ( 2. , 3. , 8. ) , pnt . coords ) NEW_LINE self . assertRaises ( TypeError , pnt . set_coords , ( 1. , 2. ) ) NEW_LINE pnt . coords = ( 1. , 2. , 3. ) NEW_LINE self . assertEqual ( ( 1. , 2. , 3. ) , pnt . coords ) NEW_LINE # ▁ Testing ▁ a ▁ 3D ▁ LineString ENDCOM ls = LineString ( ( 2. , 3. , 8. ) , ( 50. , 250. , - 117. ) ) NEW_LINE self . assertEqual ( ( ( 2. , 3. , 8. ) , ( 50. , 250. , - 117. ) ) , ls . tuple ) NEW_LINE self . assertRaises ( TypeError , ls . __setitem__ , 0 , ( 1. , 2. ) ) NEW_LINE ls [ 0 ] = ( 1. , 2. , 3. ) NEW_LINE self . assertEqual ( ( 1. , 2. , 3. ) , ls [ 0 ] ) NEW_LINE DEDENT
def test18_distance ( self ) : NEW_LINE INDENT " Testing ▁ the ▁ distance ( ) ▁ function . " NEW_LINE # ▁ Distance ▁ to ▁ self ▁ should ▁ be ▁ 0 . ENDCOM pnt = Point ( 0 , 0 ) NEW_LINE self . assertEqual ( 0.0 , pnt . distance ( Point ( 0 , 0 ) ) ) NEW_LINE # ▁ Distance ▁ should ▁ be ▁ 1 ENDCOM self . assertEqual ( 1.0 , pnt . distance ( Point ( 0 , 1 ) ) ) NEW_LINE # ▁ Distance ▁ should ▁ be ▁ ~ ▁ sqrt ( 2 ) ENDCOM self . assertAlmostEqual ( 1.41421356237 , pnt . distance ( Point ( 1 , 1 ) ) , 11 ) NEW_LINE # ▁ Distances ▁ are ▁ from ▁ the ▁ closest ▁ vertex ▁ in ▁ each ▁ geometry ▁ - - ENDCOM # ▁ should ▁ be ▁ 3 ▁ ( distance ▁ from ▁ ( 2 , ▁ 2 ) ▁ to ▁ ( 5 , ▁ 2 ) ) . ENDCOM ls1 = LineString ( ( 0 , 0 ) , ( 1 , 1 ) , ( 2 , 2 ) ) NEW_LINE ls2 = LineString ( ( 5 , 2 ) , ( 6 , 1 ) , ( 7 , 0 ) ) NEW_LINE self . assertEqual ( 3 , ls1 . distance ( ls2 ) ) NEW_LINE DEDENT
def test19_length ( self ) : NEW_LINE INDENT " Testing ▁ the ▁ length ▁ property . " NEW_LINE # ▁ Points ▁ have ▁ 0 ▁ length . ENDCOM pnt = Point ( 0 , 0 ) NEW_LINE self . assertEqual ( 0.0 , pnt . length ) NEW_LINE # ▁ Should ▁ be ▁ ~ ▁ sqrt ( 2 ) ENDCOM ls = LineString ( ( 0 , 0 ) , ( 1 , 1 ) ) NEW_LINE self . assertAlmostEqual ( 1.41421356237 , ls . length , 11 ) NEW_LINE # ▁ Should ▁ be ▁ circumfrence ▁ of ▁ Polygon ENDCOM poly = Polygon ( LinearRing ( ( 0 , 0 ) , ( 0 , 1 ) , ( 1 , 1 ) , ( 1 , 0 ) , ( 0 , 0 ) ) ) NEW_LINE self . assertEqual ( 4.0 , poly . length ) NEW_LINE # ▁ Should ▁ be ▁ sum ▁ of ▁ each ▁ element ' s ▁ length ▁ in ▁ collection . ENDCOM mpoly = MultiPolygon ( poly . clone ( ) , poly ) NEW_LINE self . assertEqual ( 8.0 , mpoly . length ) NEW_LINE DEDENT
def test20a_emptyCollections ( self ) : NEW_LINE INDENT " Testing ▁ empty ▁ geometries ▁ and ▁ collections . " NEW_LINE gc1 = GeometryCollection ( [ ] ) NEW_LINE gc2 = fromstr ( ' GEOMETRYCOLLECTION ▁ EMPTY ' ) NEW_LINE pnt = fromstr ( ' POINT ▁ EMPTY ' ) NEW_LINE ls = fromstr ( ' LINESTRING ▁ EMPTY ' ) NEW_LINE poly = fromstr ( ' POLYGON ▁ EMPTY ' ) NEW_LINE mls = fromstr ( ' MULTILINESTRING ▁ EMPTY ' ) NEW_LINE mpoly1 = fromstr ( ' MULTIPOLYGON ▁ EMPTY ' ) NEW_LINE mpoly2 = MultiPolygon ( ( ) ) NEW_LINE for g in [ gc1 , gc2 , pnt , ls , poly , mls , mpoly1 , mpoly2 ] : NEW_LINE INDENT self . assertEqual ( True , g . empty ) NEW_LINE # ▁ Testing ▁ len ( ) ▁ and ▁ num _ geom . ENDCOM if isinstance ( g , Polygon ) : NEW_LINE INDENT self . assertEqual ( 1 , len ( g ) ) # ▁ Has ▁ one ▁ empty ▁ linear ▁ ring ENDCOM NEW_LINE self . assertEqual ( 1 , g . num_geom ) NEW_LINE self . assertEqual ( 0 , len ( g [ 0 ] ) ) NEW_LINE DEDENT elif isinstance ( g , ( Point , LineString ) ) : NEW_LINE INDENT self . assertEqual ( 1 , g . num_geom ) NEW_LINE self . assertEqual ( 0 , len ( g ) ) NEW_LINE DEDENT else : NEW_LINE INDENT self . assertEqual ( 0 , g . num_geom ) NEW_LINE self . assertEqual ( 0 , len ( g ) ) NEW_LINE # ▁ Testing ▁ _ _ getitem _ _ ▁ ( doesn ' t ▁ work ▁ on ▁ Point ▁ or ▁ Polygon ) ENDCOM DEDENT if isinstance ( g , Point ) : NEW_LINE INDENT self . assertRaises ( GEOSIndexError , g . get_x ) NEW_LINE DEDENT elif isinstance ( g , Polygon ) : NEW_LINE INDENT lr = g . shell NEW_LINE self . assertEqual ( ' LINEARRING ▁ EMPTY ' , lr . wkt ) NEW_LINE self . assertEqual ( 0 , len ( lr ) ) NEW_LINE self . assertEqual ( True , lr . empty ) NEW_LINE self . assertRaises ( GEOSIndexError , lr . __getitem__ , 0 ) NEW_LINE DEDENT else : NEW_LINE INDENT self . assertRaises ( GEOSIndexError , g . __getitem__ , 0 ) NEW_LINE DEDENT DEDENT DEDENT
def test20b_collections_of_collections ( self ) : NEW_LINE INDENT " Testing ▁ GeometryCollection ▁ handling ▁ of ▁ other ▁ collections . " NEW_LINE # ▁ Creating ▁ a ▁ GeometryCollection ▁ WKT ▁ string ▁ composed ▁ of ▁ other ENDCOM # ▁ collections ▁ and ▁ polygons . ENDCOM coll = [ mp . wkt for mp in self . geometries . multipolygons if mp . valid ] NEW_LINE coll . extend ( [ mls . wkt for mls in self . geometries . multilinestrings ] ) NEW_LINE coll . extend ( [ p . wkt for p in self . geometries . polygons ] ) NEW_LINE coll . extend ( [ mp . wkt for mp in self . geometries . multipoints ] ) NEW_LINE gc_wkt = ' GEOMETRYCOLLECTION ( % s ) ' % ' , ' . join ( coll ) NEW_LINE # ▁ Should ▁ construct ▁ ok ▁ from ▁ WKT ENDCOM gc1 = GEOSGeometry ( gc_wkt ) NEW_LINE # ▁ Should ▁ also ▁ construct ▁ ok ▁ from ▁ individual ▁ geometry ▁ arguments . ENDCOM gc2 = GeometryCollection ( * tuple ( g for g in gc1 ) ) NEW_LINE # ▁ And , ▁ they ▁ should ▁ be ▁ equal . ENDCOM self . assertEqual ( gc1 , gc2 ) NEW_LINE DEDENT
def test21_test_gdal ( self ) : NEW_LINE INDENT " Testing ▁ ` ogr ` ▁ and ▁ ` srs ` ▁ properties . " NEW_LINE if not gdal . HAS_GDAL : return NEW_LINE g1 = fromstr ( ' POINT ( 5 ▁ 23 ) ' ) NEW_LINE self . assertEqual ( True , isinstance ( g1 . ogr , gdal . OGRGeometry ) ) NEW_LINE self . assertEqual ( g1 . srs , None ) NEW_LINE g2 = fromstr ( ' LINESTRING ( 0 ▁ 0 , ▁ 5 ▁ 5 , ▁ 23 ▁ 23 ) ' , srid = 4326 ) NEW_LINE self . assertEqual ( True , isinstance ( g2 . ogr , gdal . OGRGeometry ) ) NEW_LINE self . assertEqual ( True , isinstance ( g2 . srs , gdal . SpatialReference ) ) NEW_LINE self . assertEqual ( g2 . hex , g2 . ogr . hex ) NEW_LINE self . assertEqual ( ' WGS ▁ 84' , g2 . srs . name ) NEW_LINE DEDENT
def test22_copy ( self ) : NEW_LINE INDENT " Testing ▁ use ▁ with ▁ the ▁ Python ▁ ` copy ` ▁ module . " NEW_LINE import django . utils . copycompat as copy NEW_LINE poly = GEOSGeometry ( ' POLYGON ( (0 ▁ 0 , ▁ 0 ▁ 23 , ▁ 23 ▁ 23 , ▁ 23 ▁ 0 , ▁ 0 ▁ 0 ) , ▁ ( 5 ▁ 5 , ▁ 5 ▁ 10 , ▁ 10 ▁ 10 , ▁ 10 ▁ 5 , ▁ 5 ▁ 5 ) ) ' ) NEW_LINE cpy1 = copy . copy ( poly ) NEW_LINE cpy2 = copy . deepcopy ( poly ) NEW_LINE self . assertNotEqual ( poly . _ptr , cpy1 . _ptr ) NEW_LINE self . assertNotEqual ( poly . _ptr , cpy2 . _ptr ) NEW_LINE DEDENT
def test23_transform ( self ) : NEW_LINE INDENT " Testing ▁ ` transform ` ▁ method . " NEW_LINE if not gdal . HAS_GDAL : return NEW_LINE orig = GEOSGeometry ( ' POINT ▁ ( -104.609 ▁ 38.255 ) ' , 4326 ) NEW_LINE trans = GEOSGeometry ( ' POINT ▁ ( 992385.4472045 ▁ 481455.4944650 ) ' , 2774 ) NEW_LINE # ▁ Using ▁ a ▁ srid , ▁ a ▁ SpatialReference ▁ object , ▁ and ▁ a ▁ CoordTransform ▁ object ENDCOM # ▁ for ▁ transformations . ENDCOM t1 , t2 , t3 = orig . clone ( ) , orig . clone ( ) , orig . clone ( ) NEW_LINE t1 . transform ( trans . srid ) NEW_LINE t2 . transform ( gdal . SpatialReference ( ' EPSG : 2774' ) ) NEW_LINE ct = gdal . CoordTransform ( gdal . SpatialReference ( ' WGS84' ) , gdal . SpatialReference ( 2774 ) ) NEW_LINE t3 . transform ( ct ) NEW_LINE # ▁ Testing ▁ use ▁ of ▁ the ▁ ` clone ` ▁ keyword . ENDCOM k1 = orig . clone ( ) NEW_LINE k2 = k1 . transform ( trans . srid , clone = True ) NEW_LINE self . assertEqual ( k1 , orig ) NEW_LINE self . assertNotEqual ( k1 , k2 ) NEW_LINE prec = 3 NEW_LINE for p in ( t1 , t2 , t3 , k2 ) : NEW_LINE INDENT self . assertAlmostEqual ( trans . x , p . x , prec ) NEW_LINE self . assertAlmostEqual ( trans . y , p . y , prec ) NEW_LINE DEDENT DEDENT
def test23_transform_noop ( self ) : NEW_LINE INDENT """ ▁ Testing ▁ ` transform ` ▁ method ▁ ( SRID ▁ match ) ▁ """ NEW_LINE # ▁ transform ( ) ▁ should ▁ no - op ▁ if ▁ source ▁ & ▁ dest ▁ SRIDs ▁ match , ENDCOM # ▁ regardless ▁ of ▁ whether ▁ GDAL ▁ is ▁ available . ENDCOM if gdal . HAS_GDAL : NEW_LINE INDENT g = GEOSGeometry ( ' POINT ▁ ( -104.609 ▁ 38.255 ) ' , 4326 ) NEW_LINE gt = g . tuple NEW_LINE g . transform ( 4326 ) NEW_LINE self . assertEqual ( g . tuple , gt ) NEW_LINE self . assertEqual ( g . srid , 4326 ) NEW_LINE g = GEOSGeometry ( ' POINT ▁ ( -104.609 ▁ 38.255 ) ' , 4326 ) NEW_LINE g1 = g . transform ( 4326 , clone = True ) NEW_LINE self . assertEqual ( g1 . tuple , g . tuple ) NEW_LINE self . assertEqual ( g1 . srid , 4326 ) NEW_LINE self . assert_ ( g1 is not g , " Clone ▁ didn ' t ▁ happen " ) NEW_LINE DEDENT old_has_gdal = gdal . HAS_GDAL NEW_LINE try : NEW_LINE INDENT gdal . HAS_GDAL = False NEW_LINE g = GEOSGeometry ( ' POINT ▁ ( -104.609 ▁ 38.255 ) ' , 4326 ) NEW_LINE gt = g . tuple NEW_LINE g . transform ( 4326 ) NEW_LINE self . assertEqual ( g . tuple , gt ) NEW_LINE self . assertEqual ( g . srid , 4326 ) NEW_LINE g = GEOSGeometry ( ' POINT ▁ ( -104.609 ▁ 38.255 ) ' , 4326 ) NEW_LINE g1 = g . transform ( 4326 , clone = True ) NEW_LINE self . assertEqual ( g1 . tuple , g . tuple ) NEW_LINE self . assertEqual ( g1 . srid , 4326 ) NEW_LINE self . assert_ ( g1 is not g , " Clone ▁ didn ' t ▁ happen " ) NEW_LINE DEDENT finally : NEW_LINE INDENT gdal . HAS_GDAL = old_has_gdal NEW_LINE DEDENT DEDENT
def test23_transform_nogdal ( self ) : NEW_LINE INDENT """ ▁ Testing ▁ ` transform ` ▁ method ▁ ( GDAL ▁ not ▁ available ) ▁ """ NEW_LINE old_has_gdal = gdal . HAS_GDAL NEW_LINE try : NEW_LINE INDENT gdal . HAS_GDAL = False NEW_LINE g = GEOSGeometry ( ' POINT ▁ ( -104.609 ▁ 38.255 ) ' , 4326 ) NEW_LINE self . assertRaises ( GEOSException , g . transform , 2774 ) NEW_LINE g = GEOSGeometry ( ' POINT ▁ ( -104.609 ▁ 38.255 ) ' , 4326 ) NEW_LINE self . assertRaises ( GEOSException , g . transform , 2774 , clone = True ) NEW_LINE DEDENT finally : NEW_LINE INDENT gdal . HAS_GDAL = old_has_gdal NEW_LINE DEDENT DEDENT
def test24_extent ( self ) : NEW_LINE INDENT " Testing ▁ ` extent ` ▁ method . " NEW_LINE # ▁ The ▁ xmin , ▁ ymin , ▁ xmax , ▁ ymax ▁ of ▁ the ▁ MultiPoint ▁ should ▁ be ▁ returned . ENDCOM mp = MultiPoint ( Point ( 5 , 23 ) , Point ( 0 , 0 ) , Point ( 10 , 50 ) ) NEW_LINE self . assertEqual ( ( 0.0 , 0.0 , 10.0 , 50.0 ) , mp . extent ) NEW_LINE pnt = Point ( 5.23 , 17.8 ) NEW_LINE # ▁ Extent ▁ of ▁ points ▁ is ▁ just ▁ the ▁ point ▁ itself ▁ repeated . ENDCOM self . assertEqual ( ( 5.23 , 17.8 , 5.23 , 17.8 ) , pnt . extent ) NEW_LINE # ▁ Testing ▁ on ▁ the ▁ ' real ▁ world ' ▁ Polygon . ENDCOM poly = fromstr ( self . geometries . polygons [ 3 ] . wkt ) NEW_LINE ring = poly . shell NEW_LINE x , y = ring . x , ring . y NEW_LINE xmin , ymin = min ( x ) , min ( y ) NEW_LINE xmax , ymax = max ( x ) , max ( y ) NEW_LINE self . assertEqual ( ( xmin , ymin , xmax , ymax ) , poly . extent ) NEW_LINE DEDENT
def test25_pickle ( self ) : NEW_LINE INDENT " Testing ▁ pickling ▁ and ▁ unpickling ▁ support . " NEW_LINE # ▁ Using ▁ both ▁ pickle ▁ and ▁ cPickle ▁ - - ▁ just ▁ ' cause . ENDCOM import pickle , cPickle NEW_LINE # ▁ Creating ▁ a ▁ list ▁ of ▁ test ▁ geometries ▁ for ▁ pickling , ENDCOM # ▁ and ▁ setting ▁ the ▁ SRID ▁ on ▁ some ▁ of ▁ them . ENDCOM def get_geoms ( lst , srid = None ) : NEW_LINE INDENT return [ GEOSGeometry ( tg . wkt , srid ) for tg in lst ] NEW_LINE DEDENT tgeoms = get_geoms ( self . geometries . points ) NEW_LINE tgeoms . extend ( get_geoms ( self . geometries . multilinestrings , 4326 ) ) NEW_LINE tgeoms . extend ( get_geoms ( self . geometries . polygons , 3084 ) ) NEW_LINE tgeoms . extend ( get_geoms ( self . geometries . multipolygons , 900913 ) ) NEW_LINE # ▁ The ▁ SRID ▁ won ' t ▁ be ▁ exported ▁ in ▁ GEOS ▁ 3.0 ▁ release ▁ candidates . ENDCOM no_srid = self . null_srid == - 1 NEW_LINE for geom in tgeoms : NEW_LINE INDENT s1 , s2 = cPickle . dumps ( geom ) , pickle . dumps ( geom ) NEW_LINE g1 , g2 = cPickle . loads ( s1 ) , pickle . loads ( s2 ) NEW_LINE for tmpg in ( g1 , g2 ) : NEW_LINE INDENT self . assertEqual ( geom , tmpg ) NEW_LINE if not no_srid : self . assertEqual ( geom . srid , tmpg . srid ) NEW_LINE DEDENT DEDENT DEDENT
def test26_prepared ( self ) : NEW_LINE INDENT " Testing ▁ PreparedGeometry ▁ support . " NEW_LINE if not GEOS_PREPARE : return NEW_LINE # ▁ Creating ▁ a ▁ simple ▁ multipolygon ▁ and ▁ getting ▁ a ▁ prepared ▁ version . ENDCOM mpoly = GEOSGeometry ( ' MULTIPOLYGON ( ( ( 0 ▁ 0,0 ▁ 5,5 ▁ 5,5 ▁ 0,0 ▁ 0 ) ) , ( (5 ▁ 5,5 ▁ 10,10 ▁ 10,10 ▁ 5,5 ▁ 5 ) ) ) ' ) NEW_LINE prep = mpoly . prepared NEW_LINE # ▁ A ▁ set ▁ of ▁ test ▁ points . ENDCOM pnts = [ Point ( 5 , 5 ) , Point ( 7.5 , 7.5 ) , Point ( 2.5 , 7.5 ) ] NEW_LINE covers = [ True , True , False ] # ▁ No ▁ ` covers ` ▁ op ▁ for ▁ regular ▁ GEOS ▁ geoms . ENDCOM NEW_LINE for pnt , c in zip ( pnts , covers ) : NEW_LINE # ▁ Results ▁ should ▁ be ▁ the ▁ same ▁ ( but ▁ faster ) ENDCOM INDENT self . assertEqual ( mpoly . contains ( pnt ) , prep . contains ( pnt ) ) NEW_LINE self . assertEqual ( mpoly . intersects ( pnt ) , prep . intersects ( pnt ) ) NEW_LINE self . assertEqual ( c , prep . covers ( pnt ) ) NEW_LINE DEDENT DEDENT
def test26_line_merge ( self ) : NEW_LINE INDENT " Testing ▁ line ▁ merge ▁ support " NEW_LINE ref_geoms = ( fromstr ( ' LINESTRING ( 1 ▁ 1 , ▁ 1 ▁ 1 , ▁ 3 ▁ 3 ) ' ) , fromstr ( ' MULTILINESTRING ( (1 ▁ 1 , ▁ 3 ▁ 3 ) , ▁ ( 3 ▁ 3 , ▁ 4 ▁ 2 ) ) ' ) , ) NEW_LINE ref_merged = ( fromstr ( ' LINESTRING ( 1 ▁ 1 , ▁ 3 ▁ 3 ) ' ) , fromstr ( ' LINESTRING ▁ ( 1 ▁ 1 , ▁ 3 ▁ 3 , ▁ 4 ▁ 2 ) ' ) , ) NEW_LINE for geom , merged in zip ( ref_geoms , ref_merged ) : NEW_LINE INDENT self . assertEqual ( merged , geom . merged ) NEW_LINE DEDENT DEDENT
def __init__ ( self , dm ) : NEW_LINE INDENT self . dm = dm NEW_LINE DEDENT
def determine_upsample ( self , interpolation = None , use_cols = None ) : NEW_LINE INDENT " Resolve ▁ ( and ▁ if ▁ necessary ▁ validate ) ▁ upsampling ▁ rules . " NEW_LINE if interpolation is None : NEW_LINE INDENT interpolation = dict ( ) NEW_LINE DEDENT if use_cols is None : NEW_LINE INDENT use_cols = self . dm . columns NEW_LINE DEDENT rules = dict ( ) NEW_LINE for name in use_cols : NEW_LINE INDENT col_info = self . dm . col_info [ name ] NEW_LINE rule = _validate_upsample ( interpolation . get ( name , col_info . upsample ) ) NEW_LINE rule = _normalize_string_none ( rule ) NEW_LINE if ( rule is not None ) and ( col_info . ndim > 0 ) : NEW_LINE INDENT raise NotImplementedError ( " Only ▁ scalar ▁ data ▁ can ▁ be ▁ upsampled . ▁ " " The ▁ { 0 } - dimensional ▁ source ▁ { 1 } ▁ was ▁ given ▁ the ▁ " " upsampling ▁ rule ▁ { 2 } . " . format ( col_info . ndim , name , rule ) ) NEW_LINE DEDENT rules [ name ] = rule NEW_LINE DEDENT return rules NEW_LINE DEDENT
def determine_downsample ( self , agg = None , use_cols = None ) : NEW_LINE INDENT " Resolve ▁ ( and ▁ if ▁ necessary ▁ validate ) ▁ sampling ▁ rules . " NEW_LINE if agg is None : NEW_LINE INDENT agg = dict ( ) NEW_LINE DEDENT if use_cols is None : NEW_LINE INDENT use_cols = self . dm . columns NEW_LINE DEDENT rules = dict ( ) NEW_LINE for name in use_cols : NEW_LINE INDENT col_info = self . dm . col_info [ name ] NEW_LINE rule = _validate_downsample ( agg . get ( name , col_info . downsample ) ) NEW_LINE rule = _normalize_string_none ( rule ) NEW_LINE rules [ name ] = rule NEW_LINE DEDENT return rules NEW_LINE DEDENT
def bin_by_edges ( self , bin_edges , bin_anchors , interpolation = None , agg = None , use_cols = None ) : NEW_LINE INDENT """ Explain ▁ operation ▁ of ▁ DataMuxer . bin _ by _ edges STRNEWLINE STRNEWLINE ▁ Parameters STRNEWLINE ▁ - - - - - STRNEWLINE ▁ bin _ edges ▁ : ▁ list STRNEWLINE ▁ list ▁ of ▁ two - element ▁ items ▁ like ▁ [ ( t1 , ▁ t2 ) , ▁ ( t3 , ▁ t4 ) , ▁ . . . ] STRNEWLINE ▁ bin _ anchors ▁ : ▁ list STRNEWLINE ▁ These ▁ are ▁ time ▁ points ▁ where ▁ interpolated ▁ values ▁ will ▁ be STRNEWLINE ▁ evaluated . ▁ Bin ▁ centers ▁ are ▁ usually ▁ a ▁ good ▁ choice . STRNEWLINE ▁ interpolation ▁ : ▁ dict , ▁ optional STRNEWLINE ▁ Override ▁ the ▁ default ▁ interpolation ▁ ( upsampling ) ▁ behavior ▁ of ▁ any STRNEWLINE ▁ data ▁ source ▁ by ▁ passing ▁ a ▁ dictionary ▁ of ▁ source ▁ names ▁ mapped ▁ onto STRNEWLINE ▁ one ▁ of ▁ the ▁ following ▁ interpolation ▁ methods . STRNEWLINE STRNEWLINE ▁ { None , ▁ ' linear ' , ▁ ' nearest ' , ▁ ' zero ' , ▁ ' slinear ' , ▁ ' quadratic ' , STRNEWLINE ▁ ' cubic ' , ▁ ' ffill ' , ▁ ' bfill ' } STRNEWLINE STRNEWLINE ▁ None ▁ means ▁ that ▁ each ▁ time ▁ bin ▁ must ▁ have ▁ at ▁ least ▁ one ▁ value . STRNEWLINE ▁ See ▁ scipy . interpolator ▁ for ▁ more ▁ on ▁ the ▁ other ▁ methods . STRNEWLINE ▁ agg ▁ : ▁ dict , ▁ optional STRNEWLINE ▁ Override ▁ the ▁ default ▁ reduction ▁ ( downsampling ) ▁ behavior ▁ of ▁ any STRNEWLINE ▁ data ▁ source ▁ by ▁ passing ▁ a ▁ dictionary ▁ of ▁ source ▁ names ▁ mapped ▁ onto STRNEWLINE ▁ any ▁ callable ▁ that ▁ reduces ▁ multiple ▁ data ▁ points ▁ ( of ▁ whatever STRNEWLINE ▁ dimension ) ▁ to ▁ a ▁ single ▁ data ▁ point . STRNEWLINE ▁ use _ cols ▁ : ▁ list , ▁ optional STRNEWLINE ▁ List ▁ of ▁ columns ▁ to ▁ include ▁ in ▁ binning ; ▁ use ▁ all ▁ columns ▁ by STRNEWLINE ▁ default . STRNEWLINE STRNEWLINE ▁ Returns STRNEWLINE ▁ - - - - - STRNEWLINE ▁ df ▁ : ▁ pandas . DataFrame STRNEWLINE ▁ table ▁ giving ▁ upsample ▁ and ▁ downsample ▁ rules ▁ for ▁ each ▁ data ▁ column STRNEWLINE ▁ and ▁ indicating ▁ whether ▁ those ▁ rules ▁ are ▁ applicable STRNEWLINE STRNEWLINE ▁ References STRNEWLINE ▁ - - - - - STRNEWLINE ▁ http : / / docs . scipy . org / doc / scipy / reference / generated / scipy . interpolate . interp1d . html STRNEWLINE ▁ """ NEW_LINE bin_anchors , binning = self . dm . _bin_by_edges ( bin_anchors , bin_edges ) NEW_LINE # ▁ TODO ▁ Cache ▁ the ▁ grouping ▁ for ▁ reuse ▁ by ▁ resample . ENDCOM grouped = self . dm . _dataframe . groupby ( binning ) NEW_LINE counts = grouped . count ( ) NEW_LINE df = pd . DataFrame . from_dict ( _is_resampling_applicable ( counts ) ) NEW_LINE df [ ' upsample ' ] = self . determine_upsample ( interpolation , use_cols ) NEW_LINE df [ ' downsample ' ] = self . determine_downsample ( agg , use_cols ) NEW_LINE return df NEW_LINE DEDENT
def bin_on ( self , source_name , interpolation = None , agg = None , use_cols = None ) : NEW_LINE INDENT """ Explain ▁ operation ▁ of ▁ DataMuxer . bin _ on . STRNEWLINE STRNEWLINE ▁ Parameters STRNEWLINE ▁ - - - - - STRNEWLINE ▁ source _ name ▁ : ▁ string STRNEWLINE ▁ interpolation ▁ : ▁ dict , ▁ optional STRNEWLINE ▁ Override ▁ the ▁ default ▁ interpolation ▁ ( upsampling ) ▁ behavior ▁ of ▁ any STRNEWLINE ▁ data ▁ source ▁ by ▁ passing ▁ a ▁ dictionary ▁ of ▁ source ▁ names ▁ mapped ▁ onto STRNEWLINE ▁ one ▁ of ▁ the ▁ following ▁ interpolation ▁ methods . STRNEWLINE STRNEWLINE ▁ { None , ▁ ' linear ' , ▁ ' nearest ' , ▁ ' zero ' , ▁ ' slinear ' , ▁ ' quadratic ' , STRNEWLINE ▁ ' cubic ' } STRNEWLINE STRNEWLINE ▁ None ▁ means ▁ that ▁ each ▁ time ▁ bin ▁ must ▁ have ▁ at ▁ least ▁ one ▁ value . STRNEWLINE ▁ See ▁ scipy . interpolator ▁ for ▁ more ▁ on ▁ the ▁ other ▁ methods . STRNEWLINE ▁ agg ▁ : ▁ dict , ▁ optional STRNEWLINE ▁ Override ▁ the ▁ default ▁ reduction ▁ ( downsampling ) ▁ behavior ▁ of ▁ any STRNEWLINE ▁ data ▁ source ▁ by ▁ passing ▁ a ▁ dictionary ▁ of ▁ source ▁ names ▁ mapped ▁ onto STRNEWLINE ▁ any ▁ callable ▁ that ▁ reduces ▁ multiple ▁ data ▁ points ▁ ( of ▁ whatever STRNEWLINE ▁ dimension ) ▁ to ▁ a ▁ single ▁ data ▁ point . STRNEWLINE ▁ use _ cols ▁ : ▁ list , ▁ optional STRNEWLINE ▁ List ▁ of ▁ columns ▁ to ▁ include ▁ in ▁ binning ; ▁ use ▁ all ▁ columns ▁ by STRNEWLINE ▁ default . STRNEWLINE STRNEWLINE ▁ Returns STRNEWLINE ▁ - - - - - STRNEWLINE ▁ df ▁ : ▁ pandas . DataFrame STRNEWLINE ▁ table ▁ giving ▁ upsample ▁ and ▁ downsample ▁ rules ▁ for ▁ each ▁ data ▁ column STRNEWLINE ▁ and ▁ indicating ▁ whether ▁ those ▁ rules ▁ are ▁ applicable STRNEWLINE STRNEWLINE ▁ References STRNEWLINE ▁ - - - - - STRNEWLINE ▁ http : / / docs . scipy . org / doc / scipy / reference / generated / scipy . interpolate . interp1d . html STRNEWLINE ▁ """ NEW_LINE centers , bin_edges = self . dm . _bin_on ( source_name ) NEW_LINE bin_anchors , binning = self . dm . _bin_by_edges ( centers , bin_edges ) NEW_LINE # ▁ TODO ▁ Cache ▁ the ▁ grouping ▁ for ▁ reuse ▁ by ▁ resample . ENDCOM grouped = self . dm . _dataframe . groupby ( binning ) NEW_LINE counts = grouped . count ( ) NEW_LINE df = pd . DataFrame . from_dict ( _is_resampling_applicable ( counts ) ) NEW_LINE df [ ' upsample ' ] = self . determine_upsample ( interpolation , use_cols ) NEW_LINE df [ ' downsample ' ] = self . determine_downsample ( agg , use_cols ) NEW_LINE return df NEW_LINE DEDENT
def __init__ ( self ) : NEW_LINE INDENT self . sources = { } NEW_LINE self . col_info = { } NEW_LINE self . col_info [ ' time ' ] = ColSpec ( ' time ' , 0 , [ ] , ' linear ' , ' mean ' ) NEW_LINE self . _data = deque ( ) NEW_LINE self . _time = deque ( ) NEW_LINE self . _timestamps = deque ( ) NEW_LINE self . _timestamps_as_data = set ( ) NEW_LINE self . _known_events = set ( ) NEW_LINE self . _known_descriptors = set ( ) NEW_LINE self . _stale = True NEW_LINE self . plan = self . Planner ( self ) NEW_LINE self . convert_times = True NEW_LINE self . _reference_time = None NEW_LINE DEDENT
def reference_time ( self ) : NEW_LINE INDENT return self . _reference_time NEW_LINE DEDENT
def reference_time ( self , val ) : NEW_LINE INDENT self . _reference_time = pd . Timestamp ( val , unit = ' s ' ) NEW_LINE DEDENT
def columns ( self ) : NEW_LINE INDENT " The ▁ columns ▁ of ▁ DataFrames ▁ returned ▁ by ▁ methods ▁ that ▁ return ▁ DataFrames . " NEW_LINE return set ( self . sources ) | self . _time_columns NEW_LINE DEDENT
def _time_columns ( self ) : NEW_LINE INDENT ts_names = [ name + ' _ timestamp ' for name in self . _timestamps_as_data ] NEW_LINE return { ' time ' } | set ( ts_names ) NEW_LINE DEDENT
def append_events ( self , events , verbose = False ) : NEW_LINE INDENT """ Add ▁ a ▁ list ▁ of ▁ events ▁ to ▁ the ▁ DataMuxer . STRNEWLINE STRNEWLINE ▁ Parameters STRNEWLINE ▁ - - - - - STRNEWLINE ▁ events ▁ : ▁ list STRNEWLINE ▁ list ▁ of ▁ Events ▁ ( any ▁ objects ▁ with ▁ the ▁ expected ▁ attributes ▁ will ▁ do ) STRNEWLINE ▁ """ NEW_LINE for idx , event in enumerate ( events ) : NEW_LINE INDENT if verbose and idx % 25 == 0 : NEW_LINE INDENT print ( ' loading ▁ event ▁ % s ' % idx ) , NEW_LINE DEDENT self . append_event ( event ) NEW_LINE DEDENT DEDENT
def append_event ( self , event ) : NEW_LINE INDENT """ Add ▁ an ▁ event ▁ to ▁ the ▁ DataMuxer . STRNEWLINE STRNEWLINE ▁ Parameters STRNEWLINE ▁ - - - - - STRNEWLINE ▁ event ▁ : ▁ Event STRNEWLINE ▁ Event ▁ Document ▁ or ▁ any ▁ object ▁ with ▁ the ▁ expected ▁ attributes STRNEWLINE STRNEWLINE ▁ Returns STRNEWLINE ▁ - - - - - STRNEWLINE ▁ is _ new ▁ : ▁ bool STRNEWLINE ▁ True ▁ if ▁ event ▁ was ▁ added , ▁ False ▁ is ▁ it ▁ has ▁ already ▁ been ▁ added STRNEWLINE ▁ """ NEW_LINE if event . uid in self . _known_events : NEW_LINE INDENT return False NEW_LINE DEDENT self . _known_events . add ( event . uid ) NEW_LINE self . _stale = True NEW_LINE if event . descriptor . uid not in self . _known_descriptors : NEW_LINE INDENT self . _process_new_descriptor ( event . descriptor ) NEW_LINE # ▁ Both ▁ scalar ▁ and ▁ nonscalar ▁ data ▁ will ▁ get ▁ stored ▁ in ▁ the ▁ DataFrame . ENDCOM # ▁ This ▁ may ▁ be ▁ optimized ▁ later , ▁ but ▁ it ▁ might ▁ not ▁ actually ▁ help ▁ much . ENDCOM DEDENT self . _data . append ( { name : data for name , data in six . iteritems ( event . data ) } ) NEW_LINE self . _timestamps . append ( { name : ts for name , ts in six . iteritems ( event . timestamps ) } ) NEW_LINE self . _time . append ( event . time ) NEW_LINE return True NEW_LINE DEDENT
def _process_new_descriptor ( self , descriptor ) : NEW_LINE INDENT " Build ▁ a ▁ ColSpec ▁ and ▁ update ▁ state . " NEW_LINE for name , description in six . iteritems ( descriptor . data_keys ) : NEW_LINE # ▁ If ▁ we ▁ already ▁ have ▁ this ▁ source ▁ name , ▁ the ▁ unique ▁ source ENDCOM # ▁ identifiers ▁ must ▁ match . ▁ Ambiguous ▁ names ▁ are ▁ not ▁ allowed . ENDCOM INDENT if name in self . sources : NEW_LINE INDENT if self . sources [ name ] != description [ ' source ' ] : NEW_LINE INDENT raise ValueError ( " In ▁ a ▁ previously ▁ loaded ▁ descriptor , ▁ " " ' {0 } ' ▁ refers ▁ to ▁ { 1 } ▁ but ▁ in ▁ Event ▁ " " Descriptor ▁ { 2 } ▁ it ▁ refers ▁ to ▁ { 3 } . " . format ( name , self . sources [ name ] , descriptor . uid , description [ ' source ' ] ) ) NEW_LINE DEDENT if name == ' time ' : NEW_LINE # ▁ We ▁ can ▁ argue ▁ later ▁ about ▁ how ▁ best ▁ to ▁ handle ▁ this ▁ corner ENDCOM # ▁ case , ▁ but ▁ anything ▁ is ▁ better ▁ than ▁ silently ▁ mislabeling ENDCOM # ▁ data . ENDCOM INDENT raise ValueError ( " The ▁ name ▁ ' time ' ▁ is ▁ reserved ▁ and ▁ cannot ▁ " " be ▁ used ▁ as ▁ an ▁ alias . " ) NEW_LINE # ▁ If ▁ it ▁ is ▁ a ▁ new ▁ name , ▁ determine ▁ a ▁ ColSpec . ENDCOM DEDENT DEDENT else : NEW_LINE INDENT self . sources [ name ] = description [ ' source ' ] NEW_LINE if ' external ' in description and ' shape ' in description : NEW_LINE INDENT shape = description [ ' shape ' ] NEW_LINE ndim = len ( shape ) NEW_LINE DEDENT else : NEW_LINE # ▁ External ▁ data ▁ can ▁ be ▁ scalar . ▁ Nonscalar ▁ data ▁ must ENDCOM # ▁ have ▁ a ▁ specified ▁ shape . ▁ Thus , ▁ if ▁ no ▁ shape ▁ is ▁ given , ENDCOM # ▁ assume ▁ scalar . ENDCOM INDENT shape = None NEW_LINE ndim = 0 NEW_LINE DEDENT upsample = self . default_upsample NEW_LINE if ndim > 0 : NEW_LINE INDENT upsample = None NEW_LINE DEDENT col_info = ColSpec ( name , ndim , shape , upsample , self . default_downsample ) # ▁ defaults ENDCOM NEW_LINE # ▁ TODO ▁ Look ▁ up ▁ source - specific ▁ default ▁ in ▁ a ▁ config ▁ file ENDCOM # ▁ or ▁ some ▁ other ▁ source ▁ of ▁ reference ▁ data . ENDCOM self . col_info [ name ] = col_info NEW_LINE DEDENT DEDENT self . _known_descriptors . add ( descriptor . uid ) NEW_LINE DEDENT
def _dataframe ( self ) : NEW_LINE INDENT " See ▁ also ▁ to _ sparse _ dataframe , ▁ the ▁ public ▁ version ▁ of ▁ this . " NEW_LINE # ▁ Rebuild ▁ the ▁ DataFrame ▁ if ▁ more ▁ data ▁ has ▁ been ▁ added . ENDCOM if self . _stale : NEW_LINE INDENT df = pd . DataFrame ( list ( self . _data ) ) NEW_LINE df [ ' time ' ] = list ( self . _time ) NEW_LINE if self . _timestamps_as_data : NEW_LINE # ▁ Only ▁ build ▁ this ▁ if ▁ we ▁ need ▁ it . ENDCOM # ▁ TODO : ▁ We ▁ shouldn ' t ▁ have ▁ to ▁ build ENDCOM # ▁ the ▁ whole ▁ thing , ▁ but ▁ there ▁ is ▁ already ▁ a ▁ lot ▁ of ▁ trickiness ENDCOM # ▁ here ▁ so ▁ we ' ll ▁ worry ▁ about ▁ optimization ▁ later . ENDCOM INDENT timestamps = pd . DataFrame ( list ( self . _timestamps ) ) NEW_LINE DEDENT for source_name in self . _timestamps_as_data : NEW_LINE INDENT col_name = _timestamp_col_name ( source_name ) NEW_LINE df [ col_name ] = timestamps [ source_name ] NEW_LINE logger . debug ( " Including ▁ % s ▁ timestamps ▁ as ▁ data " , source_name ) NEW_LINE DEDENT self . _df = df . sort ( ' time ' ) . reset_index ( drop = True ) NEW_LINE self . _stale = False NEW_LINE DEDENT return self . _df NEW_LINE DEDENT
def to_sparse_dataframe ( self , include_all_timestamps = False ) : NEW_LINE INDENT """ Obtain ▁ all ▁ measurements ▁ in ▁ a ▁ DataFrame , ▁ one ▁ row ▁ per ▁ Event ▁ time . STRNEWLINE STRNEWLINE ▁ Parameters STRNEWLINE ▁ - - - - - STRNEWLINE ▁ include _ all _ timestamps ▁ : ▁ bool STRNEWLINE ▁ The ▁ result ▁ will ▁ always ▁ contain ▁ a ▁ ' time ' ▁ column ▁ but , ▁ by ▁ default , STRNEWLINE ▁ not ▁ timestamps ▁ for ▁ individual ▁ data ▁ sources ▁ like ▁ ' motor _ timestamp ' . STRNEWLINE ▁ Set ▁ this ▁ to ▁ True ▁ to ▁ export ▁ timestamp ▁ columns ▁ for ▁ each ▁ data ▁ column STRNEWLINE STRNEWLINE ▁ Returns STRNEWLINE ▁ - - - - - STRNEWLINE ▁ df ▁ : ▁ pandas . DataFrame STRNEWLINE ▁ """ NEW_LINE if include_all_timestamps : NEW_LINE INDENT raise NotImplementedError ( " TODO " ) NEW_LINE DEDENT result = self . _dataframe . copy ( ) NEW_LINE for col_name in self . _time_columns : NEW_LINE INDENT result [ col_name ] = self . _maybe_convert_times ( result [ col_name ] ) NEW_LINE DEDENT return result NEW_LINE DEDENT
def _maybe_convert_times ( self , data ) : NEW_LINE INDENT if self . convert_times : NEW_LINE INDENT t = pd . to_datetime ( data , unit = ' s ' , utc = True ) . dt . tz_localize ( TZ ) NEW_LINE if self . reference_time is None : NEW_LINE INDENT return t NEW_LINE DEDENT else : NEW_LINE INDENT return t - self . reference_time NEW_LINE DEDENT DEDENT return data # ▁ no - op ENDCOM NEW_LINE DEDENT
def include_timestamp_data ( self , source_name ) : NEW_LINE INDENT """ Add ▁ the ▁ exact ▁ timing ▁ of ▁ a ▁ data ▁ source ▁ as ▁ a ▁ data ▁ column . STRNEWLINE STRNEWLINE ▁ Parameters STRNEWLINE ▁ - - - - - STRNEWLINE ▁ source _ name ▁ : ▁ string STRNEWLINE ▁ one ▁ of ▁ the ▁ source ▁ names ▁ in ▁ DataMuxer . sources STRNEWLINE ▁ """ NEW_LINE # ▁ self . _ timestamps _ as _ data ▁ is ▁ a ▁ set ▁ of ▁ sources ▁ who ▁ timestamps ENDCOM # ▁ should ▁ be ▁ treated ▁ as ▁ data ▁ in ▁ the ▁ _ dataframe ▁ method ▁ above . ENDCOM self . _timestamps_as_data . add ( source_name ) NEW_LINE name = _timestamp_col_name ( source_name ) NEW_LINE self . col_info [ name ] = ColSpec ( name , 0 , None , None , np . mean ) NEW_LINE self . _stale = True NEW_LINE DEDENT
def remove_timestamp_data ( self , source_name ) : NEW_LINE INDENT """ Remove ▁ the ▁ exact ▁ timing ▁ of ▁ a ▁ data ▁ source ▁ from ▁ the ▁ data ▁ columns . STRNEWLINE STRNEWLINE ▁ Parameters STRNEWLINE ▁ - - - - - STRNEWLINE ▁ source _ name ▁ : ▁ string STRNEWLINE ▁ one ▁ of ▁ the ▁ source ▁ names ▁ in ▁ DataMuxer . sources STRNEWLINE ▁ """ NEW_LINE self . _timestamps_as_data . remove ( source_name ) NEW_LINE # ▁ Do ▁ not ▁ force ▁ a ▁ rebuilt ▁ ( i . e . , ▁ self . _ stale ) . ▁ Just ▁ remove ▁ it ▁ here . ENDCOM del self . _df [ _timestamp_col_name ( source_name ) ] NEW_LINE DEDENT
def bin_on ( self , source_name , interpolation = None , agg = None , use_cols = None ) : NEW_LINE INDENT """ STRNEWLINE ▁ Return ▁ data ▁ resampled ▁ to ▁ align ▁ with ▁ the ▁ data ▁ from ▁ a ▁ particular ▁ source . STRNEWLINE STRNEWLINE ▁ Parameters STRNEWLINE ▁ - - - - - STRNEWLINE ▁ source _ name ▁ : ▁ string STRNEWLINE ▁ interpolation ▁ : ▁ dict , ▁ optional STRNEWLINE ▁ Override ▁ the ▁ default ▁ interpolation ▁ ( upsampling ) ▁ behavior ▁ of ▁ any STRNEWLINE ▁ data ▁ source ▁ by ▁ passing ▁ a ▁ dictionary ▁ of ▁ source ▁ names ▁ mapped ▁ onto STRNEWLINE ▁ one ▁ of ▁ the ▁ following ▁ interpolation ▁ methods . STRNEWLINE STRNEWLINE ▁ { None , ▁ ' linear ' , ▁ ' nearest ' , ▁ ' zero ' , ▁ ' slinear ' , ▁ ' quadratic ' , STRNEWLINE ▁ ' cubic ' } STRNEWLINE STRNEWLINE ▁ None ▁ means ▁ that ▁ each ▁ time ▁ bin ▁ must ▁ have ▁ at ▁ least ▁ one ▁ value . STRNEWLINE ▁ See ▁ scipy . interpolator ▁ for ▁ more ▁ on ▁ the ▁ other ▁ methods . STRNEWLINE ▁ agg ▁ : ▁ dict , ▁ optional STRNEWLINE ▁ Override ▁ the ▁ default ▁ reduction ▁ ( downsampling ) ▁ behavior ▁ of ▁ any ▁ data STRNEWLINE ▁ source ▁ by ▁ passing ▁ a ▁ dictionary ▁ of ▁ source ▁ names ▁ mapped ▁ onto ▁ any STRNEWLINE ▁ callable ▁ that ▁ reduces ▁ multiple ▁ data ▁ points ▁ ( of ▁ whatever ▁ dimension ) STRNEWLINE ▁ to ▁ a ▁ single ▁ data ▁ point . STRNEWLINE ▁ use _ cols ▁ : ▁ list , ▁ optional STRNEWLINE ▁ List ▁ of ▁ columns ▁ to ▁ include ▁ in ▁ binning ; ▁ use ▁ all ▁ columns ▁ by ▁ default . STRNEWLINE STRNEWLINE ▁ Returns STRNEWLINE ▁ - - - - - STRNEWLINE ▁ resampled _ df ▁ : ▁ pandas . DataFrame STRNEWLINE STRNEWLINE ▁ References STRNEWLINE ▁ - - - - - STRNEWLINE ▁ http : / / docs . scipy . org / doc / scipy / reference / generated / scipy . interpolate . interp1d . html STRNEWLINE ▁ """ NEW_LINE centers , bin_edges = self . _bin_on ( source_name ) NEW_LINE return self . bin_by_edges ( bin_edges , bin_anchors = centers , interpolation = interpolation , agg = agg , use_cols = use_cols ) NEW_LINE DEDENT
def _bin_on ( self , source_name ) : NEW_LINE INDENT " Compute ▁ bin ▁ edges ▁ spaced ▁ around ▁ centers ▁ defined ▁ by ▁ source _ name ▁ points . " NEW_LINE col = self . _dataframe [ source_name ] NEW_LINE centers = self . _dataframe [ ' time ' ] . reindex_like ( col . dropna ( ) ) . values NEW_LINE # ▁ [ 2 , ▁ 4 , ▁ 6 ] ▁ - > ▁ [ - inf , ▁ 3 , ▁ 5 , ▁ inf ] ENDCOM bin_edges = np . mean ( [ centers [ 1 : ] , centers [ : - 1 ] ] , 0 ) NEW_LINE # ▁ [ - inf , ▁ 3 , ▁ 5 , ▁ inf ] ▁ - > ▁ [ ( - inf , ▁ 3 ) , ▁ ( 3 , ▁ 5 ) , ▁ ( 5 , ▁ inf ) ] ENDCOM bin_edges = [ - np . inf ] + list ( np . repeat ( bin_edges , 2 ) ) + [ np . inf ] NEW_LINE bin_edges = np . reshape ( bin_edges , ( - 1 , 2 ) ) NEW_LINE return centers , bin_edges NEW_LINE DEDENT
def bin_by_edges ( self , bin_edges , bin_anchors , interpolation = None , agg = None , use_cols = None ) : NEW_LINE INDENT """ STRNEWLINE ▁ Return ▁ data ▁ resampled ▁ into ▁ bins ▁ with ▁ the ▁ specified ▁ edges . STRNEWLINE STRNEWLINE ▁ Parameters STRNEWLINE ▁ - - - - - STRNEWLINE ▁ bin _ edges ▁ : ▁ list STRNEWLINE ▁ list ▁ of ▁ two - element ▁ items ▁ like ▁ [ ( t1 , ▁ t2 ) , ▁ ( t3 , ▁ t4 ) , ▁ . . . ] STRNEWLINE ▁ bin _ anchors ▁ : ▁ list STRNEWLINE ▁ These ▁ are ▁ time ▁ points ▁ where ▁ interpolated ▁ values ▁ will ▁ be ▁ evaluated . STRNEWLINE ▁ Bin ▁ centers ▁ are ▁ usually ▁ a ▁ good ▁ choice . STRNEWLINE ▁ interpolation ▁ : ▁ dict , ▁ optional STRNEWLINE ▁ Override ▁ the ▁ default ▁ interpolation ▁ ( upsampling ) ▁ behavior ▁ of ▁ any STRNEWLINE ▁ data ▁ source ▁ by ▁ passing ▁ a ▁ dictionary ▁ of ▁ source ▁ names ▁ mapped ▁ onto STRNEWLINE ▁ one ▁ of ▁ the ▁ following ▁ interpolation ▁ methods . STRNEWLINE STRNEWLINE ▁ { None , ▁ ' linear ' , ▁ ' nearest ' , ▁ ' zero ' , ▁ ' slinear ' , ▁ ' quadratic ' , STRNEWLINE ▁ ' cubic ' } STRNEWLINE STRNEWLINE ▁ None ▁ means ▁ that ▁ each ▁ time ▁ bin ▁ must ▁ have ▁ at ▁ least ▁ one ▁ value . STRNEWLINE ▁ See ▁ scipy . interpolator ▁ for ▁ more ▁ on ▁ the ▁ other ▁ methods . STRNEWLINE ▁ agg ▁ : ▁ dict , ▁ optional STRNEWLINE ▁ Override ▁ the ▁ default ▁ reduction ▁ ( downsampling ) ▁ behavior ▁ of ▁ any ▁ data STRNEWLINE ▁ source ▁ by ▁ passing ▁ a ▁ dictionary ▁ of ▁ source ▁ names ▁ mapped ▁ onto ▁ any STRNEWLINE ▁ callable ▁ that ▁ reduces ▁ multiple ▁ data ▁ points ▁ ( of ▁ whatever ▁ dimension ) STRNEWLINE ▁ to ▁ a ▁ single ▁ data ▁ point . STRNEWLINE ▁ use _ cols ▁ : ▁ list , ▁ optional STRNEWLINE ▁ List ▁ of ▁ columns ▁ to ▁ include ▁ in ▁ binning ; ▁ use ▁ all ▁ columns ▁ by ▁ default . STRNEWLINE STRNEWLINE ▁ Returns STRNEWLINE ▁ - - - - - STRNEWLINE ▁ resampled _ df ▁ : ▁ pandas . DataFrame STRNEWLINE STRNEWLINE ▁ References STRNEWLINE ▁ - - - - - STRNEWLINE ▁ http : / / docs . scipy . org / doc / scipy / reference / generated / scipy . interpolate . interp1d . html STRNEWLINE ▁ """ NEW_LINE bin_anchors , binning = self . _bin_by_edges ( bin_anchors , bin_edges ) NEW_LINE return self . resample ( bin_anchors , binning , interpolation , agg , use_cols = use_cols ) NEW_LINE DEDENT
def _bin_by_edges ( self , bin_anchors , bin_edges ) : NEW_LINE INDENT " Compute ▁ bin ▁ assignment ▁ and , ▁ if ▁ needed , ▁ bin _ anchors . " NEW_LINE time = self . _dataframe [ ' time ' ] . values NEW_LINE # ▁ Get ▁ edges ▁ into ▁ 1D ▁ array [ L , ▁ R , ▁ L , ▁ R , ▁ . . . ] ENDCOM edges_as_pairs = np . reshape ( bin_edges , ( - 1 , 2 ) ) NEW_LINE all_edges = np . ravel ( edges_as_pairs ) NEW_LINE if not np . all ( np . diff ( all_edges ) >= 0 ) : NEW_LINE INDENT raise ValueError ( " Illegal ▁ binning : ▁ the ▁ left ▁ edge ▁ must ▁ be ▁ less ▁ " " than ▁ the ▁ right ▁ edge . " ) NEW_LINE # ▁ Sort ▁ out ▁ where ▁ the ▁ array ▁ each ▁ time ▁ would ▁ be ▁ inserted . ENDCOM DEDENT binning = np . searchsorted ( all_edges , time ) . astype ( float ) NEW_LINE # ▁ Times ▁ that ▁ would ▁ get ▁ inserted ▁ at ▁ even ▁ positions ▁ are ▁ between ▁ bins . ENDCOM # ▁ Mark ▁ them ENDCOM binning [ binning % 2 == 0 ] = np . nan NEW_LINE binning //= 2 # ▁ Make ▁ bin ▁ number ▁ sequential , ▁ not ▁ odds ▁ only . ENDCOM NEW_LINE if bin_anchors is None : NEW_LINE INDENT bin_anchors = np . mean ( edges_as_pairs , axis = 1 ) # ▁ bin ▁ centers ENDCOM NEW_LINE DEDENT else : NEW_LINE INDENT if len ( bin_anchors ) != len ( bin_edges ) : NEW_LINE INDENT raise ValueError ( " There ▁ are ▁ { 0 } ▁ bin _ anchors ▁ but ▁ { 1 } ▁ pairs ▁ of ▁ " " bin _ edges . ▁ These ▁ must ▁ match . " . format ( len ( bin_anchors ) , len ( bin_edges ) ) ) NEW_LINE DEDENT DEDENT return bin_anchors , binning NEW_LINE DEDENT
def resample ( self , bin_anchors , binning , interpolation = None , agg = None , verify_integrity = True , use_cols = None ) : NEW_LINE INDENT """ STRNEWLINE ▁ Return ▁ data ▁ resampled ▁ into ▁ bins ▁ with ▁ the ▁ specified ▁ edges . STRNEWLINE STRNEWLINE ▁ Parameters STRNEWLINE ▁ - - - - - STRNEWLINE ▁ bin _ anchors ▁ : ▁ list STRNEWLINE ▁ These ▁ are ▁ time ▁ points ▁ where ▁ interpolated ▁ values ▁ will ▁ be ▁ evaluated . STRNEWLINE ▁ Bin ▁ centers ▁ are ▁ usually ▁ a ▁ good ▁ choice . STRNEWLINE ▁ bin _ anchors ▁ : ▁ list STRNEWLINE ▁ Bin ▁ assignment . ▁ Example : ▁ [ 1 , ▁ 1 , ▁ 2 , ▁ 2 , ▁ 3 , ▁ 3 ] ▁ puts ▁ six ▁ data ▁ points STRNEWLINE ▁ into ▁ three ▁ bins ▁ with ▁ two ▁ points ▁ each . STRNEWLINE ▁ interpolation ▁ : ▁ dict , ▁ optional STRNEWLINE ▁ Override ▁ the ▁ default ▁ interpolation ▁ ( upsampling ) ▁ behavior ▁ of ▁ any STRNEWLINE ▁ data ▁ source ▁ by ▁ passing ▁ a ▁ dictionary ▁ of ▁ source ▁ names ▁ mapped ▁ onto STRNEWLINE ▁ one ▁ of ▁ the ▁ following ▁ interpolation ▁ methods . STRNEWLINE STRNEWLINE ▁ { None , ▁ ' linear ' , ▁ ' nearest ' , ▁ ' zero ' , ▁ ' slinear ' , ▁ ' quadratic ' , STRNEWLINE ▁ ' cubic ' } STRNEWLINE STRNEWLINE ▁ None ▁ means ▁ that ▁ each ▁ time ▁ bin ▁ must ▁ have ▁ at ▁ least ▁ one ▁ value . STRNEWLINE ▁ See ▁ scipy . interpolator ▁ for ▁ more ▁ on ▁ the ▁ other ▁ methods . STRNEWLINE ▁ agg ▁ : ▁ dict , ▁ optional STRNEWLINE ▁ Override ▁ the ▁ default ▁ reduction ▁ ( downsampling ) ▁ behavior ▁ of ▁ any ▁ data STRNEWLINE ▁ source ▁ by ▁ passing ▁ a ▁ dictionary ▁ of ▁ source ▁ names ▁ mapped ▁ onto ▁ any STRNEWLINE ▁ callable ▁ that ▁ reduces ▁ multiple ▁ data ▁ points ▁ ( of ▁ whatever ▁ dimension ) STRNEWLINE ▁ to ▁ a ▁ single ▁ data ▁ point . STRNEWLINE ▁ verify _ integrity ▁ : ▁ bool , ▁ optional STRNEWLINE ▁ For ▁ a ▁ cost ▁ in ▁ performance , ▁ verify ▁ that ▁ the ▁ downsampling ▁ function STRNEWLINE ▁ produces ▁ data ▁ of ▁ the ▁ expected ▁ shape . ▁ True ▁ by ▁ default . STRNEWLINE ▁ use _ cols ▁ : ▁ list , ▁ optional STRNEWLINE ▁ List ▁ of ▁ columns ▁ to ▁ include ▁ in ▁ binning ; ▁ use ▁ all ▁ columns ▁ by ▁ default . STRNEWLINE STRNEWLINE ▁ Returns STRNEWLINE ▁ - - - - - STRNEWLINE ▁ resampled _ df ▁ : ▁ pandas . DataFrame STRNEWLINE STRNEWLINE ▁ References STRNEWLINE ▁ - - - - - STRNEWLINE ▁ http : / / docs . scipy . org / doc / scipy / reference / generated / scipy . interpolate . interp1d . html STRNEWLINE ▁ """ NEW_LINE if use_cols is None : NEW_LINE INDENT use_cols = self . columns NEW_LINE DEDENT plan = self . Planner ( self ) NEW_LINE upsampling_rules = plan . determine_upsample ( interpolation , use_cols ) NEW_LINE downsampling_rules = plan . determine_downsample ( agg , use_cols ) NEW_LINE grouped = self . _dataframe . groupby ( binning ) NEW_LINE first_point = grouped . first ( ) NEW_LINE counts = grouped . count ( ) NEW_LINE resampling_requirements = _is_resampling_applicable ( counts ) NEW_LINE index = np . arange ( len ( bin_anchors ) ) NEW_LINE result = { } # ▁ dict ▁ of ▁ DataFrames , ▁ to ▁ become ▁ one ▁ MultiIndexed ▁ DataFrame ENDCOM NEW_LINE for name in use_cols : NEW_LINE INDENT upsample = upsampling_rules [ name ] NEW_LINE downsample = downsampling_rules [ name ] NEW_LINE upsampling_possible = resampling_requirements [ ' upsampling _ possible ' ] [ name ] NEW_LINE downsampling_needed = resampling_requirements [ ' downsampling _ needed ' ] [ name ] NEW_LINE result [ name ] = pd . DataFrame ( index = index ) NEW_LINE # ▁ Put ▁ the ▁ first ▁ ( maybe ▁ only ) ▁ value ▁ into ▁ a ▁ Series . ENDCOM # ▁ We ▁ will ▁ overwrite ▁ as ▁ needed ▁ below . ENDCOM result [ name ] [ ' val ' ] = pd . Series ( data = first_point [ name ] ) NEW_LINE # ▁ Short - circuit ▁ if ▁ we ▁ are ▁ done . ENDCOM if not ( upsampling_possible or downsampling_needed ) : NEW_LINE INDENT logger . debug ( " % s ▁ has ▁ exactly ▁ one ▁ data ▁ point ▁ per ▁ bin " , name ) NEW_LINE continue NEW_LINE DEDENT result [ name ] [ ' count ' ] = counts [ name ] NEW_LINE # ▁ If ▁ any ▁ bin ▁ has ▁ no ▁ data , ▁ use ▁ the ▁ upsampling ▁ rule ▁ to ▁ interpolate ENDCOM # ▁ at ▁ the ▁ center ▁ of ▁ the ▁ empty ▁ bins . ▁ If ▁ there ▁ is ▁ no ▁ rule , ▁ simply ENDCOM # ▁ leave ▁ some ▁ bins ▁ empty . ▁ Do ▁ not ▁ raise ▁ an ▁ error . ENDCOM if upsampling_possible and ( upsample is not None ) : NEW_LINE INDENT if upsample in ( ' ffill ' , ' bfill ' ) : NEW_LINE INDENT result [ name ] [ ' val ' ] . fillna ( method = upsample , inplace = True ) NEW_LINE DEDENT else : NEW_LINE INDENT dense_col = self . _dataframe [ name ] . dropna ( ) NEW_LINE y = dense_col . values NEW_LINE x = self . _dataframe [ ' time ' ] . reindex_like ( dense_col ) . values NEW_LINE interpolator = interp1d ( x , y , kind = upsample ) NEW_LINE # ▁ Outside ▁ the ▁ limits ▁ of ▁ the ▁ data , ▁ the ▁ interpolator ▁ will ENDCOM # ▁ fail . ▁ Leave ▁ any ▁ such ▁ entires ▁ empty . ENDCOM is_safe = ( ( bin_anchors > np . min ( x ) ) & ( bin_anchors < np . max ( x ) ) ) NEW_LINE safe_times = bin_anchors [ is_safe ] NEW_LINE safe_bins = index [ is_safe ] NEW_LINE interp_points = pd . Series ( interpolator ( safe_times ) , index = safe_bins ) NEW_LINE logger . debug ( " Interpolating ▁ to ▁ fill ▁ % d ▁ of ▁ % d ▁ " " empty ▁ bins ▁ in ▁ % s " , len ( safe_bins ) , ( counts [ name ] == 0 ) . sum ( ) , name ) NEW_LINE result [ name ] [ ' val ' ] . fillna ( interp_points , inplace = True ) NEW_LINE # ▁ Short - circuit ▁ if ▁ we ▁ are ▁ done . ENDCOM DEDENT DEDENT if not downsampling_needed : NEW_LINE INDENT logger . debug ( " % s ▁ has ▁ at ▁ most ▁ one ▁ data ▁ point ▁ per ▁ bin " , name ) NEW_LINE continue NEW_LINE # ▁ Multi - valued ▁ bins ▁ must ▁ be ▁ downsampled ▁ ( reduced ) . ▁ If ▁ there ▁ is ▁ no ENDCOM # ▁ rule ▁ for ▁ downsampling , ▁ we ▁ have ▁ no ▁ recourse : ▁ we ▁ must ▁ raise . ENDCOM DEDENT if ( downsample is None ) : NEW_LINE INDENT raise BinningError ( " The ▁ specified ▁ binning ▁ puts ▁ multiple ▁ " " ' {0 } ' ▁ measurements ▁ in ▁ at ▁ least ▁ one ▁ bin , ▁ " " and ▁ there ▁ is ▁ no ▁ rule ▁ for ▁ downsampling ▁ " " ( i . e . , ▁ reducing ) ▁ it . " . format ( name ) ) NEW_LINE DEDENT if verify_integrity and callable ( downsample ) : NEW_LINE INDENT downsample = _build_verified_downsample ( downsample , self . col_info [ name ] . shape ) NEW_LINE DEDENT g = grouped [ name ] # ▁ for ▁ brevity ENDCOM NEW_LINE if self . col_info [ name ] . ndim == 0 : NEW_LINE INDENT logger . debug ( " The ▁ scalar ▁ column ▁ % s ▁ must ▁ be ▁ downsampled . " , name ) NEW_LINE # ▁ For ▁ scalars , ▁ pandas ▁ knows ▁ what ▁ to ▁ do . ENDCOM downsampled = g . agg ( downsample ) NEW_LINE std_series = g . std ( ) NEW_LINE max_series = g . max ( ) NEW_LINE min_series = g . min ( ) NEW_LINE DEDENT else : NEW_LINE # ▁ For ▁ nonscalars , ▁ we ▁ are ▁ abusing ▁ groupby ▁ and ▁ must ▁ go ▁ to ▁ a ENDCOM # ▁ a ▁ little ▁ more ▁ trouble ▁ to ▁ guarantee ▁ success . ENDCOM INDENT logger . debug ( " The ▁ nonscalar ▁ column ▁ % s ▁ must ▁ be ▁ downsampled . " , name ) NEW_LINE if not callable ( downsample ) : NEW_LINE # ▁ Do ▁ this ▁ lookup ▁ here ▁ so ▁ that ▁ strings ▁ can ▁ be ▁ passed ENDCOM # ▁ in ▁ the ▁ call ▁ to ▁ resample . ENDCOM INDENT downsample = ColSpec . _downsample_mapping [ downsample ] NEW_LINE DEDENT downsampled = g . apply ( lambda x : downsample ( np . asarray ( x . dropna ( ) ) ) ) NEW_LINE std_series = g . apply ( lambda x : np . std ( np . asarray ( x . dropna ( ) ) , 0 ) ) NEW_LINE max_series = g . apply ( lambda x : np . max ( np . asarray ( x . dropna ( ) ) , 0 ) ) NEW_LINE min_series = g . apply ( lambda x : np . min ( np . asarray ( x . dropna ( ) ) , 0 ) ) NEW_LINE # ▁ This ▁ ( counts [ name ] ▁ > ▁ 1 ) ▁ is ▁ redundant , ▁ but ▁ there ▁ is ▁ no ▁ clean ▁ way ▁ to ENDCOM # ▁ pass ▁ it ▁ here ▁ without ▁ refactoring . ▁ Not ▁ a ▁ huge ▁ cost . ENDCOM DEDENT result [ name ] [ ' val ' ] . where ( ~ ( counts [ name ] > 1 ) , downsampled , inplace = True ) NEW_LINE result [ name ] [ ' std ' ] = std_series NEW_LINE result [ name ] [ ' max ' ] = max_series NEW_LINE result [ name ] [ ' min ' ] = min_series NEW_LINE DEDENT result = pd . concat ( result , axis = 1 ) # ▁ one ▁ MultiIndexed ▁ DataFrame ENDCOM NEW_LINE result . index . name = ' bin ' NEW_LINE # ▁ Convert ▁ time ▁ timestamp ▁ or ▁ timedelta , ▁ depending ▁ on ▁ the ▁ state ▁ of ENDCOM # ▁ self . convert _ times ▁ and ▁ self . reference _ time . ENDCOM for col_name in self . _time_columns : NEW_LINE INDENT if isinstance ( result [ col_name ] , pd . DataFrame ) : NEW_LINE INDENT subcols = result [ col_name ] . columns NEW_LINE for subcol in subcols & { ' max ' , ' min ' , ' val ' } : NEW_LINE INDENT result [ ( col_name , subcol ) ] = self . _maybe_convert_times ( result [ ( col_name , subcol ) ] ) NEW_LINE DEDENT for subcol in subcols & { ' std ' } : NEW_LINE INDENT result [ ( col_name , subcol ) ] = pd . to_timedelta ( result [ ( col_name , subcol ) ] , unit = ' s ' ) NEW_LINE DEDENT DEDENT else : NEW_LINE INDENT result [ col_name ] = self . _maybe_convert_times ( result [ col_name ] ) NEW_LINE DEDENT DEDENT return result NEW_LINE DEDENT
def __getitem__ ( self , source_name ) : NEW_LINE INDENT if source_name not in list ( self . col_info . keys ( ) ) + [ ' time ' ] : NEW_LINE INDENT raise KeyError ( " No ▁ data ▁ from ▁ a ▁ source ▁ called ▁ ' {0 } ' ▁ has ▁ been ▁ " " added . " . format ( source_name ) ) NEW_LINE # ▁ Unlike ▁ output ▁ from ▁ binning ▁ functions , ▁ this ▁ is ▁ indexed ENDCOM # ▁ on ▁ time . ENDCOM DEDENT result = self . _dataframe [ source_name ] . dropna ( ) NEW_LINE result . index = self . _dataframe [ ' time ' ] . reindex_like ( result ) NEW_LINE return result NEW_LINE DEDENT
def __getattr__ ( self , attr ) : NEW_LINE # ▁ Developer ▁ beware : ▁ if ▁ any ▁ properties ▁ raise ▁ an ▁ AttributeError , ENDCOM # ▁ this ▁ will ▁ mask ▁ it . ▁ Comment ▁ this ▁ magic ▁ method ▁ to ▁ debug ▁ properties . ENDCOM INDENT if attr in self . col_info . keys ( ) : NEW_LINE INDENT return self [ attr ] NEW_LINE DEDENT else : NEW_LINE INDENT raise AttributeError ( " DataMuxer ▁ has ▁ no ▁ attribute ▁ { 0 } ▁ and ▁ no ▁ " " data ▁ source ▁ named ▁ ' {0 } ' " . format ( attr ) ) NEW_LINE DEDENT DEDENT
def ncols ( self ) : NEW_LINE INDENT """ STRNEWLINE ▁ The ▁ number ▁ of ▁ columns ▁ that ▁ the ▁ DataMuxer ▁ contains STRNEWLINE ▁ """ NEW_LINE return len ( self . col_info ) NEW_LINE DEDENT
def col_info_by_ndim ( self ) : NEW_LINE INDENT """ Dictionary ▁ mapping ▁ dimensionality ▁ ( ndim ) ▁ onto ▁ a ▁ list ▁ of ▁ ColSpecs """ NEW_LINE result = { } NEW_LINE for name , col_spec in six . iteritems ( self . col_info ) : NEW_LINE INDENT try : NEW_LINE INDENT result [ col_spec . ndim ] NEW_LINE DEDENT except KeyError : NEW_LINE INDENT result [ col_spec . ndim ] = [ ] NEW_LINE DEDENT result [ col_spec . ndim ] . append ( col_spec ) NEW_LINE DEDENT return result NEW_LINE DEDENT
