<DOCUMENT_ID="scnak@@ andala/derby/tree/master/java/engine/org/apache/@@ derby/iapi/services/@@ crypto/Cipher@@ Provider.java"> package org . apache . derby . iapi . services . crypto ; import java . security . Key ; import org . apache . derby . iapi . error . StandardException ; public interface CipherProvider { int encrypt ( byte [ ] cleartext , int offset , int length , byte [ ] ciphertext , int outputOffset ) throws StandardException ; int decryp@@ t ( byte [ ] ciphertext , int offset , int length , byte [ ] cleartext , int outputOffset ) throws StandardException ; public int getEncryptionBlockSize ( ) ; } </DOCUMENT>
<DOCUMENT_ID="dslomov/intellij-community/tree/master/java/java-tests/testData/@@ inspection/deadCode/@@ junitAbstractClassWith@@ Inheritor/src/ChildTest.java"> public class ChildTest extends AbstractTest { } </DOCUMENT>
<DOCUMENT_ID="eli@@ jah513/@@ ice/tree/master/java/@@ test/src/main/java/@@ test/Ice@@ /location/ServerLocatorRegistry.java"> package test . Ice . location ; import test . Ice . location . Test . _TestLocatorRegistryDisp ; public class ServerLocatorRegistr@@ y extends _TestLocatorRegistryDisp { @ Override public void setAdapterDirectProxy_async ( Ice . AMD_LocatorRegistry_setAdapterDirectProxy cb , String adapter , Ice . ObjectPrx object , Ice . Current current ) { if ( object != null ) { _adapters . put ( adapter , object ) ; } else { _adapters . remove ( adapter ) ; } cb . ice_response ( ) ; } @ Override public void setReplicatedAdapterDirectProxy_async ( Ice . AMD_LocatorRegistry_setReplicatedAdapterDirect@@ Proxy cb , String adapter , String replica , Ice . ObjectPrx object , Ice . Current current ) { if ( object != null ) { _adapters . put ( adapter , object ) ; _adapters . put ( replica , object ) ; } else { _adapters . remove ( adapter ) ; _adapters . remove ( replica ) ; } cb . ice_response ( ) ; } @ Override public void setServerProcessProxy_async ( Ice . AMD_LocatorRegistry_setServerProcess@@ Proxy cb , String id , Ice . ProcessPrx proxy , Ice . Current current ) { } @ Override public void addObject ( Ice . ObjectPrx object , Ice . Current current ) { _objects . put ( object . ice_getIdentity ( ) , object ) ; } public Ice . ObjectPrx getAdapter ( String adapter ) throws Ice . AdapterNotFoundException { Ice . ObjectPrx obj = _adapters . get ( adapter ) ; if ( obj == null ) { throw new Ice . AdapterNotFoundException ( ) ; } return obj ; } public Ice . ObjectPrx getObject ( Ice . Identity id ) throws Ice . ObjectNotFoundException { Ice . ObjectPrx obj = _objects . get ( id ) ; if ( obj == null ) { throw new Ice . ObjectNotFoundException ( ) ; } return obj ; } private java . util . HashMap < String , Ice . ObjectPrx > _adapters = new java . util . HashMap < String , Ice . ObjectPrx > ( ) ; private java . util . HashMap < Ice . Identity , Ice . ObjectPrx > _objects = new java . util . HashMap < Ice . Identity , Ice . ObjectPrx > ( ) ; } </DOCUMENT>
<DOCUMENT_ID="iamjak@@ ob/elasticsearch@@ /tree/master/core/src/test/java/org/@@ elasticsearch/benchmark/search@@ /aggregations/@@ GlobalOrdinalsBenchmark.java"> package org . elasticsearch . benchmark . search . aggregations ; import com . carrotsearch . hppc . IntIntHashMap ; import com . carrotsearch . hppc . ObjectHashSet ; import com . carrotsearch . randomizedtesting . generators . RandomStrings ; import org . elasticsearch . action . admin . cluster . health . ClusterHealthResponse ; import org . elasticsearch . action . admin . cluster . stats . ClusterStatsResponse ; import org . elasticsearch . action . bulk . BulkRequestBuilder ; import org . elasticsearch . action . search . SearchResponse ; import org . elasticsearch . benchmark . search . aggregations . TermsAggregation@@ SearchBenchmark . StatsResult ; import org . elasticsearch . bootstrap . BootstrapForTesting ; import org . elasticsearch . client . Client ; import org . elasticsearch . common . settings . Settings ; import org . elasticsearch . common . unit . ByteSizeValue ; import org . elasticsearch . common . unit . SizeValue ; import org . elasticsearch . common . unit . TimeValue ; import org . elasticsearch . discovery . Discovery ; import org . elasticsearch . indices . IndexAlreadyExistsException ; import org . elasticsearch . node . Node ; import org . elasticsearch . search . aggregations . AggregationBuilders ; import org . elasticsearch . transport . TransportModule ; import java . util . * ; import static org . elasticsearch . cluster . metadata . IndexMetaData . SETTING_NUMBER_OF_REPLICAS ; import static org . elasticsearch . cluster . metadata . IndexMetaData . SETTING_NUMBER_OF_SHARDS ; import static org . elasticsearch . common . settings . Settings . settingsBuilder ; import static org . elasticsearch . common . xcontent . XContentFactory . jsonBuilder ; import static org . elasticsearch . index . query . QueryBuilders . matchAllQuery ; import static org . elasticsearch . node . NodeBuilder . nodeBuilder ; public class GlobalOrdinalsBenchmark { private static final String INDEX_NAME = " index " ; private static final String TYPE_NAME = " type " ; private static final int QUERY_WARMUP = 25 ; private static final int QUERY_COUNT = 100 ; private static final int FIELD_START = 1 ; private static final int FIELD_LIMIT = 1 << 22 ; private static final boolean USE_DOC_VALUES = false ; static long COUNT = SizeValue . parseSizeValue ( "5m " ) . singles ( ) ; static Node node ; static Client client ; public static void main ( String [ ] args ) throws Exception { System . setProperty ( " es . logger . prefix " , " " ) ; BootstrapForTesting . ensureInitialized ( ) ; Random random = new Random ( ) ; Settings settings = settingsBuilder ( ) . put ( " index . refresh _ interval " , " - 1" ) . put ( SETTING_NUMBER_OF_SHARDS , 1 ) . put ( SETTING_NUMBER_OF_REPLICAS , 0 ) . put ( TransportModule . TRANSPORT_TYPE_KEY , " local " ) . build ( ) ; String clusterName = GlobalOrdinalsBenchmark . class . getSimpleName ( ) ; node = nodeBuilder ( ) . clusterName ( clusterName ) . settings ( settingsBuilder ( ) . put ( settings ) ) . node ( ) ; client = node . client ( ) ; try { client . admin ( ) . indices ( ) . prepareCreate ( INDEX_NAME ) . addMapping ( TYPE_NAME , jsonBuilder ( ) . startObject ( ) . startObject ( TYPE_NAME ) . startArray ( " dynamic _ templates " ) . startObject ( ) . startObject ( " default " ) . field ( " match " , " * " ) . field ( " match _ mapping _ type " , " string " ) . startObject ( " mapping " ) . field ( " type " , " string " ) . field ( " index " , " not _ analyzed " ) . startObject ( " fields " ) . startObject ( " doc _ values " ) . field ( " type " , " string " ) . field ( " index " , " no " ) . startObject ( " fielddata " ) . field ( " format " , " doc _ values " ) . endObject ( ) . endObject ( ) . endObject ( ) . endObject ( ) . endObject ( ) . endObject ( ) . endArray ( ) . endObject ( ) . endObject ( ) ) . get ( ) ; ObjectHashSet < String > uniqueTerms = new ObjectHashSet < > ( ) ; for ( int i = 0 ; i < FIELD_LIMIT ; i ++ ) { boolean added ; do { added = uniqueTerms . add ( RandomStrings . randomAsciiOfLength ( random , 16 ) ) ; } while ( ! added ) ; } String [ ] sValues = uniqueTerms . toArray ( String . class ) ; uniqueTerms = null ; BulkRequestBuilder builder = client . prepareBulk ( ) ; IntIntHashMap tracker = new IntIntHashMap ( ) ; for ( int i = 0 ; i < COUNT ; i ++ ) { Map < String , Object > fieldValues = new HashMap < > ( ) ; for ( int fieldSuffix = 1 ; fieldSuffix <= FIELD_LIMIT ; fieldSuffix <<= 1 ) { int index = tracker . putOr@@ Add ( fieldSuffix , 0 , 0 ) ; if ( index >= fieldSuffix ) { index = random . nextInt ( fieldSuffix ) ; fieldValues . put ( " field _ " + fieldSuffix , sValues [ index ] ) ; } else { fieldValues . put ( " field _ " + fieldSuffix , sValues [ index ] ) ; tracker . put ( fieldSuffix , ++ index ) ; } } builder . add ( client . prepareIndex ( INDEX_NAME , TYPE_NAME , String . valueOf ( i ) ) . setSource ( fieldValues ) ) ; if ( builder . numberOfActions ( ) >= 1000 ) { builder . get ( ) ; builder = client . prepareBulk ( ) ; } } if ( builder . numberOfActions ( ) > 0 ) { builder . get ( ) ; } } catch ( IndexAlreadyExistsException e ) { System . out . println ( " - - > ▁ Index ▁ already ▁ exists , ▁ ignoring ▁ indexing ▁ phase , ▁ waiting ▁ for ▁ green " ) ; ClusterHealthResponse clusterHealthResponse = client . admin ( ) . cluster ( ) . prepareHealth ( ) . setWaitForGreenStatus ( ) . setTimeout ( "10m " ) . execute ( ) . actionGet ( ) ; if ( clusterHealthResponse . isTimedOut ( ) ) { System . err . println ( " - - > ▁ Timed ▁ out ▁ waiting ▁ for ▁ cluster ▁ health " ) ; } } client . admin ( ) . cluster ( ) . prepareUpdateSettings ( ) . setTransientSettings ( Settings . builder ( ) . put ( " logger . index . fielddata . ordinals " , " DEBUG " ) ) . get ( ) ; client . admin ( ) . indices ( ) . prepareRefresh ( INDEX_NAME ) . execute ( ) . actionGet ( ) ; COUNT = client . prepareCount ( INDEX_NAME ) . setQuery ( matchAllQuery ( ) ) . execute ( ) . actionGet ( ) . getCount ( ) ; System . out . println ( " - - > ▁ Number ▁ of ▁ docs ▁ in ▁ index : ▁ " + COUNT ) ; List < StatsResult > stats = new ArrayList < > ( ) ; for ( int fieldSuffix = FIELD_START ; fieldSuffix <= FIELD_LIMIT ; fieldSuffix <<= 1 ) { String fieldName = " field _ " + fieldSuffix ; String name = " global _ ordinals - " + fieldName ; if ( USE_DOC_VALUES ) { fieldName = fieldName + " . doc _ values " ; name = name + " _ doc _ values " ; } stats . add ( terms ( name , fieldName , " global _ ordinals _ low _ cardinality " ) ) ; } for ( int fieldSuffix = FIELD_START ; fieldSuffix <= FIELD_LIMIT ; fieldSuffix <<= 1 ) { String fieldName = " field _ " + fieldSuffix ; String name = " ordinals - " + fieldName ; if ( USE_DOC_VALUES ) { fieldName = fieldName + " . doc _ values " ; name = name + " _ doc _ values " ; } stats . add ( terms ( name , fieldName , " ordinals " ) ) ; } System . out . println ( " - - - - - - - - - - - - - - - - - - ▁ SUMMARY ▁ - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - " ) ; System . out . format ( Locale . ENGLISH , " % 30s % 10s % 10s % 15s \n " , " name " , " took " , " millis " , " fieldata ▁ size " ) ; for ( StatsResult stat : stats ) { System . out . format ( Locale . ENGLISH , " % 30s % 10s % 10d % 15s \n " , stat . name , TimeValue . timeValueMillis ( stat . took ) , ( stat . took / QUERY_COUNT ) , stat . fieldDataMemoryUsed ) ; } System . out . println ( " - - - - - - - - - - - - - - - - - - ▁ SUMMARY ▁ - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - " ) ; client . close ( ) ; node . close ( ) ; } private static StatsResult terms ( String name , String field , String executionHint ) { long totalQueryTime ; client . admin ( ) . indices ( ) . prepareClearCache ( ) . setFieldDataCache ( true ) . execute ( ) . actionGet ( ) ; System . gc ( ) ; System . out . println ( " - - > ▁ Warmup ▁ ( " + name + " ) . . . " ) ; for ( int j = 0 ; j < QUERY_WARMUP ; j ++ ) { SearchResponse searchResponse = client . prepareSearch ( INDEX_NAME ) . setSize ( 0 ) . setQuery ( matchAllQuery ( ) ) . addAggregation ( AggregationBuilders . terms ( name ) . field ( field ) . executionHint ( executionHint ) ) . get ( ) ; if ( j == 0 ) { System . out . println ( " - - > ▁ Loading ▁ ( " + field + " ) : ▁ took : ▁ " + searchResponse . getTook ( ) ) ; } if ( searchResponse . getHits ( ) . totalHits ( ) != COUNT ) { System . err . println ( " - - > ▁ mismatch ▁ on ▁ hits " ) ; } } System . out . println ( " - - > ▁ Warmup ▁ ( " + name + " ) ▁ DONE " ) ; System . out . println ( " - - > ▁ Running ▁ ( " + name + " ) . . . " ) ; totalQueryTime = 0 ; for ( int j = 0 ; j < QUERY_COUNT ; j ++ ) { SearchResponse searchResponse = client . prepareSearch ( INDEX_NAME ) . setSize ( 0 ) . setQuery ( matchAllQuery ( ) ) . addAggregation ( AggregationBuilders . terms ( name ) . field ( field ) . executionHint ( executionHint ) ) . get ( ) ; if ( searchResponse . getHits ( ) . totalHits ( ) != COUNT ) { System . err . println ( " - - > ▁ mismatch ▁ on ▁ hits " ) ; } totalQueryTime += searchResponse . getTookInMillis ( ) ; } System . out . println ( " - - > ▁ Terms ▁ Agg ▁ ( " + name + " ) : ▁ " + ( totalQueryTime / QUERY_COUNT ) + " ms " ) ; String nodeId = node . injector ( ) . getInstance ( Discovery . class ) . localNode ( ) . getId ( ) ; ClusterStatsResponse clusterStateResponse = client . admin ( ) . cluster ( ) . prepareCluster@@ Stats ( ) . setNodesIds ( nodeId ) . get ( ) ; System . out . println ( " - - > ▁ Heap ▁ used : ▁ " + clusterStateResponse . getNodesStats ( ) . getJvm ( ) . getHeapUsed ( ) ) ; ByteSizeValue fieldDataMemoryUsed = clusterStateResponse . getIndices@@ Stats ( ) . getFieldData ( ) . getMemorySize ( ) ; System . out . println ( " - - > ▁ Fielddata ▁ memory ▁ size : ▁ " + fieldDataMemoryUsed ) ; return new StatsResult ( name , totalQueryTime , fieldDataMemoryUsed ) ; } } </DOCUMENT>
<DOCUMENT_ID="beano/@@ gocd/tree/master/plugin-infra@@ /go-plugin-infra/test/com/@@ thoughtworks/go@@ /plugin/infra@@ /MultipleExtensionPluginWithPluginManagerIntegrationTest.java"> package com . thoughtworks . go . plugin . infra ; import java . io . File ; import java . io . IOException ; import com . thoughtworks . go . util . SystemEnvironment ; import com . thoughtworks . go . plugin . activation . DefaultGoPluginActivator ; import com . thoughtworks . go . plugin . infra . listeners . DefaultPluginJarChangeListener ; import com . thoughtworks . go . plugin . infra . monitor . PluginFileDetails ; import com . thoughtworks . go . plugin . infra . plugininfo . GoPluginDescriptor ; import org . apache . commons . io . FileUtils ; import org . junit . After ; import org . junit . Before ; import org . junit . Test ; import org . junit . runner . RunWith ; import org . springframework . beans . factory . annotation . Autowired ; import org . springframework . test . context . ContextConfiguration ; import org . springframework . test . context . junit4 . SpringJUnit4ClassRunner ; import static com . thoughtworks . go . util . SystemEnvironment . PLUGIN_ACTIVATOR_JAR_PATH ; import static com . thoughtworks . go . util . SystemEnvironment . PLUGIN_BUNDLE_PATH ; import static com . thoughtworks . go . util . SystemEnvironment . PLUGIN_FRAMEWORK_ENABLED ; import static org . hamcrest . CoreMatch@@ ers . is ; import static org . hamcrest . MatcherAssert . assertThat ; @ RunWith ( SpringJUnit4ClassRunner . class ) @ ContextConfiguration ( locations = { " classpath : / applicationContext - plugin - infra . xml " } ) public class MultipleExtensionPluginWithPluginManagerIntegrationTest { public static final String PLUGIN_DESC_PROPERTY_SET_BY_PLUGIN_EXT_1 = " testplugin . multiple . extension . DescriptorPlugin1 . setPluginDescriptor . invoked " ; public static final String PLUGIN_DESC_PROPERTY_SET_BY_PLUGIN_EXT_2 = " testplugin . multiple . extension . DescriptorPlugin2 . setPluginDescriptor . invoked " ; private static final String PLUGIN_DIR_NAME = " . / tmp - DefPlgnMgrIntTest " ; private static final String BUNDLE_DIR_NAME = " . / tmp - bundles - DefPlgnMgrIntTest " ; private static final File PLUGIN_DIR = new File ( PLUGIN_DIR_NAME ) ; private static final File BUNDLE_DIR = new File ( BUNDLE_DIR_NAME ) ; private static final String PLUGIN_ID = " testplugin . multiple . extension " ; @ Autowired DefaultPluginManager pluginManager ; @ Autowired DefaultPluginJarChangeListener jarChangeListener ; @ Autowired SystemEnvironment systemEnvironment ; static { System . setProperty ( PLUGIN_ACTIVATOR_JAR_PATH . propertyName ( ) , " defaultFiles / go - plugin - activator . jar " ) ; System . setProperty ( PLUGIN_BUNDLE_PATH . propertyName ( ) , BUNDLE_DIR_NAME ) ; System . setProperty ( PLUGIN_FRAMEWORK_ENABLED . propertyName ( ) , " Y " ) ; } private static File pathOfFileInDefaultFiles ( String filePath ) { return new File ( MultipleExtensionPluginWithPluginManagerIntegrationTest . class . getClassLoader ( ) . getResource ( " defaultFiles / " + filePath ) . getFile ( ) ) ; } @ Test public void shouldProvide@@ DescriptorToMultipleExtension@@ sImplementingThe@@ PluginDescriptorAware@@ Interface ( ) throws Exception { GoPluginDescriptor plugin = pluginManager . getPluginDescriptorFor ( PLUGIN_ID ) ; assertThat ( plugin . id ( ) , is ( PLUGIN_ID ) ) ; assertThat ( plugin . bundleSymbolicName ( ) , is ( PLUGIN_ID ) ) ; assertThat ( plugin . bundleClassPath ( ) , is ( " lib / go - plugin - activator . jar , . " ) ) ; assertThat ( plugin . bundleActivator ( ) , is ( DefaultGoPluginActivator . class . getCanonicalName ( ) ) ) ; assertThat ( plugin . isInvalid ( ) , is ( false ) ) ; assertThat ( System . getProperty ( PLUGIN_DESC_PROPERTY_SET_BY_PLUGIN_EXT_1 ) , is ( plugin . toString ( ) ) ) ; assertThat ( System . getProperty ( PLUGIN_DESC_PROPERTY_SET_BY_PLUGIN_EXT_2 ) , is ( plugin . toString ( ) ) ) ; } @ Before public void setUpPluginInfrastructure ( ) throws IOException { PLUGIN_DIR . mkdirs ( ) ; BUNDLE_DIR . mkdirs ( ) ; try { pluginManager . startInfrastructure ( ) ; } catch ( Exception e ) { e . printStackTrace ( ) ; } jarChangeListener . pluginJarAdded ( new PluginFileDetails ( pathOfFileInDefaultFiles ( " plugin - with - multiple - extensions . jar " ) , false ) ) ; } @ After public void tearDown ( ) throws Exception { System . clearProperty ( PLUGIN_DESC_PROPERTY_SET_BY_PLUGIN_EXT_1 ) ; System . clearProperty ( PLUGIN_DESC_PROPERTY_SET_BY_PLUGIN_EXT_2 ) ; FileUtils . deleteQuietly ( PLUGIN_DIR ) ; FileUtils . deleteQuietly ( BUNDLE_DIR ) ; pluginManager . stopInfrastructure ( ) ; FileUtils . deleteQuietly ( PLUGIN_DIR ) ; FileUtils . deleteQuietly ( BUNDLE_DIR ) ; } } </DOCUMENT>
<DOCUMENT_ID="lik@@ aiwalk@@ man/hadoop/tree/master/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/dao/TaskAttemptsInfo.java"> package org . apache . hadoop . mapreduce . v2 . app . webapp . dao ; import java . util . ArrayList ; import javax . xml . bind . annotation . XmlAccessType ; import javax . xml . bind . annotation . XmlAccessorType ; import javax . xml . bind . annotation . XmlRootElement ; @ XmlRootElement ( name = " taskAttempts " ) @ XmlAccessorType ( XmlAccessType . FIELD ) public class TaskAttemptsInfo { protected ArrayList < TaskAttemptInfo > taskAttempt = new ArrayList < TaskAttemptInfo > ( ) ; public TaskAttemptsInfo ( ) { } public void add ( TaskAttemptInfo taskattemptInfo ) { taskAttempt . add ( taskattemptInfo ) ; } public ArrayList < TaskAttemptInfo > getTaskAttempts ( ) { return taskAttempt ; } } </DOCUMENT>
<DOCUMENT_ID="seanzwx@@ /tmp/tree/master/@@ shop/android/imageloader@@ /src/com/nostra@@ 13/universalimageloader@@ /cache/@@ memory/impl/LimitedAgeMemoryCache.java"> package com . nostra13 . universalimageloader . cache . memory . impl ; import java . util . Collection ; import java . util . Collections ; import java . util . HashMap ; import java . util . Map ; import com . nostra13 . universalimageloader . cache . memory . MemoryCacheAware ; public class LimitedAgeMemoryCache < K , V > implements MemoryCacheAware < K , V > { private final MemoryCacheAware < K , V > cache ; private final long maxAge ; private final Map < K , Long > loadingDates = Collections . synchron@@ izedMap ( new HashMap < K , Long > ( ) ) ; public LimitedAgeMemoryCache ( MemoryCacheAware < K , V > cache , long maxAge ) { this . cache = cache ; this . maxAge = maxAge * 1000 ; } @ Override public boolean put ( K key , V value ) { boolean putSuccesfully = cache . put ( key , value ) ; if ( putSuccesfully ) { loadingDates . put ( key , System . currentTimeMillis ( ) ) ; } return putSuccesfully ; } @ Override public V get ( K key ) { Long loadingDate = loadingDates . get ( key ) ; if ( loadingDate != null && System . currentTimeMillis ( ) - loadingDate > maxAge ) { cache . remove ( key ) ; loadingDates . remove ( key ) ; } return cache . get ( key ) ; } @ Override public void remove ( K key ) { cache . remove ( key ) ; loadingDates . remove ( key ) ; } @ Override public Collection < K > keys ( ) { return cache . keys ( ) ; } @ Override public void clear ( ) { cache . clear ( ) ; loadingDates . clear ( ) ; } } </DOCUMENT>
<DOCUMENT_ID="didoup@@ impon/CC15@@ 2/tree/master/src/main/java/net/minec@@ raft/server/@@ ItemReed.java"> package net . minecraft . server ; import org . bukkit . block . BlockState ; import org . bukkit . craftbukkit . block . CraftBlockState ; import org . bukkit . craftbukkit . event . CraftEventFactory ; import org . bukkit . event . block . BlockPlaceEvent ; public class ItemReed extends Item { private int id ; public ItemReed ( int i , Block block ) { super ( i ) ; this . id = block . id ; } public boolean a ( ItemStack itemstack , EntityHuman entityhuman , World world , int i , int j , int k , int l ) { int clickedX = i , clickedY = j , clickedZ = k ; if ( world . getTypeId ( i , j , k ) == Block . SNOW . id ) { l = 0 ; } else { if ( l == 0 ) { -- j ; } if ( l == 1 ) { ++ j ; } if ( l == 2 ) { -- k ; } if ( l == 3 ) { ++ k ; } if ( l == 4 ) { -- i ; } if ( l == 5 ) { ++ i ; } } if ( itemstack . count == 0 ) { return false ; } else { if ( world . a ( this . id , i , j , k , false ) ) { Block block = Block . byId [ this . id ] ; BlockState replacedBlockState = CraftBlockState . getBlockState ( world , i , j , k ) ; if ( world . setRawTypeId ( i , j , k , this . id ) ) { BlockPlaceEvent event = CraftEventFactory . callBlockPlaceEvent ( world , entityhuman , replacedBlockState , clickedX , clickedY , clickedZ , block ) ; if ( event . isCancelled ( ) || ! event . canBuild ( ) ) { world . setTypeIdAndData ( i , j , k , replacedBlockState . getTypeId ( ) , replacedBlockState . getRawData ( ) ) ; } else { world . update ( i , j , k , this . id ) ; Block . byId [ this . id ] . postPlace ( world , i , j , k , l ) ; Block . byId [ this . id ] . postPlace ( world , i , j , k , entityhuman ) ; world . makeSound ( ( double ) ( ( float ) i + 0.5F ) , ( double ) ( ( float ) j + 0.5F ) , ( double ) ( ( float ) k + 0.5F ) , block . stepSound . getName ( ) , ( block . stepSound . getVolume1 ( ) + 1.0F ) / 2.0F , block . stepSound . getVolume2 ( ) * 0.8F ) ; -- itemstack . count ; } } } return true ; } } } </DOCUMENT>
<DOCUMENT_ID="voly@@ rique/FrameworkBenchmark@@ s/tree/master/framework@@ s/Java/@@ dropwizard/src/main/java/com/@@ example/helloworld/HelloJDBI@@ Service.java"> package com . example . helloworld ; import com . example . helloworld . db . jdbi . FortuneRepository ; import io . dropwizard . Application ; import io . dropwizard . jdbi3 . JdbiFactory ; import io . dropwizard . jdbi3 . bundles . JdbiExceptionsBundle ; import io . dropwizard . setup . Bootstrap ; import io . dropwizard . setup . Environment ; import io . dropwizard . views . ViewBundle ; import org . jdbi . v3 . core . Jdbi ; import com . example . helloworld . config . HelloWorldConfiguration ; import com . example . helloworld . db . jdbi . WorldRepository ; import com . example . helloworld . resources . FortuneResource ; import com . example . helloworld . resources . WorldResource ; public class HelloJDBIService extends Application < HelloWorldConfiguration > { public static void main ( String [ ] args ) throws Exception { new HelloJDBIService ( ) . run ( args ) ; } @ Override public void initialize ( Bootstrap < HelloWorldConfiguration > bootstrap ) { bootstrap . addBundle ( new ViewBundle < > ( ) ) ; bootstrap . addBundle ( new JdbiExceptionsBundle ( ) ) ; } @ Override public void run ( HelloWorldConfiguration config , Environment environment ) { final JdbiFactory factory = new JdbiFactory ( ) ; final Jdbi jdbi = factory . build ( environment , config . getDatabase@@ Configuration ( ) , " RDBMS " ) ; environment . jersey ( ) . register ( new WorldResource ( new WorldRepository ( jdbi ) ) ) ; environment . jersey ( ) . register ( new FortuneResource ( new FortuneRepository ( jdbi ) ) ) ; } } </DOCUMENT>
